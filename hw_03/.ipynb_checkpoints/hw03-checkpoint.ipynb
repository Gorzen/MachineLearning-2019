{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\renewcommand{\\real}{\\mathbb{R}}$\n",
    "$\\renewcommand{\\xb}{\\mathbf{x}}$\n",
    "$\\renewcommand{\\wb}{\\mathbf{w}}$\n",
    "$\\renewcommand{\\Xb}{\\mathbf{X}}$\n",
    "$\\renewcommand{\\Lb}{\\mathbf{L}}$\n",
    "\n",
    "$\\DeclareMathOperator*{\\argmin}{argmin}$\n",
    "# Exercise Session 3\n",
    "\n",
    "\n",
    "# Paper and pencil exercises\n",
    "\n",
    "## Logic gates\n",
    "\n",
    "Let the perceptron be defined as a function $y(\\xb) = f(\\wb^{\\top}\\xb)$, where $\\wb$ is a vector of (learned) weights and $f$ is a step function $ f(a) = \\begin{cases} +1, & a \\ge 0 \\\\ -1, & a < 0 \\end{cases}$, as seen in the lecture. Such a perceptron can be used to model various logic gate functions. However, in our particular case the logic gates do not output the usual values $\\{0, 1\\}$ but rather $\\mathbf{\\{-1, 1\\}}$ to stay consistent with the perceptron's step function $f$:\n",
    "\n",
    "\n",
    "#### 1. AND gate (no bias)\n",
    "\n",
    "Given a perceptron taking an input $\\xb = [x_{1}, x_{2}]$ where $x_{1}, x_{2} \\in \\{0, 1\\}$, find a weight vector $\\wb_{AND} = [w_{1}, w_{2}]$ which models logic gate **AND**, i.e. $y(x_{1}, x_{2}) = x_{1}\\ \\text{AND}\\ x_{2}$. Does any $\\wb_{AND}$ exist? If not, why?\n",
    "\n",
    "<div class=\"row\">\n",
    "  <div class=\"col-md-1\">\n",
    "      \n",
    "$x_{1}$ | $x_{2}$ | $y$\n",
    "---     | ---     | ---\n",
    "0       | 0       | -1\n",
    "0       | 1       | -1\n",
    "1       | 0       | -1\n",
    "1       | 1       | 1\n",
    "\n",
    "  </div>\n",
    "  <div class=\"col-md-4\">\n",
    "\n",
    "<span></span>\n",
    "![and gate](imgs/and.svg)\n",
    "\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "#### 2. AND gate\n",
    "Now let us add a *bias* term to the perceptron, i.e. an additional input $x_{0}$, where $x_{0} = 1$. The input to the perceptron is now $\\xb = [x_{0}, x_{1}, x_{2}] = [1, x_{1}, x_{2}]$. Find a weight vector $\\wb_{AND} = [w_{0}, w_{1}, w_{2}]$ such that the perceptron would model a logic gate **AND**.\n",
    "\n",
    "<div class=\"row\">\n",
    "  <div class=\"col-md-6\">\n",
    "  <span></span>\n",
    "\n",
    "![and gate](imgs/and_bias.svg)\n",
    "\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "#### 3. OR gate\n",
    "Find a weight vector $\\wb_{OR} = [w_{0}, w_{1}, w_{2}]$ such that the perceptron would model logic gate **OR**, i.e. $y(x_{1}, x_{2}) = x_{1}\\ \\text{OR}\\ x_{2}$.\n",
    "\n",
    "<div class=\"row\">\n",
    "  <div class=\"col-md-1\">\n",
    "      \n",
    "$x_{1}$ | $x_{2}$ | y\n",
    "--- | --- | ---\n",
    "0 | 0 | -1\n",
    "0 | 1 | 1\n",
    "1 | 0 | 1\n",
    "1 | 1 | 1\n",
    "\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "#### 4. XOR gate\n",
    "Find a weight vector $\\wb_{XOR} = [w_{0}, w_{1}, w_{2}]$ such that the perceptron would model logic gate **XOR**, i.e. $y(x_{1}, x_{2}) = x_{1}\\ \\text{XOR}\\ x_{2}$. Does $\\wb_{XOR}$ exist? If not, why?\n",
    "\n",
    "<div class=\"row\">\n",
    "  <div class=\"col-md-1\">\n",
    "      \n",
    "$x_{1}$ | $x_{2}$ | y\n",
    "--- | --- | ---\n",
    "0 | 0 | -1\n",
    "0 | 1 | 1\n",
    "1 | 0 | 1\n",
    "1 | 1 | -1\n",
    "\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "#### 5. XOR gate (2 biases)\n",
    "We have seen that adding a bias term to the perceptron from task *1. AND gate (no bias)* saved the day and helped us model the AND gate properly using perceptron. What if we add one more bias to the perceptron modeling the XOR gate from task *4. XOR gate*, such that we would have $\\xb = [x_{0_{a}}, x_{0_{b}}, x_{1}, x_{2}] = [1, 1, x_{1}, x_{2}]$ $\\wb_{XOR} = [w_{0_{a}}, w_{0_{b}} w_{1}, w_{2}]$ does it allow us to model the XOR gate now? Why?\n",
    "\n",
    "<div class=\"row\">\n",
    "  <div class=\"col-md-6\">\n",
    "  <span></span>\n",
    "\n",
    "![and gate](imgs/xor_2bias.svg)\n",
    "\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'common'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-39f511e4def9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'..'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhw_03\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhelpers\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhelpers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhelpers\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mchelpers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# 3rd party\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'common'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "\n",
    "# project files\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import hw_03.helpers as helpers\n",
    "import common.helpers as chelpers\n",
    "\n",
    "# 3rd party\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Perceptron\n",
    "\n",
    "## 1.1 Loading and visualizing data\n",
    "\n",
    "As we've done in the last part of previous week's exercise, we will work with and _Iris flower dataset_. For simplicity, we will only use 2 out of 4 features and 2 out of 3 classes (named as *setosa* and *versicolor*). Therefore, our dataset is as follows:\n",
    "\n",
    "  - data: $\\Xb \\in \\real^{N \\times 2}$, $\\forall \\xb \\in \\Xb: \\xb \\in \\real^{2}$\n",
    "  - labels: $\\Lb \\in \\real^{N}$, $\\forall l \\in \\Lb: l \\in \\{-1, 1\\}$ \n",
    "\n",
    "Let us first load the data and visualize the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and visualize the dataset.\n",
    "data, labels = helpers.load_ds1(center=True)\n",
    "fig = helpers.vis_ds1(data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually, the classes appear to be linearly separable, which is a good news, since as the *perceptron convergence theorem* states, if there exists an exact solution (i.e. data are linearly separable), the perceptron learning algorithm is guaranteed to find the solution in finite number of steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Perceptron learning algorithm\n",
    "\n",
    "Given an input vector $\\xb$ and a weight vector $\\wb$, the prediction performed by perceptron is defined as \n",
    "\n",
    "$$ y(\\xb) = \\begin{cases} +1 & \\text{if}\\ \\wb^{\\top}\\xb \\ge 0 \\\\ -1 & \\text{otherwise} \\end{cases} $$\n",
    "\n",
    "Please implement the function `predict()` accordingly. Recall that you can use the function `np.dot()` to do matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w):\n",
    "    \"\"\" Perceptron prediction.\n",
    "    \n",
    "    Args:\n",
    "        x (np.array): Input data of shape (N, D), N is # samples, D is dimension.\n",
    "        w (np.array): Perceptron weights of shape (D, ).\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Predictions of shape (N, ), where each value is in {-1, 1}.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### YOUR CODE ###\n",
    "    \n",
    "    return ...\n",
    "    \n",
    "    #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to track the training progress it is useful to monitor the current accuracy of the perceptron on the whole dataset, which is defined as\n",
    "\n",
    "$$ f_{\\text{acc}} = \\frac{\\text{# correct predictions}}{\\text{# all predictions}}$$\n",
    "\n",
    "Please implement the function `accuracy()` accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(labels_gt, labels_pred):\n",
    "    \"\"\" Computes accuracy.\n",
    "    \n",
    "    Args:\n",
    "        labels_gt (np.array): GT (ground truth) labels of shape (N, ).\n",
    "        labels_pred (np.array): Predicted labels of shape (N, ).\n",
    "        \n",
    "    Returns:\n",
    "        float: Accuracy, in range [0, 1].\n",
    "    \"\"\"\n",
    "    \n",
    "    ### YOUR CODE ###\n",
    "    \n",
    "    return ...\n",
    "    \n",
    "    #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, in order to find the weight vector $\\wb$ corresponding to a decision boundary separating the classes, we need a learning algorithm. The perceptron training objective can be formulated as an energy minimization problem\n",
    "\n",
    "$$ \\wb^{*} = \\argmin_{\\wb} E(\\wb), $$\n",
    "\n",
    "with an energy function $E$ defined as\n",
    "\n",
    "$$ E(\\wb) = -\\sum_{n \\in M}(\\wb^{\\top}\\xb_{n})l_{n}, $$\n",
    "\n",
    "where $M$ is a set of wrongly classified samples $\\xb_{n}$ with corresponding GT (ground truth) labels $l_{n}$. To find $\\wb^{*}$ we apply stochastic gradient descent, where the change in $\\wb$ after one step is given as\n",
    "\n",
    "$$ \\wb^{t + 1}  = \\wb^{t} - \\eta \\nabla E(\\wb^{t}) $$\n",
    "\n",
    "where $\\eta$ is the learning rate. In general, we try to choose a learning rate that is small enough to converge and large enough to converge quickly. \n",
    "\n",
    "The algorithm continuously iterates through the dataset and performs the weight update each time it stumbles upon a misclassified sample (predicted given the most recent weight vector $\\wb$). The algorithm stops after no missclassified sample is found or after the maximum number of epochs is reached. The algorithm can be described by the following pseudocode:\n",
    "\n",
    "<ol>\n",
    "<li>Initialize the weight vector $\\wb$ arbitrarily.</li>\n",
    "    \n",
    "<li>Shuffle dataset and for each data sample $\\xb_{n}$:\n",
    "    <ol>\n",
    "        <li>If $\\xb_{n}$ is not correctly classified, update weights.</li>\n",
    "    </ol>\n",
    "</li>\n",
    "<li>If no misclassified sample encountered or maximum epochs reached, end. Else, go to 2.</li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "Please fill in the function `fit_perceptron()`:\n",
    "- Extract one sample and a corresponding label and find out whether it is correctly classified.\n",
    "- Mathematically derive $\\nabla E(\\wb^{t})$ and use it to update the weights. (Check the lecture slides for hints on what $\\nabla E(\\wb^{t})$ is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_perceptron(data, labels, w, eta, max_epochs, vis=None, \n",
    "                   history_save_period=100, verbose=True):\n",
    "    \"\"\" Trains the perceptron to find a decision boundary between \n",
    "    two classes given `data` and corresponding `labels`.\n",
    "    \n",
    "    Args:\n",
    "        data (np.array): Dataset of shape (N, D).\n",
    "        labels (np.array): Labels of shape (N, ).\n",
    "        w (np.array): Initial weights of shape (D, ).\n",
    "        eta float: The learning rate.\n",
    "        max_epochs (int): Maximum number of epochs for which \n",
    "            the training is run.\n",
    "            \n",
    "    Returns:\n",
    "        w (np.array): Weights after convergence or `max_epochs`\n",
    "            reached. Shape (D, )\n",
    "        history (np.array): History of accuracy monitored during \n",
    "            training.\n",
    "    \"\"\"\n",
    "    \n",
    "    history = []\n",
    "    num_samples = data.shape[0]\n",
    "    \n",
    "    for ep in range(max_epochs):\n",
    "        # Shuffle the data.\n",
    "        inds = np.random.permutation(num_samples)\n",
    "        num_errs = 0\n",
    "        \n",
    "        # Train for one epoch.\n",
    "        for it, ind in enumerate(inds):\n",
    "            c_pred = predict(data, w)\n",
    "            acc = accuracy(labels, c_pred)\n",
    "\n",
    "            # Print and save stats.\n",
    "            if verbose:\n",
    "                print('\\rep {}/{}, it {}/{}, accuracy {:.3f}'.\n",
    "                      format(ep + 1, max_epochs, it + 1, num_samples, acc), \n",
    "                      end='')\n",
    "            if (it + 1) % history_save_period == 0:\n",
    "                history.append(acc)\n",
    "            \n",
    "            ### YOUR CODE ###\n",
    "            \n",
    "            # Extract one sample from the dataste.\n",
    "            sample = ...\n",
    "            lab = ...\n",
    "            \n",
    "            # Find whether the sample is correctly classified.\n",
    "            correct = ... \n",
    "            \n",
    "            #################\n",
    "            \n",
    "            # Update weights.\n",
    "            if not correct:\n",
    "                num_errs += 1\n",
    "                \n",
    "                ### YOUR CODE ###\n",
    "                \n",
    "                w_new = ...\n",
    "            \n",
    "                #################\n",
    "            \n",
    "                # Visualize current state.\n",
    "                if vis is not None:\n",
    "                    vis.step(sample, w, w_new, c_pred)\n",
    "                w = w_new\n",
    "        \n",
    "        if num_errs == 0:\n",
    "            break\n",
    "\n",
    "    print()\n",
    "    return w, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Training a perceptron on centered data\n",
    "\n",
    "Let us test the perceptron learning algorithm on the *Iris dataset* which we have already loaded. Note that for now the dataset is centered, i.e. the decision boundary will coincide with the coordinate system origin $[0, 0]$ and therefore **the bias term is not needed** for the time being.\n",
    "\n",
    "The cell below runs the perceptron learning algorithm and it lets you manually step through the iterations. Once you run the cell, **the input field appears** below the plot. Please click in the field and gradually keep pressing `Enter` key - this advances the training step by step. The algorithm stops each time it encounters an incorrectly classified sample, it visualizes the current decision boundary and its normal given by $\\wb^{t}$, the selected data sample (black arrow) and a new decision boundary and its normal given by $\\wb^{t+1}$. If you have troubles running this cell (plot does not show, cell gets stuck, etc.), please call the `fit_perceptron` function as this instead: `fit_perceptron(data, labels, w, eta, max_epochs, verbose=True, vis=None)` and observe the text output (accuracy).\n",
    "\n",
    "Please fill in the code below, namely select a suitable number of `max_epochs` and initialize the weight vector `w`. Use the visualization tool to debug  your implementation of `predict()` and `fit_perceptron()`.\n",
    "\n",
    "**Hints:**\n",
    "- If the training algorithm stops but it is not yet fully converged, either there is a problem in your implementation of `predict()` and `fit_perceptron()`, or you have selected too small value for `max_epochs`.\n",
    "- Set the weight vector components `w` to a reasonably small values (e.g. $<1$), otherwise the decision boundary normal will not be fully seen in the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the visualizer\n",
    "vis = helpers.PerceptronStepVisualizer(data, labels)\n",
    "\n",
    "# Training settings\n",
    "\n",
    "### YOUR CODE ###\n",
    "\n",
    "max_epochs = ...  \n",
    "w = ... # (w1, w2)\n",
    "\n",
    "#################\n",
    "\n",
    "eta = 1.0  # learning rate\n",
    "\n",
    "# Training and prediction.\n",
    "w_star, history = fit_perceptron(data, labels, w, eta, max_epochs, verbose=False, vis=vis)\n",
    "labels_pred = predict(data, w_star)\n",
    "\n",
    "# Visualize the result.\n",
    "vis._reset()\n",
    "_ = helpers.vis_ds1_dec_bound(data, labels, labels_pred, w_star, fig=vis._fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Training a perceptron on non-centered data\n",
    "\n",
    "Let us load the same dataset but it will not be centered now as you can see on the plot below (note that all the data samples are now centered around a point $\\sim[5.5, 3.25]$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load non-centered dataset and visualize.\n",
    "data_nc, labels = helpers.load_ds1(center=False)\n",
    "fig = helpers.vis_ds1(data_nc, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, run the same perceptron learning algorithm (for sufficiently many epochs). Do not forget to initialize the weights vector $\\wb$ again. Did the algorithm converge? If not, why?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit perceptron without bias to non-centered data.\n",
    "\n",
    "### YOUR CODE ###\n",
    "\n",
    "w = ...  # (w1, w2)\n",
    "max_epochs = ...\n",
    "\n",
    "#################\n",
    "\n",
    "eta = 1.0\n",
    "\n",
    "# Training and prediction\n",
    "w_star, history = fit_perceptron(data_nc, labels, w, eta, max_epochs, verbose=True)\n",
    "labels_pred = predict(data_nc, w_star)\n",
    "\n",
    "# Visualize the decision boundary.\n",
    "_ = helpers.vis_ds1_dec_bound(data_nc, labels, labels_pred, w_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Adding a bias term\n",
    "\n",
    "It is clear from the previous experiment that a bias term will be needed in order to find a decision boundary of non-centered data. In the cell below, augment each data sample $\\xb_{n}$ with a bias term to get $\\hat{\\xb}_{n} = [1\\ \\xb_{n}^{\\top}]$ and run the perceptron training algorithm again. Do not forget that since a bias term was added, the weight vector also needs to be extended, i.e. now $\\wb \\in \\real^{3} = [w_{0}, w_{1}, w_{2}]$.\n",
    "\n",
    "Note that you should **not** use a `for` cycle to augment the data by a bias term, i.e. your line of code should correspond to $\\hat{\\Xb} = [\\mathbf{1}\\ \\Xb]$, where $\\mathbf{1} \\in \\real^{N}$ is a (column) vector of 1. You can create a vector of ones using `np.ones()` and you can use the function `np.concatenate()` to concatenate the ones vector with your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE ###\n",
    "\n",
    "# Extend data by bias.\n",
    "data_nc_bias = ...\n",
    "\n",
    "# Initialize the weights.\n",
    "w = ...  # (w0, w1, w2)\n",
    "\n",
    "# Training settings.\n",
    "max_epochs = ...\n",
    "\n",
    "################\n",
    "\n",
    "eta = 1.0\n",
    "\n",
    "# Train and predict.\n",
    "w_star, history = fit_perceptron(data_nc_bias, labels, w, eta,\n",
    "                                 max_epochs, verbose=True)\n",
    "labels_pred = predict(data_nc_bias, w_star)\n",
    "\n",
    "# Visualize the decision boundary.\n",
    "_ = helpers.vis_ds1_dec_bound(data_nc, labels, labels_pred, w_star)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 BONUS - Perceptron in sklearn\n",
    "\n",
    "Since the perceptron is a well-known machine learnin algorithm, standard implementation exist in various libraries. [scikit-learn](https://scikit-learn.org/stable/) is one of popular libraries which implements many machine learning algorithms and provides a Python interface. Let us use the `scikit-learn`'s implementation of perceptron and compare the results to our implementation.\n",
    "\n",
    "In general, a given model has to be instantiated first, then its method `fit` is used to find a set of parameters (weights) which meet a given objective. Finally, a prediction can be made by models function `predict` and it can be evaluated by e.g. its `score` function (which will in case of perceptron compute the accuracy). You will learn more about `scikit-learn` in the next week's exercise.\n",
    "\n",
    "Start by studying the documentation of the [perceptron model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html). Then, please fill in the code in the cell below to create and train the perceptron, extract the learned parameters, and visualize the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a perceptron implemented in sklearn.\n",
    "\n",
    "data = data_nc\n",
    "labels = labels\n",
    "\n",
    "### YOUR CODE ###\n",
    "\n",
    "# Instantiate the perceptron model\n",
    "\n",
    "\n",
    "# Train it by feeding it the non-centered dataset without a bias.\n",
    "\n",
    "\n",
    "# Get the predicted labels.\n",
    "labels_pred = ...\n",
    "\n",
    "# Print out the accuracy.\n",
    "acc = ...\n",
    "print('Accuracy: {}'.format(acc))\n",
    "      \n",
    "# Extract the weight vector, w = (w0, w1, w2)\n",
    "w_star = ...\n",
    "      \n",
    "#################\n",
    "\n",
    "# Visualize the decision boundary.\n",
    "_ = helpers.vis_ds1_dec_bound(data, labels, labels_pred, w_star)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
