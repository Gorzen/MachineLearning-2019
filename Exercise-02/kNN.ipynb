{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise session #2 - $k$-NN Classifier \n",
    "\n",
    "In this hands-on exercise you will implement your first machine learning\n",
    "algorithm, the **$k$-Nearest Neighbor classifier ($k$-NN)**. You will also get familiar with\n",
    "other very important concepts related to machine learning in practice,\n",
    "including data preprocessing, distance metrics, visualization, and model evaluation.\n",
    "\n",
    "We have provided general functionality and pointers for you here. Please complete the code with your own implementation below. Please also discuss and answer the follow-up questions.\n",
    "\n",
    "### Dataset and problem description\n",
    "\n",
    "The Healthy Body dataset contains body measurements acquired from **1250 people _from different ages, genders, and nationalities_** from different hospitals around the world. Health professionals have performed medical examinations and classified the individuals into three different body categories: **underweight, normal weight, and overweight.**\n",
    "\n",
    "Our goal is to automate the role of the health professionals. However, due to anonymity reasons, we have been provided access to limited information about the individuals: their measured _weights_ and _heights_, and their respective _body category_ only.\n",
    "\n",
    "We will use these features to train a $k$-NN classifier for the task.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable interactive plots, so you can zoom/pan/resize plots\n",
    "%matplotlib notebook\n",
    "\n",
    "# Libraries for numerical handling and visualization. Install if required\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and visualization\n",
    "\n",
    "The goal of supervised classification algorithms such as $k$-NN is to use information from a set of labeled examples, i.e., examples for which we know their class assignments, to infer the classes for unlabeled examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "\n",
    "# Paths\n",
    "features_annotated_path = \"hbody_feats_annotated.npy\"     # Weights, heights of individuals with known body category\n",
    "labels_annotated_path = \"hbody_labels_annotated.npy\"      # Body categories of those individuals\n",
    "features_unannotated_path = \"hbody_feats_unannotated.npy\" # Weights and heights of unknown body category individuals\n",
    "                                                          # - Goal: Figure out their body categories\n",
    "\n",
    "# Features organized in an NxD matrix; N examples, D features.\n",
    "# Another way to look at it: each of the N examples is a D-dimensional feature vector.\n",
    "\n",
    "features_annotated = np.load(features_annotated_path)\n",
    "features_unannotated = np.load(features_unannotated_path)\n",
    "labels_annotated = np.load(labels_annotated_path)\n",
    "\n",
    "class_names = ('Underweight', 'Normal weight', 'Overweight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. What are the target variables? What are the predictor variables?**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize annotated and unannotated sets\n",
    "\n",
    "colors = np.array([[0.85, 0.85, 0], [0, 0.5, 0], [0.25, 0.25, 1]])\n",
    "\n",
    "plt.figure(figsize=(9,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(f\"Annotated set ({len(labels_annotated)} examples)\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    plt.scatter(*features_annotated[labels_annotated==i].T,\n",
    "                c=colors[i, None], alpha=0.5, s=15, lw=0, label=class_name)\n",
    "plt.xlabel(\"Weight (kg)\")\n",
    "plt.ylabel(\"Height (cm)\")\n",
    "plt.legend();\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(f\"Unannotated set ({len(features_unannotated)} examples)\")\n",
    "plt.scatter(*features_unannotated.T, c='gray', alpha=0.5, s=15, lw=0, label='Unknown body category')\n",
    "plt.xlabel(\"Weight (kg)\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. Do you think this is an easy or a difficult classification problem? Why?**\n",
    "\n",
    "**Q. What should the unannotated set share in common with the annotated set?**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing data\n",
    "\n",
    "$k$-NN determines neighbors by computing the \"distance\" between two examples. For this process to work, we are required\n",
    "to normalize the features. This is true for many other machine learning algorithms as well.\n",
    "\n",
    "**Q. What would happen if we don't do this?**\n",
    "\n",
    "A very common way to normalize the data is by the so-called z-score standardization. It transforms values from an arbitrary range such that the result has mean $0$ and standard deviation $1$. The operation is defined as follows:\n",
    "\n",
    "$$\n",
    "x_{norm} = \\frac {x - \\mu_x} {\\sigma_x},\n",
    "$$\n",
    "for _each feature independently_.\n",
    "\n",
    "**Q. Why does this particular operation make sense?**\n",
    "\n",
    "**Q. What are the right $\\mu_x$ and $\\sigma_x$ to use? Why?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data.\n",
    "# Tip: Use numpy's broadcasting to write your normalization function\n",
    "\n",
    "def normalize...\n",
    "    [your code here]\n",
    "\n",
    "norm_features_annotated = normalize...\n",
    "\n",
    "# Verify normalization\n",
    "if np.allclose(norm_features_annotated.mean(axis=0), 0) and np.allclose(norm_features_annotated.std(axis=0), 1):\n",
    "    print(\"Everything alright here.\")\n",
    "else:\n",
    "    print(\"Nope. Try again.\")\n",
    "    \n",
    "# Remember to use the normalized version of your data from here onwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the annotated data for training and test sets\n",
    "\n",
    "We need to ensure that our method generalizes, which means it will correctly predict the class for new provided examples.\n",
    "\n",
    "In order to simulate this scenario, we will split our annotated data into two groups: the training set, and the test set.\n",
    "- The **training set** will be used for finding a classification criterion, a.k.a. learning the model.\n",
    "- The **test set** will be used for testing how well the learned model generalizes to data beyond that used for training.\n",
    "\n",
    "While the training set helps us find out how exactly to manipulate our data to find the right prediction, the test set tells us how well we expect to perform when given new data. Our training procedure is allowed to handle data from the training set only, and should not in any way use the information from the test set.\n",
    "\n",
    "### Cross-validation\n",
    "\n",
    "If we are only allowed to assess our model generalization _after_ training, how can we monitor and guide the training process? How can we know beforehand that we are using the best version of our model?\n",
    "\n",
    "The most common strategy for this, called **[cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics))**, is, simply put, to pretend that a part of our training data is in fact unannotated, and see if our method manages to predict the annotations correctly. In other words, we will reserve a portion of our training data to temporarily act as a small test set. The simplest way to do this, called _holdout method_, is to extract a fixed amount of annotated data, called a **validation set**, to estimate the performance of our model in a test set, and to use the rest for training purposes. [Other types](https://www.cs.cmu.edu/~schneide/tut5/node42.html) of cross validation include k-fold cross validation, and leave-one-out cross validation.\n",
    "\n",
    "Splitting your training data into a training and a validation set is also used for comparing different \"versions\" of your model. Many machine learning methods depend on predefined configuration settings, called _hyperparameters_, that heavily influence how the method behaves. In the case of $k$-NN, one such parameter is $k$, the number of neighbors.\n",
    "\n",
    "**Q. How exactly do you think a validation set can be used for hyperparameter optimization?**\n",
    "\n",
    "In this exercise, we will be applying holdout cross validation. Technically, your validation set is part of your training data. To avoid confusion, however, we often call the _training set_ the portion of your training data used for tuning your method. Therefore, for holdout cross validation, we can split our annotated data into a training set, a validation set, and a test set.\n",
    "\n",
    "**Q. Do you understand the difference between the validation set and the test set?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split labeled data into training and validation set\n",
    "# Tip: answering the questions below might give you hints for this step.\n",
    "\n",
    "np.random.seed(330)\n",
    "\n",
    "# How much annotated data for training and validation. The rest is used for testing.\n",
    "training_perc, validation_perc = 0.4, 0.1\n",
    "\n",
    "training_features = \n",
    "training_labels = \n",
    "validation_features = \n",
    "validation_labels = \n",
    "test_features = \n",
    "test_labels = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. What does `np.random.seed` do? Why is this useful?**\n",
    "\n",
    "**Q. How should one select the size of the validation set?**\n",
    " \n",
    "**Q. What would be the most appropriate way to select examples for the validation set?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training and validation data\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(f\"Training set ({len(training_labels)} examples)\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    plt.scatter(*training_features[training_labels==i].T,\n",
    "                c=colors[i, None], alpha=0.5, s=15, lw=0, label=class_name)\n",
    "plt.xlabel(\"Weight (normalized)\")\n",
    "plt.ylabel(\"Height (normalized)\")\n",
    "plt.gca().set_aspect('equal')\n",
    "xlims, ylims = plt.xlim(), plt.ylim()\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(f\"Validation set ({len(validation_labels)} examples)\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    plt.scatter(*validation_features[validation_labels==i].T,\n",
    "                c=colors[i, None], alpha=0.5, s=15, lw=0, label=class_name)\n",
    "plt.xlabel(\"Weight (normalized)\")\n",
    "plt.xlim(xlims)\n",
    "plt.ylim(ylims)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. Is validation set representative of whole dataset?**\n",
    "\n",
    "**Q. Notice that there is class imbalance. Would this be an issue?**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The $k$-Nearest Neighbors Classifier\n",
    "\n",
    "$k$-NN assigns as label to a given example the most popular label from its surroundings. The method is very intuitive, and can be summarized as:\n",
    "- Compute the distance between the example to classify and all the training examples.\n",
    "- Select the closest $k$ training examples.\n",
    "- Assign to the example the most common label among those neighbors.\n",
    "\n",
    "### Distance metrics\n",
    "\n",
    "There are many ways to define a distance between two examples. Two very common ones that we will use in this exercise are:\n",
    "\n",
    "#### Euclidean distance:\n",
    "\n",
    "$$\n",
    "m(\\mathbf{v}, \\mathbf{w}) = \\sqrt{ \\sum_{i=1}^d \\left(\\mathbf{v}_i - \\mathbf{w}_i\\right)^2 }\n",
    "$$\n",
    "\n",
    "This is the generalization of the Pythagorean theorem to an arbitrary number of dimensions, and corresponds to our intuitive interpretation of the straight-line distance between two points.\n",
    "\n",
    "#### Manhattan distance:\n",
    "\n",
    "$$\n",
    "m(\\mathbf{v}, \\mathbf{w}) = \\sum_{i=1}^d |\\mathbf{v}_i - \\mathbf{w}_i|\n",
    "$$\n",
    "\n",
    "Aggregates differences in features independently from one another. It is also known as city block distance. One can think of it as the minimum distance one would have to walk between two intersections in a city organized by regular blocks.\n",
    "\n",
    "\n",
    "**Q. Would you expect to find the same nearest neighbors to a point with both distance metrics?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the euclidean distance between a vector and a collection of vectors (matrix)\n",
    "# Tip: numpy's broadcasting allows you to write this in a very intuitive and simple way.\n",
    "def euclidean_dist(example, training_examples):\n",
    "    [your code here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the euclidean distance between a vector and a matrix\n",
    "def manhattan_dist(example, training_examples):\n",
    "    [your code here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's dissect $k$-NN by classifying one example\n",
    "\n",
    "Recall that the $k$-NN algorithm can be summarized as:\n",
    "1. Compute the distance between an example to classify and all the training examples.\n",
    "2. Select the closest $k$ training examples.\n",
    "3. Assign to the example the most common label among those neighbors.\n",
    "\n",
    "Let's get to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, let's set k to an arbitrary value.\n",
    "k = 7  # Number of neighbors to examine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a random example from the validation set\n",
    "\n",
    "example = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize the randomly chosen example in context\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"An unlabeled example,\\nrandomly chosen from the validation set\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    plt.scatter(*training_features[training_labels==i].T,\n",
    "                c=colors[i, None], alpha=0.25, s=15, lw=0, label=class_name + \" (training)\")\n",
    "\n",
    "plt.scatter(*example, marker='*', c='brown', alpha=0.75, s=50, label='Random unlabeled example')\n",
    "plt.xlabel(\"Weight (normalized)\")\n",
    "plt.ylabel(\"Height (normalized)\")\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. What class would you assign to this example?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the euclidean distances between the chosen example and all the annotated examples\n",
    "distances = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the indices of the k shortest distances from a list of distances\n",
    "def find_k_nearest_neighbors(k, distances):\n",
    "    [your code here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the k neighbors of the chosen example, and their respective labels\n",
    "neighbor_indices = \n",
    "neighbor_labels = \n",
    "\n",
    "print(\"\\n\".join(class_names[i] for i in neighbor_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize neighbors\n",
    "\n",
    "plt.figure()\n",
    "plt.title(f\"A randomly chosen unlabeled example from the validation set\\nand its {k}-nearest neighbors\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    plt.scatter(*training_features[training_labels==i].T,\n",
    "                c=colors[i, None], alpha=0.25, s=15, lw=0, label=class_name)\n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_indices = neighbor_indices[training_labels[neighbor_indices] == i]\n",
    "    if len(class_indices) > 0:\n",
    "        plt.scatter(*training_features[class_indices].T,\n",
    "                    c=colors[i, None], alpha=1, s=25, lw=0, label='Neighbor')\n",
    "\n",
    "ax = plt.scatter(*example, marker='*', c='brown', alpha=0.5, s=50, label='unlabeled example')\n",
    "plt.xlabel(\"Weight (normalized)\")\n",
    "plt.ylabel(\"Height (normalized)\")\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. What class would you assign to this example?**\n",
    "\n",
    "**Q. What class would $k$-NN assign to this example?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a list of neighbor labels, choose the most popular.\n",
    "# Tip: np.bincount is your friend.\n",
    "\n",
    "def predict_label(neighbor_labels):\n",
    "    [your code here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the label suggested by the example's neighbors\n",
    "example_label = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize prediction\n",
    "\n",
    "plt.figure()\n",
    "plt.title(f\"Labeling of a randomly chosen unlabeled example from the validation set\\nby the {k}-Nearest Neighbors algorithm\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    plt.scatter(*training_features[training_labels==i].T,\n",
    "                c=colors[i, None], alpha=0.25, s=15, lw=0, label=class_name)\n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_indices = neighbor_indices[training_labels[neighbor_indices] == i]\n",
    "    if len(class_indices) > 0:\n",
    "        plt.scatter(*training_features[class_indices].T,\n",
    "                    c=colors[i, None], alpha=.9, s=15, lw=1, label='Neighbor')\n",
    "\n",
    "ax = plt.scatter(*example, marker='*', c=colors[example_label, None], alpha=1, s=100, label=class_names[example_label])\n",
    "plt.xlabel(\"Weight (normalized)\")\n",
    "plt.ylabel(\"Height (normalized)\")\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. How would the $k$-NN algorithm change if we had more than two dimensions?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function kNN_one_example that applies all the previous steps\n",
    "# to predict the label of one example. Should return the predicted class.\n",
    "\n",
    "def kNN_one_example(unlabeled_example, training_features, training_labels, k):\n",
    "    [your code here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function kNN that applies kNN_one_example to an arbitrary number of examples.\n",
    "# Tip: numpy's apply_along_axis does most of the work for you.\n",
    "\n",
    "def kNN(unlabeled, training_features, training_labels, k):\n",
    "    [your code here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. While the above implementation works, it has some drawbacks. Can you identify them?**\n",
    "\n",
    "**Q. Can you think of a better implementation?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function you just defined to predict the labels of all examples from the validation set.\n",
    "predicted_labels = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize the results.\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Predicted classes for the entire validation set\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    plt.scatter(*training_features[training_labels==i].T,\n",
    "                c=colors[i, None], alpha=0.1, s=15, lw=0)\n",
    "for i, class_name in enumerate(class_names):    \n",
    "    plt.scatter(*validation_features[predicted_labels==i].T,\n",
    "                c=colors[i, None], marker='*', alpha=0.6, s=50, lw=0, label=class_name)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.xlabel(\"Weight (normalized)\")\n",
    "plt.ylabel(\"Height (normalized)\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But... how do we know this prediction is good?\n",
    "\n",
    "In order to quantify the performance of our model, we want to obtain a score that tells us how close the predictions were to the expected classification.\n",
    "\n",
    "The simplest way to do this is to compute the ratio of correctly predicted examples, also known as the accuracy:\n",
    "\n",
    "$$\n",
    "\\frac 1 N \\sum_{n=1}^N \\mathbf{1}[\\hat{y} \\neq y]\n",
    "$$\n",
    "\n",
    "**Q. Do you see any limitation to using accuracy to evaluate your model?**\n",
    "\n",
    "**Q. Can you think of other ways to evaluate your model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that computes the accuracy between a prediction and the expected labels.\n",
    "def accuracy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How well did your classifier perform?\n",
    "\n",
    "[your code here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. Is accuracy suitable for multiclass classification?**\n",
    "\n",
    "**Q. What other criteria, aside from accuracy, should one consider when choosing hyperparameters?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization\n",
    "\n",
    "Did we choose the best $k$?\n",
    "\n",
    "We can evaluate our model under different values of $k$ to compare them. A simple way to do it is to evaluate our model's predictions for the same validation set when using different values of $k$.\n",
    "\n",
    "**Q. Why should we use the same validation set for all cases?**\n",
    "\n",
    "**Q. What problems can we face by doing this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the evaluation metric for different values of k\n",
    "\n",
    "model_performace_validation = []  # Store the computed metrics here\n",
    "k_values = range(1, 21)           # Try these values for hyperparameter k\n",
    "\n",
    "[your code here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the performances for different values of k\n",
    "\n",
    "plt.figure(figsize=(9,4))\n",
    "plt.title(\"Performance on the validation set for different values of $k$\")\n",
    "plt.plot(k_values, model_performace_validation)\n",
    "plt.xlabel(\"Number of neighbors $k$\")\n",
    "plt.xticks(k_values)\n",
    "plt.ylabel(\"Performance (accuracy)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick hyperparameter value that yields the best performance\n",
    "best_k = \n",
    "\n",
    "print(f\"Best number of neighbors on validation set is k={best_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does your final model generalize?\n",
    "\n",
    "Now that we have tuned our model, we can apply it for prediction on the test set.\n",
    "\n",
    "**Q. How do you expect the model to perform, compared with the validation set performance?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test_labels = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize the predictions on the test set\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Predicted classes for the test set\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    plt.scatter(*training_features[training_labels==i].T,\n",
    "                c=colors[i, None], alpha=0.1, s=15, lw=0)\n",
    "for i, class_name in enumerate(class_names):    \n",
    "    plt.scatter(*test_features[predicted_test_labels==i].T,\n",
    "                c=colors[i, None], marker='*', alpha=0.5, s=50, lw=0, label=class_name)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.xlabel(\"Weight (normalized)\")\n",
    "plt.ylabel(\"Height (normalized)\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_performance = \n",
    "print(f\"{best_k}-NN Classifier predicted correctly {test_performance:.2%} of the test examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. Was this the value you were expecting?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on unannotated data\n",
    "\n",
    "We are finally ready to apply our model for prediction on unannotated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and preparation\n",
    "\n",
    "features_unannotated = np.load(features_unannotated_path)\n",
    "[your code here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. What should one take into account when feeding new data to a machine learning model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for unlabeled data.\n",
    "predicted_labels = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize the predictions on the test set\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Predicted classes for unannotated data\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    plt.scatter(*training_features[training_labels==i].T,\n",
    "                c=colors[i, None], alpha=0.1, s=15, lw=0)\n",
    "for i, class_name in enumerate(class_names):    \n",
    "    plt.scatter(*norm_features_unannotated[predicted_labels==i].T,\n",
    "                c=colors[i, None], marker='*', alpha=0.5, s=50, lw=0, label=class_name)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.xlabel(\"Weight (normalized)\")\n",
    "plt.ylabel(\"Height (normalized)\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. Do these class assignments look reasonable to you?**\n",
    "\n",
    "**Q. How would you evaluate if your predictions are reasonable here, without labels?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A multidimensional dataset\n",
    "\n",
    "Let's try to apply what we have learned on a different dataset.\n",
    "\n",
    "The Iris dataset quantifies the morphologic variation of [Iris flowers](https://en.wikipedia.org/wiki/Iris_(plant)) of three related species: _Iris setosa_, _Iris versicolor_, and _Iris virginica_. The dataset contains measurements of the length and the width of the sepals and petals, in centimeters, of different flowers from the three different species.\n",
    "\n",
    "Your goal is to train and apply the $k$-NN algorithm to this multiclass classification problem. You are encouraged to reuse the functions that you wrote for the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset path\n",
    "\n",
    "iris_path = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "iris_class_names = ('Iris-setosa', 'Iris-versicolor', 'Iris-virginica')\n",
    "iris_feature_names = (\"Sepal length\", \"Sepal width\", \"Petal length\", \"Petal width\")\n",
    "\n",
    "# Load data\n",
    "\n",
    "def iris_label_converter(string_class):\n",
    "    \"\"\" Converts labels from their string representation to a numerical value.\"\"\"\n",
    "    return iris_class_names.index(string_class)\n",
    "\n",
    "iris_data = np.loadtxt(iris_path, delimiter=',', converters={4: iris_label_converter}, encoding='latin1')\n",
    "\n",
    "iris_features, iris_labels = np.split(iris_data, [-1], axis=1)\n",
    "iris_labels = iris_labels.flatten().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (very minimal) Exploratory data analysis\n",
    "\n",
    "In order to make yourself familiar with the main characteristics of the dataset, let's visualize the pairwise interactions between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all possible pairs of features.\n",
    "# Use matplotlib's subplot to arrange those plots on a grid.\n",
    "\n",
    "plt.figure(figsize=(9, 9))\n",
    "\n",
    "[your code here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. Can you draw preliminary conclusions about the data?**\n",
    "\n",
    "**Q. Do you think this is an easy or a difficult classification problem? Why?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation, and test data\n",
    "np.random.seed(476)\n",
    "\n",
    "perc_training = 0.3\n",
    "perc_validation = 0.2\n",
    "perc_testing = 0.5\n",
    "\n",
    "...\n",
    "\n",
    "iris_training_features = \n",
    "iris_training_labels = \n",
    "\n",
    "iris_validation_features = \n",
    "iris_validation_labels = \n",
    "\n",
    "iris_testing_features = \n",
    "iris_testing_labels = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply $k$-NN\n",
    "Train a k-NN classifier, and use it to predict the classes on the testing set.\n",
    "\n",
    "You should know what to do from here on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[your code here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the test set\n",
    "iris_testing_prediction = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on the test set (remember this is only possible because this is a toy example.)\n",
    "[your code here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A closer look at the predictions\n",
    "\n",
    "Let's see what our classifier got right and what it got wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot of the Petal length vs Petal width\n",
    "# Highlight the examples where your classifier failed.\n",
    "\n",
    "[your code here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. Do misclassifications occur where you would expect?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the step above, but now plot Sepal length vs Sepal width.\n",
    "\n",
    "[your code here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. Do misclassifications occur where you would expect?**\n",
    "\n",
    "**Q. Why is this plot not consistent with the one above?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
