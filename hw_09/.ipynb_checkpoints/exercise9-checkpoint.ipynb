{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1 Introduction\n",
    "\n",
    "Welcome to the ninth practical session of CS233 - Introduction to Machine Learning.  \n",
    "In this exercise class we will start using Machine Learning methods to solve regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Regression Problem\n",
    "\n",
    "Let $f$ be a function $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}^v$ with a data set $\\left(X \\subseteq \\mathbb{R}^d, y\n",
    "\\subseteq\\mathbb{R}^v \\right )$. The regression problem is the task of estimating an approximation $\\hat{f}$ of $f$.\n",
    "Within this exercise we consider the special case of $v=1$, i.e. the problem is univariate as opposed to multivariate.\n",
    "Specifically, we will analyze the Boston house prices data set and predict costs based on properties such as per capita crime rate by town, pupil-teacher ratio by town etc.\n",
    "\n",
    "We will model the given data by means of a linear regression model, i.e. a model that explains a dependent variable in terms of a linear combination of independent variables.\n",
    "\n",
    "- How does a regression problem differ from a classification problem?\n",
    "- Why is the linear regression model a linear model? Is it linear in the dependent variables? Is it linear in the parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Load and inspect data\n",
    "\n",
    "We load the data and split it such that 80% and 20% are train and test data, respectively. \n",
    "After, we normalize the data such that each feature has zero mean and unit standard deviation. Please fill in the required code and complete the function `normalize`.\n",
    "\n",
    "- Explore the relation between different features and the house prices. Describe what you see. Can you identify any trends?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data set and print a description\n",
    "from sklearn.datasets import load_boston\n",
    "boston_dataset = load_boston()\n",
    "print(boston_dataset.DESCR)\n",
    "\n",
    "X = boston_dataset[\"data\"]\n",
    "y = boston_dataset[\"target\"]\n",
    "\n",
    "# remove categorical feature\n",
    "X = np.delete(X, 3, axis=1)\n",
    "# removing second mode\n",
    "ind = y<40\n",
    "X = X[ind,:]\n",
    "y = y[ind]\n",
    "\n",
    "# split the data into 80% training and 20% test data\n",
    "indices = np.arange(X.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "splitRatio = 0.8\n",
    "n          = X.shape[0]\n",
    "X_train    = X[indices[0:int(n*splitRatio)],:] \n",
    "y_train    = y[indices[0:int(n*splitRatio)]] \n",
    "X_test     = X[indices[int(n*(splitRatio)):],:] \n",
    "y_test     = y[indices[int(n*(splitRatio)):]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Make mean 1 and std dev 0 of the data.\n",
    "'''\n",
    "def normalize(X):\n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\"\n",
    "    \n",
    "    mu    = ...\n",
    "    std   = ...\n",
    "    X     = ...  # normalize\n",
    "    return X, mu, std\n",
    "\n",
    "#Use train stats for normalizing test set\n",
    "X_train,mu_train,std_train = normalize(X_train)\n",
    "X_test = (X_test-mu_train)/std_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploratory analysis of the data\n",
    "\n",
    "feature = 0\n",
    "plt.scatter(X_train[:,feature], y_train)\n",
    "plt.xlabel(f\"Attribute $X_{feature}$\")\n",
    "plt.ylabel(\"Price $y$\")\n",
    "plt.title(f\"Attribute $X_{feature}$ vs Price $y$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Closed-form solution for linear regression\n",
    "\n",
    "The linear regression model has an [analytical solution](https://en.wikipedia.org/wiki/Linear_least_squares). \n",
    "Please use this solution to complete the function `get_w_analytical` and to obtain the weight parameters $w$. Tip: You may want to use the function np.linalg.solve. \n",
    "\n",
    "- What is the time complexity of this approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_analytical(X_train,y_train):\n",
    "    \"\"\"compute the weight parameters w\"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\"\n",
    "        \n",
    "    # compute w via the normal equation\n",
    "    w = \n",
    "    return w\n",
    "\n",
    "def get_loss(w, X_train, y_train,X_test,y_test,val=False):\n",
    "    # predict dependent variables and MSE loss for seen training data\n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\"\n",
    "    loss_train = \n",
    "    loss_train_std = \n",
    "        \n",
    "    # predict dependent variables and MSE loss for unseen test data\n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\"\n",
    "    loss_test = \n",
    "    loss_test_std = \n",
    "    \n",
    "    if not val:\n",
    "        print(\"The training loss is {} with std:{}. The test loss is {} with std:{}.\".format(loss_train, loss_train_std, loss_test,loss_test_std))\n",
    "    else:\n",
    "        print(\"The training loss is {} with std:{}. The val loss is {} with std:{}.\".format(loss_train, loss_train_std, loss_test,loss_test_std))\n",
    "\n",
    "    return loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute w and calculate its goodness\n",
    "w_ana = get_w_analytical(X_train,y_train)\n",
    "get_loss(w_ana, X_train,y_train, X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2.3 Feature expansion\n",
    "\n",
    "Similar to feature expansion for classification problems, we can also perform feature expansion here. Please complete the function `expand_X` and perform a degree-2 polynomial feature expansion of X, including a bias term but omitting interaction terms.\n",
    "\n",
    "- Is our model still a linear regression model? Why (not)?\n",
    "- How does linear regression on degree-2 polynomially expanded data compare against our previous model? Explain!\n",
    "- Try polynomial feature expansion for different parameters values of $d$. What do you observe? Explain!\n",
    "- Look up the concept of the condition number of a matrix. What does this tell us about the feature-expanded data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_X(X,y,d):\n",
    "    \"\"\"perform degree-d polynomial feature expansion of X, with bias but omitting interaction terms\"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\"\n",
    "\n",
    "    return expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_and_normalize_X(X,d):\n",
    "    \"\"\"\n",
    "    perform degree-d polynomial feature expansion of X, with bias but omitting interaction terms\n",
    "    and normalize them and return mean and std\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\"\n",
    "\n",
    "    return expand, mu, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform polynomial feature expansion\n",
    "d  = 2\n",
    "\n",
    "#normalize the data after expansion\n",
    "X_train_poly,mu_train_poly,std_train_poly = expand_and_normalize_X(X_train,d)\n",
    "X_test_poly  = expand_X(X_test,d)\n",
    "X_test_poly[:,1:]  = (X_test_poly[:,1:]-mu_train_poly)/std_train_poly\n",
    "\n",
    "\n",
    "print(\"The original data has {} features.\".format(X_train.shape[1]))\n",
    "print(\"After degree-{} polynomial feature expansion (with bias, without interaction terms) the data has {} features.\".format(d,X_train_poly.shape[1]))\n",
    "\n",
    "cond_num_before = np.linalg.cond(X_train.T@X_train)\n",
    "cond_num_after = np.linalg.cond(X_train_poly.T@X_train_poly)\n",
    "print(\"The original data X^TX has condition number {}. \\nThe expanded data X^TX has condition number {}.\".format(cond_num_before,cond_num_after))\n",
    "\n",
    "# re-compute w and calculate its goodness\n",
    "w_augm = get_w_analytical(X_train_poly,y_train)\n",
    "get_loss(w_augm, X_train_poly,y_train, X_test_poly,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above exercise, we directly evaluate model on test loss and choose the best degree of polynomial. But test should not be touched until your final model. So to choose best degree we'll use Cross Validation. We're going to K-Fold CV for that. We will use our training set and create K splits of it to choose best degree and finally evaluate on our test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for using kth split as validation set to get loss\n",
    "# and k-1 splits to train our model\n",
    "# k = kth fold\n",
    "# k_fold_ind = all the fold indices\n",
    "# X,Y= train data and labels\n",
    "# degree = degree of polynomial expansion\n",
    "\n",
    "def do_cross_validation(k,k_fold_ind,X,Y,degree=1):\n",
    "    \n",
    "    # use one split as val\n",
    "    val_ind = k_fold_ind[k]\n",
    "    # use k-1 split to train\n",
    "    train_splits = [i for i in range(k_fold_ind.shape[0]) if i is not k]\n",
    "    train_ind = k_fold_ind[train_splits,:].reshape(-1)\n",
    "    \n",
    "    #Get train and val \n",
    "    cv_X_train = X[train_ind,:]\n",
    "    cv_Y_train = Y[train_ind]\n",
    "    cv_X_val = X[val_ind,:]\n",
    "    cv_Y_val = Y[val_ind]\n",
    "\n",
    "    #expand and normalize for degree d\n",
    "    cv_X_train_poly,mu,std = \n",
    "    #apply the normalization using statistics (mean, std) computed on train data\n",
    "    cv_X_val_poly = \n",
    "\n",
    "    \n",
    "    #fit on train set\n",
    "    w = \n",
    "    \n",
    "    #get loss for val\n",
    "    loss_test = get_loss(w,cv_X_train_poly,cv_Y_train,cv_X_val_poly,cv_Y_val,val=True)\n",
    "    return loss_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do 3-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import fold_indices\n",
    "k_fold=3\n",
    "\n",
    "# We create the k_fold splits of the train data and fix this\n",
    "num_train_examples = X_train.shape[0]\n",
    "fold_ind = fold_indices(num_train_examples,k_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import grid_search_cv\n",
    "\n",
    "# put the list of degree values to be evaluated\n",
    "search_degree =\n",
    "params={'degree':search_degree}\n",
    "\n",
    "#call to the grid search function\n",
    "grid_val,grid_val_std = grid_search_cv(params,k_fold,fold_ind,do_cross_validation,X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how the validation score decreases and then increases with degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the best validation score\n",
    "best_score = \n",
    "print('Best val score {}'.format(best_score))\n",
    "\n",
    "#get degree which gives best score\n",
    "best_degree = \n",
    "print('Best val score for degree {}'.format(best_degree))\n",
    "\n",
    "\n",
    "X_train_poly,mu,std = expand_and_normalize_X(X_train,best_degree)\n",
    "w = get_w_analytical(X_train_poly,y_train)\n",
    "X_test_poly = expand_X(X_test,best_degree)\n",
    "X_test_poly[:,1:] =  (X_test_poly[:,1:]-mu)/std\n",
    "\n",
    "get_loss(w,X_train_poly,y_train,X_test_poly,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Numerical solution for linear regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The linear regression model has an analytical solution, but we can also get the weight parameters $w$ numerically, e.g. via stochastic gradient descent. Please use this approach to complete the function `get_w_numerical` below.\n",
    "\n",
    "- How do these results compare against those of the analytical solution? Explain the differences or similarities!\n",
    "- In which cases may it (not) be preferable to use the numerical approach over the analytical solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_numerical(X_train,y_train,X_test_poly,y_test,epochs,lr):\n",
    "    \"\"\"compute the weight parameters w\"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize the weights\n",
    "    w    = np.random.normal(0, 1e-1, X_train.shape[1])\n",
    "    \n",
    "    # iterate a given number of epochs over the training data\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # iterate over each data point\n",
    "        for idx,x_train in enumerate(X_train):\n",
    "            # update the weights\n",
    "            w += \n",
    "            \n",
    "        if epoch % 1000 == 0:\n",
    "            print(f\"Epoch {1000+epoch}/{epochs}\")\n",
    "            get_loss(w, X_train_poly,y_train, X_test_poly,y_test)\n",
    "            \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute w and calculate its goodness\n",
    "X_train_poly,mu,std = expand_and_normalize_X(X_train,best_degree)\n",
    "X_test_poly = expand_X(X_test,best_degree)\n",
    "X_test_poly[:,1:] =  (X_test_poly[:,1:]-mu)/std\n",
    "w_num = get_w_numerical(X_train_poly,y_train,X_test_poly,y_test,15000,8*1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the sklearn implementation of the linear regression model. Please look up the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) to \n",
    "\n",
    "1. instantiate the LinearRegression model\n",
    "2. fit the model to our training data\n",
    "3. evaluate the model on the test data\n",
    "4. and compare the results with our previous outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "\"\"\"\n",
    "Please fill in the required code here\n",
    "\"\"\"\n",
    "    \n",
    "model = \n",
    "\n",
    "y_hat = \n",
    "\n",
    "print('MSE of sklearn linear regression model on test data: ' , metrics.mean_squared_error(y_test,y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in previous section, we would like to do feature expansion to fit the non-linearity of the data, but it soon leads to overfitting. There are different ways to tackle this problem, like getting more data, changing the prediction method, regularization, etc. For the task of regression, we'll add a regularization to our training objective to mitigate this problem. Intutively, regularization restricts the domain from which the values of model parameters are taken, which means that we are biasing our model.  \n",
    "\n",
    "In Ridge Regression, we restrict the $l_2$ norm of the coefficients $\\mathbf{w}$. Our loss function looks as following,\n",
    "\\begin{align}\n",
    "L(\\mathbf{w}) &=\\frac{1}{N}(\\mathbf{y} - \\mathbf{X}\\mathbf{w})^2 + \\frac{\\lambda}{N}\\|\\mathbf{w}\\|^2 \\\\\n",
    "\\nabla L(\\mathbf{w}) &= -\\frac{2}{N}\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\mathbf{w}) + 2\\frac{\\lambda}{N}\\mathbf{w}\n",
    "\\end{align}\n",
    "\n",
    "$\\nabla L(\\mathbf{w}) = 0$ for minimum condition, we get\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{w} &= (\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}\n",
    "\\end{align}\n",
    "\n",
    "dimensions are following: $\\mathbf{w}$ is $D\\times1$; $\\mathbf{y}$ is $N\\times1$; $\\mathbf{X}$ is $N\\times D$; $\\mathbf{I}$ is identity matrix of dimension $D \\times D$ .\n",
    "\n",
    "$\\lambda$ is our penality term, also know as weight decay. By varying its value, we can allow biasing in our model.\n",
    "\n",
    "**Question**:\n",
    "When $\\lambda$ is high, our model is more complex or less?\n",
    "\n",
    "\n",
    "**Question**:\n",
    "How will $\\lambda$ affect inverse condition number of $\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I}$ ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_analytical_with_regularization(X_train,y_train,lmda):\n",
    "    \"\"\"compute the weight parameters w with ridge regression\"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\"\n",
    "    #create lambda matrix \n",
    "    lmda_mat = \n",
    "    # compute w via the normal equation\n",
    "    # np.linalg.solve is more stable than np.linalg.inv\n",
    "    w = \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform polynomial feature expansion\n",
    "d  = \n",
    "\n",
    "#normalize the data after expansion\n",
    "X_train_poly,mu_train_poly,std_train_poly = expand_and_normalize_X(X_train,d)\n",
    "X_test_poly  = expand_X(X_test,d)\n",
    "X_test_poly[:,1:]  = (X_test_poly[:,1:]-mu_train_poly)/std_train_poly\n",
    "\n",
    "\n",
    "print(\"The original data has {} features.\".format(X_train.shape[1]))\n",
    "print(\"After degree-{} polynomial feature expansion (with bias, without interaction terms) the data has {} features.\".format(d,X_train_poly.shape[1]))\n",
    "\n",
    "cond_num_before = np.linalg.cond(X_train.T@X_train)\n",
    "cond_num_after = np.linalg.cond(X_train_poly.T@X_train_poly)\n",
    "print(\"The original data X^TX has condition number {}. \\nThe expanded data X^TX has condition number {}.\".format(cond_num_before,cond_num_after))\n",
    "\n",
    "#choose lambda value\n",
    "lmda = \n",
    "\n",
    "#write the X^TX+\\lambda*I matrix\n",
    "A = \n",
    "cond_num_ridge = np.linalg.cond(A)\n",
    "print(\"The X^TX+lambda*I with lambda:{} has condition number {}\".format(lmda,cond_num_ridge))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the condition number has changed with regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation(CV) is used to choose value of $\\lambda$. As seen in previous exercise, we will use K-fold CV.\n",
    "We will use our training set and create K splits of it to choose best degree and corresponding $\\lambda$ and finally evaluate on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for using kth split as validation set to get accuracy\n",
    "# and k-1 splits to train our model\n",
    "# k = kth fold\n",
    "# k_fold_ind = all the fold indices\n",
    "# X,Y= train data and labels\n",
    "# lmbda= penalty term\n",
    "# degree = degree of polynomial expansion\n",
    "def do_cross_validation_reg(k,k_fold_ind,X,Y,lmda=0,degree=1):\n",
    "    \n",
    "    # use one split as val\n",
    "    val_ind = k_fold_ind[k]\n",
    "    # use k-1 split to train\n",
    "    train_splits = [i for i in range(k_fold_ind.shape[0]) if i is not k]\n",
    "    train_ind = k_fold_ind[train_splits,:].reshape(-1)\n",
    "   \n",
    "    #Get train and val \n",
    "    cv_X_train = X[train_ind,:]\n",
    "    cv_Y_train = Y[train_ind]\n",
    "    cv_X_val = X[val_ind,:]\n",
    "    cv_Y_val = Y[val_ind]\n",
    "    \n",
    "   \n",
    "    #expand and normalize for degree d\n",
    "    cv_X_train_poly,mu,std = \n",
    "\n",
    "    #apply the normalization using statistics (mean, std) computed on train data\n",
    "    cv_X_val_poly = \n",
    "    \n",
    "    #fit on train set using regularised version\n",
    "    w = \n",
    "    \n",
    "    #get loss for val\n",
    "    loss_test = get_loss(w,cv_X_train_poly,cv_Y_train,cv_X_val_poly,cv_Y_val,val=True)\n",
    "    print(loss_test,lmda,degree)\n",
    "    return loss_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do 3-fold CV. We will use same the training data splits as in non regularised case for fairer comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of lambda values to try.. use np.logspace\n",
    "search_lambda = \n",
    "#list of degrees\n",
    "search_degree = \n",
    "\n",
    "params = {'degree':search_degree,'lmda':search_lambda,}\n",
    "k_fold =3\n",
    "#call to the grid search function\n",
    "grid_val,grid_val_std = grid_search_cv(params,k_fold,fold_ind,do_cross_validation_reg,X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import plot_cv_result\n",
    "plot_cv_result(np.log((grid_val.T)),search_lambda,search_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import plot_cv_result\n",
    "plot_cv_result(np.log((grid_val_std.T)),search_lambda,search_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best val score\n",
    "best_score = \n",
    "print(best_score)\n",
    "\n",
    "# params which give best val score\n",
    "\n",
    "best_degree = \n",
    "best_lambda = \n",
    "print('Best score achieved using degree:{} and lambda:{}'.format(best_degree,best_lambda))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate on the test set\n",
    "X_train_poly,mu,std = expand_and_normalize_X(X_train,best_degree)\n",
    "w = get_w_analytical_with_regularization(X_train_poly,y_train,best_lambda)\n",
    "X_test_poly = expand_X(X_test,best_degree)\n",
    "X_test_poly[:,1:] =  (X_test_poly[:,1:]-mu)/std\n",
    "\n",
    "get_loss(w,X_train_poly,y_train,X_test_poly,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How can you interpret the linear regression coefficients?\n",
    "\n",
    "**Question**: Is it good to have coefficients' values close to zero? \n",
    "\n",
    "**Question**: How would you proceed to improve the prediction?\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
