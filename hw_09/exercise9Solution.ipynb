{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1 Introduction\n",
    "\n",
    "Welcome to the ninth practical session of CS233 - Introduction to Machine Learning.  \n",
    "In this exercise class we will start using Machine Learning methods to solve regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Regression Problem\n",
    "\n",
    "Let $f$ be a function $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}^v$ with a data set $\\left(X \\subseteq \\mathbb{R}^d, y\n",
    "\\subseteq\\mathbb{R}^v \\right )$. The regression problem is the task of estimating an approximation $\\hat{f}$ of $f$.\n",
    "Within this exercise we consider the special case of $v=1$, i.e. the problem is univariate as opposed to multivariate.\n",
    "Specifically, we will analyze the Boston house prices data set and predict costs based on properties such as per capita crime rate by town, pupil-teacher ratio by town etc.\n",
    "\n",
    "We will model the given data by means of a linear regression model, i.e. a model that explains a dependent variable in terms of a linear combination of independent variables.\n",
    "\n",
    "- How does a regression problem differ from a classification problem?\n",
    "    - A classification problem has discrete-valued target variables, whereas the values of target variables in regression problems may be continuous.\n",
    "- Why is the linear regression model a linear model? Is it linear in the dependent variables? Is it linear in the parameters?\n",
    "    - The linear regression model is linear in the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Load and inspect data\n",
    "\n",
    "We load the data and split it such that 80% and 20% are train and test data, respectively. \n",
    "After, we normalize the data such that each feature has zero mean and unit standard deviation. Please fill in the required code and complete the function `normalize`.\n",
    "\n",
    "- Explore the relation between different features and the house prices. Describe what you see. Can you identify any trends?\n",
    "    - The relations between feature values and house prices differ between feature dimensions. Some features (e.g. 4) are positively correlated with house prices, some (e.g. 11) are negatively correlated and for many others a clear trend is hard to spot by mere inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the data set and print a description\n",
    "from sklearn.datasets import load_boston\n",
    "boston_dataset = load_boston()\n",
    "print(boston_dataset.DESCR)\n",
    "\n",
    "X = boston_dataset[\"data\"]\n",
    "y = boston_dataset[\"target\"]\n",
    "\n",
    "# remove categorical feature\n",
    "X = np.delete(X, 3, axis=1)\n",
    "# removing second mode\n",
    "ind = y<40\n",
    "X = X[ind,:]\n",
    "y = y[ind]\n",
    "\n",
    "# split the data into 80% training and 20% test data\n",
    "indices = np.arange(X.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "splitRatio = 0.8\n",
    "n          = X.shape[0]\n",
    "X_train    = X[indices[0:int(n*splitRatio)],:] \n",
    "y_train    = y[indices[0:int(n*splitRatio)]] \n",
    "X_test     = X[indices[int(n*(splitRatio)):],:] \n",
    "y_test     = y[indices[int(n*(splitRatio)):]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Make mean 0 and std dev 1 of the data.\n",
    "'''\n",
    "def normalize(X):\n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\"\n",
    "    \n",
    "    mu    = np.mean(X,0,keepdims=True)\n",
    "    std   = np.std(X,0,keepdims=True) \n",
    "    X     = (X-mu)/std\n",
    "    return X, mu, std\n",
    "\n",
    "#Use train stats for normalizing test set\n",
    "X_train,mu_train,std_train = normalize(X_train)\n",
    "X_test = (X_test-mu_train)/std_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Attribute $X_4$ vs Price $y$')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exploratory analysis of the data. Have a look at the distribution of prices vs features\n",
    "\n",
    "feature = 4\n",
    "plt.scatter(X_train[:,feature], y_train)\n",
    "plt.xlabel(f\"Attribute $X_{feature}$\")\n",
    "plt.ylabel(\"Price $y$\")\n",
    "plt.title(f\"Attribute $X_{feature}$ vs Price $y$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Closed-form solution for linear regression\n",
    "\n",
    "The linear regression model has an [analytical solution](https://en.wikipedia.org/wiki/Linear_least_squares). \n",
    "Please use this solution to complete the function `get_w_analytical` and to obtain the weight parameters $w$. Tip: You may want to use the function np.linalg.solve. \n",
    "\n",
    "- What is the time complexity of this approach?\n",
    "    - Let N be the number of data points. The time complexity is determined by the slowest operation, i.e. the matrix inversion, which is in $\\mathcal{O}(N^3)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_analytical(X_train,y_train):\n",
    "    \"\"\"\n",
    "    compute the weight parameters w\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\"\n",
    "        \n",
    "    # compute w via the normal equation\n",
    "    # np.linalg.solve is more stable than np.linalg.inv\n",
    "    w = np.linalg.solve(X_train.T@X_train,X_train.T@y_train)\n",
    "    return w\n",
    "\n",
    "def get_loss(w, X_train, y_train,X_test,y_test,val=False):\n",
    "    # predict dependent variables and MSE loss for seen training data\n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\"\n",
    "    loss_train = (np.mean((y_train-X_train@w)**2))\n",
    "    loss_train_std = np.std((y_train-X_train@w)**2)\n",
    "    \n",
    "    # predict dependent variables and MSE loss for unseen test data\n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\"\n",
    "    loss_test = (np.mean((y_test-X_test@w)**2))\n",
    "    loss_test_std = np.std((y_test-X_test@w)**2)\n",
    "    if not val:\n",
    "        print(\"The training loss is {} with std:{}. The test loss is {} with std:{}.\".format(loss_train, loss_train_std, loss_test,loss_test_std))\n",
    "    else:\n",
    "        print(\"The training loss is {} with std:{}. The val loss is {} with std:{}.\".format(loss_train, loss_train_std, loss_test,loss_test_std))\n",
    "\n",
    "    return loss_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 436.8437185477003 with std:142.73673671586107. The test loss is 437.3294638920944 with std:148.94707784497865.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "437.3294638920944"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute w and calculate its goodness\n",
    "w_ana = get_w_analytical(X_train,y_train)\n",
    "get_loss(w_ana, X_train,y_train, X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2.3 Feature expansion\n",
    "\n",
    "Similar to feature expansion for classification problems, we can also perform feature expansion here. Please complete the function `expand_X` and perform a degree-2 polynomial feature expansion of X, including a bias term but omitting interaction terms.\n",
    "\n",
    "- Is our model still a linear regression model? Why (not)?\n",
    "    - Yes, our model is still a linear regression model because it is still linear in the parameters.\n",
    "- How does linear regression on degree-2 polynomially expanded data compare against our previous model? Explain!\n",
    "    - The model performs better on the degree-2 polynomially expanded data as compared to the previous model.\n",
    "- Try polynomial feature expansion for different parameters values of $d$. What do you observe? Explain!\n",
    "    - The model improves in performance up to degree-5 polynomial feature expansion, after which the training loss still decreases but the test loss increases again. This is due to overfitting.\n",
    "- Look up the concept of the [condition number of a matrix](https://blogs.mathworks.com/cleve/2017/07/17/what-is-the-condition-number-of-a-matrix/). What does this tell us about the feature-expanded data?\n",
    "    - The inverse condition number of a function is a measure of how much the function is sensitive to changes in its input. Functions with a large-condition number are said to be ill-conditioned and may cause our methods to deal with numerical inaccuracies. A polynomial feature expansion of a higher degree results in a larger condition number and will lead to inaccurate inverse calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_X(X,d):\n",
    "    \"\"\"\n",
    "    perform degree-d polynomial feature expansion of X, with bias but omitting interaction terms\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\"\n",
    "    \n",
    "    expand = np.ones((X.shape[0],1))\n",
    "    for idx in range(1,d+1): expand=np.hstack((expand, X**idx))\n",
    "    return expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_and_normalize_X(X,d):\n",
    "    \"\"\"\n",
    "    perform degree-d polynomial feature expansion of X, with bias but omitting interaction terms\n",
    "    and normalize them.\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\"\n",
    "    expand = expand_X(X,d)\n",
    "    expand_withoutBias,mu,std = normalize(expand[:,1:])\n",
    "    expand[:,1:] = expand_withoutBias\n",
    "    return expand, mu, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original data has 12 features.\n",
      "After degree-2 polynomial feature expansion (with bias, without interaction terms) the data has 25 features.\n",
      "The original data X^TX has condition number 94.30540153557189. \n",
      "The expanded data X^TX has condition number 304.6821566696805.\n",
      "The training loss is 8.146544587369405 with std:18.14451706916919. The test loss is 8.577004357720213 with std:13.4943321035368.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.577004357720213"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform polynomial feature expansion\n",
    "d = 2\n",
    "\n",
    "#normalize the data after expansion\n",
    "X_train_poly,mu_train_poly,std_train_poly = expand_and_normalize_X(X_train,d)\n",
    "X_test_poly  = expand_X(X_test,d)\n",
    "X_test_poly[:,1:]  = (X_test_poly[:,1:]-mu_train_poly)/std_train_poly\n",
    "\n",
    "\n",
    "print(\"The original data has {} features.\".format(X_train.shape[1]))\n",
    "print(\"After degree-{} polynomial feature expansion (with bias, without interaction terms) the data has {} features.\".format(d,X_train_poly.shape[1]))\n",
    "\n",
    "cond_num_before = np.linalg.cond(X_train.T@X_train)\n",
    "cond_num_after = np.linalg.cond(X_train_poly.T@X_train_poly)\n",
    "print(\"The original data X^TX has condition number {}. \\nThe expanded data X^TX has condition number {}.\".format(cond_num_before,cond_num_after))\n",
    "\n",
    "# re-compute w and calculate its goodness\n",
    "w_augm = get_w_analytical(X_train_poly,y_train)\n",
    "get_loss(w_augm, X_train_poly,y_train, X_test_poly,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above exercise, we directly evaluate model on test loss and choose the best degree of polynomial. But test should not be touched until your final model. So to choose best degree we'll use Cross Validation. We're going to K-Fold CV for that. We will use our training set and create K splits of it to choose best degree and finally evaluate on our test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for using kth split as validation set to get loss\n",
    "# and k-1 splits to train our model\n",
    "# k = kth fold\n",
    "# k_fold_ind = all the fold indices\n",
    "# X,Y= train data and labels\n",
    "# degree = degree of polynomial expansion\n",
    "\n",
    "def do_cross_validation(k,k_fold_ind,X,Y,degree=1):\n",
    "    \n",
    "    # use one split as val\n",
    "    val_ind = k_fold_ind[k]\n",
    "    # use k-1 split to train\n",
    "    train_splits = [i for i in range(k_fold_ind.shape[0]) if i is not k]\n",
    "    train_ind = k_fold_ind[train_splits,:].reshape(-1)\n",
    "    \n",
    "    #Get train and val \n",
    "    cv_X_train = X[train_ind,:]\n",
    "    cv_Y_train = Y[train_ind]\n",
    "    cv_X_val = X[val_ind,:]\n",
    "    cv_Y_val = Y[val_ind]\n",
    "\n",
    "    #expand and normalize for degree d\n",
    "    cv_X_train_poly,mu,std = expand_and_normalize_X(cv_X_train,degree)\n",
    "    #apply the normalization using statistics (mean, std) computed on train data\n",
    "    cv_X_val_poly = expand_X(cv_X_val,degree)\n",
    "    cv_X_val_poly[:,1:] =  (cv_X_val_poly[:,1:]-mu)/std\n",
    "    \n",
    "    #fit on train set\n",
    "    w = get_w_analytical(cv_X_train_poly,cv_Y_train)\n",
    "    \n",
    "    #get loss for val\n",
    "    loss_test = get_loss(w,cv_X_train_poly,cv_Y_train,cv_X_val_poly,cv_Y_val,val=True)\n",
    "    return loss_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do 3-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import fold_indices\n",
    "k_fold=3\n",
    "\n",
    "# We create the k_fold splits of the train data\n",
    "num_train_examples = X_train.shape[0]\n",
    "fold_ind = fold_indices(num_train_examples,k_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating for {'degree': 1} ...\n",
      "The training loss is 9.878733332550935 with std:18.37782618122855. The val loss is 12.097836086985499 with std:17.127104767852995.\n",
      "The training loss is 11.399153182137644 with std:21.252737552838692. The val loss is 8.642850525165404 with std:13.217410675430829.\n",
      "The training loss is 9.647515495078757 with std:14.163330615094118. The val loss is 12.361915261420615 with std:29.5120132938108.\n",
      "Evaluating for {'degree': 2} ...\n",
      "The training loss is 6.704434095012012 with std:13.04290453803446. The val loss is 15.33226377128941 with std:59.194356689515615.\n",
      "The training loss is 8.59812312387266 with std:19.20966159118811. The val loss is 7.99215021511865 with std:12.953280974968997.\n",
      "The training loss is 8.295107830537537 with std:13.781793734479015. The val loss is 8.952202411127612 with std:18.538578865584697.\n",
      "Evaluating for {'degree': 3} ...\n",
      "The training loss is 6.016961518016398 with std:12.98640668859862. The val loss is 219.9763051196828 with std:2147.199387698197.\n",
      "The training loss is 6.61059467066222 with std:11.603390465787877. The val loss is 9.208393316317428 with std:15.817117629028836.\n",
      "The training loss is 6.647726826379213 with std:9.462305742327297. The val loss is 8.79045927610683 with std:21.62653887360139.\n",
      "Evaluating for {'degree': 4} ...\n",
      "The training loss is 5.4892943798361244 with std:12.80478145667062. The val loss is 74.51757309120002 with std:726.1004842259258.\n",
      "The training loss is 5.473830379191732 with std:9.859770792406268. The val loss is 8.932445276085183 with std:15.892676072038716.\n",
      "The training loss is 4.760151294169597 with std:7.517319364401974. The val loss is 11.104276197433602 with std:30.667662527594764.\n",
      "Evaluating for {'degree': 5} ...\n",
      "The training loss is 4.803302814257727 with std:11.09802369989356. The val loss is 86692.64546040651 with std:952719.7229907172.\n",
      "The training loss is 4.710971731988219 with std:8.919907269774212. The val loss is 9.595861768063715 with std:17.232314947977986.\n",
      "The training loss is 4.364725903525293 with std:7.272421429102086. The val loss is 10.17497671793749 with std:29.348416108333794.\n",
      "Evaluating for {'degree': 6} ...\n",
      "The training loss is 4.174160725676965 with std:9.351346901044497. The val loss is 2110289.5464206506 with std:23354426.74003958.\n",
      "The training loss is 3.8883200040271553 with std:7.936795431717853. The val loss is 9.056525080448516 with std:17.384060610674155.\n",
      "The training loss is 3.379186432774632 with std:5.196306323036411. The val loss is 9.325098990059624 with std:24.10104216689044.\n",
      "Evaluating for {'degree': 7} ...\n",
      "The training loss is 3.454942949690028 with std:8.09768859407402. The val loss is 71169463.2709065 with std:792971783.1498725.\n",
      "The training loss is 3.5035278392126625 with std:6.557384389295983. The val loss is 38.339813897351924 with std:222.6762030708114.\n",
      "The training loss is 3.121985399006722 with std:5.09853143125587. The val loss is 12.932961969363047 with std:47.35118119855683.\n",
      "Evaluating for {'degree': 8} ...\n",
      "The training loss is 3.0808232427325684 with std:6.694596687028538. The val loss is 2177096439.794818 with std:24310024387.982788.\n",
      "The training loss is 2.888023540483583 with std:5.179397949745349. The val loss is 68.07150823982161 with std:404.28621352492235.\n",
      "The training loss is 2.9034889407272138 with std:5.091025381734397. The val loss is 25.561810641714263 with std:121.0406376894253.\n",
      "Evaluating for {'degree': 9} ...\n",
      "The training loss is 2.7576123809317057 with std:6.435594425393558. The val loss is 6835134455.083041 with std:76406820853.24263.\n",
      "The training loss is 2.5375712060184914 with std:4.745735399383841. The val loss is 458.06032342458417 with std:2813.9430653569925.\n",
      "The training loss is 2.3815234183897362 with std:4.70479167558011. The val loss is 106.17589221237208 with std:562.4954939770727.\n",
      "Evaluating for {'degree': 10} ...\n",
      "The training loss is 2.4271811717738125 with std:5.90589984052625. The val loss is 377997670633.3835 with std:4225806628382.2393.\n",
      "The training loss is 2.053433411907226 with std:3.712431430948495. The val loss is 973.665490642278 with std:5238.391831677754.\n",
      "The training loss is 2.08220448562441 with std:4.27134537244911. The val loss is 104.4430927204144 with std:579.4592649392137.\n",
      "Evaluating for {'degree': 11} ...\n",
      "The training loss is 2.267903183352421 with std:4.8943376545395605. The val loss is 514531197557593.1 with std:5751707957778364.0.\n",
      "The training loss is 2.045958995994062 with std:3.640980949487254. The val loss is 1686.7277351210726 with std:9575.741935411643.\n",
      "The training loss is 2.0236632652235325 with std:4.04684670233468. The val loss is 545.4338601434469 with std:2938.0308379107205.\n",
      "Evaluating for {'degree': 12} ...\n",
      "The training loss is 2.2052898940596473 with std:4.662254146806866. The val loss is 890688454655504.4 with std:9956634675520458.0.\n",
      "The training loss is 1.8537073595190403 with std:3.3808259856009304. The val loss is 290795.241569859 with std:2228274.549813424.\n",
      "The training loss is 1.8219687006778027 with std:3.779985421290979. The val loss is 563.5112517093773 with std:2601.605186857718.\n",
      "Evaluating for {'degree': 13} ...\n",
      "The training loss is 2.0176882830300844 with std:4.409069293961505. The val loss is 1.9120918683885404e+16 with std:2.1377317890793005e+17.\n",
      "The training loss is 1.7965490740219765 with std:3.5616262678889585. The val loss is 2066937.6370844753 with std:16137645.170576556.\n",
      "The training loss is 1.7750604948177584 with std:3.5995784792952956. The val loss is 12842.92966358202 with std:82693.5450229025.\n",
      "Evaluating for {'degree': 14} ...\n",
      "The training loss is 1.8616210480697366 with std:3.9132814753368113. The val loss is 1.8426791366920772e+16 with std:2.0599617187854138e+17.\n",
      "The training loss is 1.6960458106349987 with std:3.60520372731727. The val loss is 1989237.8823171104 with std:14751945.290853957.\n",
      "The training loss is 2.2670273736250106 with std:4.330048785668454. The val loss is 2229494.26314388 with std:21880012.987683468.\n"
     ]
    }
   ],
   "source": [
    "from helper import grid_search_cv\n",
    "\n",
    "# put the list of degree values to be evaluated\n",
    "search_degree = np.arange(1,15)\n",
    "params={'degree':search_degree}\n",
    "\n",
    "#call to the grid search function\n",
    "grid_val,grid_val_std = grid_search_cv(params,k_fold,fold_ind,do_cross_validation,X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how the validation score decreases and then increases with degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best val score 10.758872132511891\n",
      "Best val score for degree 2\n",
      "The training loss is 8.146544587369405 with std:18.14451706916919. The test loss is 8.577004357720213 with std:13.4943321035368.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.577004357720213"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the best validation score\n",
    "best_score = np.min(grid_val)\n",
    "print('Best val score {}'.format(best_score))\n",
    "\n",
    "#get degree which gives best score\n",
    "best_degree = search_degree[np.argmin(grid_val)]\n",
    "print('Best val score for degree {}'.format(best_degree))\n",
    "\n",
    "\n",
    "X_train_poly,mu,std = expand_and_normalize_X(X_train,best_degree)\n",
    "w = get_w_analytical(X_train_poly,y_train)\n",
    "X_test_poly = expand_X(X_test,best_degree)\n",
    "X_test_poly[:,1:] =  (X_test_poly[:,1:]-mu)/std\n",
    "\n",
    "get_loss(w,X_train_poly,y_train,X_test_poly,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Numerical solution for linear regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The linear regression model has an analytical solution, but we can also get the weight parameters $w$ numerically, e.g. via stochastic gradient descent. Please use this approach to complete the function `get_w_numerical` below.\n",
    "\n",
    "- How do these results compare against those of the analytical solution? Explain the differences or similarities!\n",
    "    - The analytical solution and the numerical solution are almost identical. But when condition number of matrix is\n",
    "    very high inverse may not be stable and solution will differ.\n",
    "- In which cases, it maybe be preferable to use the numerical approach over the analytical solution?\n",
    "    - Let N be the number of data points. Computing the analytical solution has run time $\\mathcal{O}(N^3)$, which may be problematic if N is very large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_numerical(X_train,y_train,X_test_poly,y_test,epochs,lr):\n",
    "    \"\"\"compute the weight parameters w\"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize the weights\n",
    "    w    = np.random.normal(0, 1e-1, X_train.shape[1])\n",
    "    # define the gradient\n",
    "    grad = lambda w,x,y: (y-x@w)*x\n",
    "    \n",
    "    # iterate a given number of epochs over the training data\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # iterate over each data point\n",
    "        for idx,x_train in enumerate(X_train):\n",
    "            # update the weights\n",
    "            w += lr*grad(w,x_train,y_train[idx])\n",
    "            \n",
    "        if epoch % 1000 == 0:\n",
    "            print(f\"Epoch {1000+epoch}/{epochs}\")\n",
    "            get_loss(w, X_train_poly,y_train, X_test_poly,y_test)\n",
    "            \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/15000\n",
      "The training loss is 438.82197783111553 with std:269.70024898557244. The test loss is 487.795755682178 with std:288.0750855084646.\n",
      "Epoch 2000/15000\n",
      "The training loss is 8.236056562702556 with std:18.501123209527982. The test loss is 9.137317350004377 with std:14.55992640535479.\n",
      "Epoch 3000/15000\n",
      "The training loss is 8.16167686880121 with std:18.180565642719948. The test loss is 8.822776407838896 with std:14.025521070509464.\n",
      "Epoch 4000/15000\n",
      "The training loss is 8.149433149908623 with std:18.106098455220362. The test loss is 8.687750036374553 with std:13.741249494747919.\n",
      "Epoch 5000/15000\n",
      "The training loss is 8.147175969113508 with std:18.08255403686697. The test loss is 8.629587660944399 with std:13.607602247208229.\n",
      "Epoch 6000/15000\n",
      "The training loss is 8.146733506632511 with std:18.074171654432142. The test loss is 8.604089986700975 with std:13.546077110524106.\n",
      "Epoch 7000/15000\n",
      "The training loss is 8.146642412498348 with std:18.07097230682923. The test loss is 8.592811783189845 with std:13.518056881839968.\n",
      "Epoch 8000/15000\n",
      "The training loss is 8.146622452296937 with std:18.069691338158993. The test loss is 8.587802614398301 with std:13.505385378018985.\n",
      "Epoch 9000/15000\n",
      "The training loss is 8.146617636083846 with std:18.06916070937. The test loss is 8.58557354698758 with std:13.499682139050417.\n",
      "Epoch 10000/15000\n",
      "The training loss is 8.146616303337446 with std:18.06893558308592. The test loss is 8.584580678157014 with std:13.49712330275415.\n",
      "Epoch 11000/15000\n",
      "The training loss is 8.146615872757668 with std:18.068838483167. The test loss is 8.584138207871659 with std:13.49597761619587.\n",
      "Epoch 12000/15000\n",
      "The training loss is 8.146615713886503 with std:18.068796131753146. The test loss is 8.583940960149839 with std:13.495465332402217.\n",
      "Epoch 13000/15000\n",
      "The training loss is 8.146615649809972 with std:18.068777520398932. The test loss is 8.58385301126355 with std:13.495236463491345.\n",
      "Epoch 14000/15000\n",
      "The training loss is 8.146615622636013 with std:18.068769300526963. The test loss is 8.583813790785237 with std:13.495134268372222.\n",
      "Epoch 15000/15000\n",
      "The training loss is 8.146615610812333 with std:18.068765658008225. The test loss is 8.58379629866519 with std:13.495088651298861.\n"
     ]
    }
   ],
   "source": [
    "# compute w and calculate its goodness\n",
    "X_train_poly,mu,std = expand_and_normalize_X(X_train,best_degree)\n",
    "X_test_poly = expand_X(X_test,best_degree)\n",
    "X_test_poly[:,1:] =  (X_test_poly[:,1:]-mu)/std\n",
    "w_num = get_w_numerical(X_train_poly,y_train,X_test_poly,y_test,15000,8*1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the sklearn implementation of the linear regression model. Please look up the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) to \n",
    "\n",
    "1. instantiate the LinearRegression model\n",
    "2. fit the model to our training data\n",
    "3. evaluate the model on the test data\n",
    "4. and compare the results with our previous outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of sklearn linear regression model on test data:  8.577004357720154\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "\"\"\"\n",
    "Please fill in the required code here\n",
    "\"\"\"\n",
    "    \n",
    "model = LinearRegression()\n",
    "model.fit(X_train_poly,y_train)\n",
    "y_hat = model.predict(X_test_poly)\n",
    "\n",
    "print('MSE of sklearn linear regression model on test data: ' , metrics.mean_squared_error(y_test,y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Ridge Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in previous section, we would like to do feature expansion to fit the non-linearity of the data, but it soon leads to overfitting. There are different ways to tackle this problem, like getting more data, changing the prediction method, regularization, etc. For the task of regression, we'll add a regularization to our training objective to mitigate this problem. Intutively, regularization restricts the domain from which the values of model parameters are taken, which means that we are biasing our model.  \n",
    "\n",
    "In Ridge Regression, we restrict the $l_2$ norm of the coefficients $\\mathbf{w}$. Our loss function looks as following,\n",
    "\\begin{align}\n",
    "L(\\mathbf{w}) &=\\frac{1}{N} \\| \\mathbf{y} - \\mathbf{X}\\mathbf{w} \\|^2 + \\frac{\\lambda}{N}\\|\\mathbf{w}\\|^2 \\\\\n",
    "\\nabla L(\\mathbf{w}) &= -\\frac{2}{N}\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\mathbf{w}) + 2\\frac{\\lambda}{N}\\mathbf{w}\n",
    "\\end{align}\n",
    "\n",
    "$\\nabla L(\\mathbf{w}) = 0$ for minimum condition, we get\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{w} &= (\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}\n",
    "\\end{align}\n",
    "\n",
    "dimensions are following: $\\mathbf{w}$ is $D\\times1$; $\\mathbf{y}$ is $N\\times1$; $\\mathbf{X}$ is $N\\times D$; $\\mathbf{I}$ is identity matrix of dimension $D \\times D$ .\n",
    "\n",
    "$\\lambda$ is our penality term, also know as weight decay. By varying its value, we can allow biasing in our model.\n",
    "\n",
    "**Question**:\n",
    "When $\\lambda$ is high, our model is more complex or less?\n",
    "\n",
    "**Answer**:\n",
    "High $\\lambda$ penalises the norm term more, hence restricts the value $\\mathbf{w}$ can have, rendering simpler model.\n",
    "\n",
    "**Question**:\n",
    "How will $\\lambda$ affect inverse condition number of $\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I}$ ?\n",
    "\n",
    "**Answer**:\n",
    "$\\lambda > 0$, decreases the condition number, hence more stable computations.\n",
    "\\begin{align}\n",
    "\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I} &= \\mathbf{U}\\mathbf{S}\\mathbf{U}^T + \\lambda \\mathbf{U}\\mathbf{I}\\mathbf{U}^T \\\\\n",
    "\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I} &= \\mathbf{U}[\\mathbf{S}+\\lambda\\mathbf{I}]\\mathbf{U}^T\n",
    "\\end{align}\n",
    "if we have singular decomposition of $\\mathbf{X}^T\\mathbf{X}$ as $\\mathbf{U}\\mathbf{S}\\mathbf{U}^T$, then the above steps show that eigenvalues of $\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I}$ are atleast $\\lambda$, hence we are lifting the all the eigenvalues. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_analytical_with_regularization(X_train,y_train,lmda):\n",
    "    \"\"\"compute the weight parameters w with ridge regression\"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\"\n",
    "    #create lambda matrix \n",
    "    lmda_mat = lmda*np.eye(X_train.shape[1])\n",
    "    # compute w via the normal equation\n",
    "    # np.linalg.solve is more stable than np.linalg.inv\n",
    "    w = np.linalg.solve(X_train.T@X_train+lmda_mat,X_train.T@y_train)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original data has 12 features.\n",
      "After degree-14 polynomial feature expansion (with bias, without interaction terms) the data has 169 features.\n",
      "The original data X^TX has condition number 94.30540153557189. \n",
      "The expanded data X^TX has condition number 6.119085746825847e+17.\n",
      "The X^TX+lambda*I with lambda:2 has condition number 87.29222508026365\n"
     ]
    }
   ],
   "source": [
    "# perform polynomial feature expansion\n",
    "d  = 14\n",
    "\n",
    "#normalize the data after expansion\n",
    "X_train_poly,mu_train_poly,std_train_poly = expand_and_normalize_X(X_train,d)\n",
    "X_test_poly  = expand_X(X_test,d)\n",
    "X_test_poly[:,1:]  = (X_test_poly[:,1:]-mu_train_poly)/std_train_poly\n",
    "\n",
    "\n",
    "print(\"The original data has {} features.\".format(X_train.shape[1]))\n",
    "print(\"After degree-{} polynomial feature expansion (with bias, without interaction terms) the data has {} features.\".format(d,X_train_poly.shape[1]))\n",
    "\n",
    "cond_num_before = np.linalg.cond(X_train.T@X_train)\n",
    "cond_num_after = np.linalg.cond(X_train_poly.T@X_train_poly)\n",
    "print(\"The original data X^TX has condition number {}. \\nThe expanded data X^TX has condition number {}.\".format(cond_num_before,cond_num_after))\n",
    "\n",
    "#choose lambda value\n",
    "lmda = 2\n",
    "\n",
    "#write the X^TX+\\lambda*I matrix\n",
    "A = X_train.T@X_train+lmda*np.eye(X_train.shape[1])\n",
    "cond_num_ridge = np.linalg.cond(A)\n",
    "print(\"The X^TX+lambda*I with lambda:{} has condition number {}\".format(lmda,cond_num_ridge))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the condition number has changed with regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation(CV) is used to choose value of $\\lambda$. As seen in previous exercise, we will use K-fold CV.\n",
    "We will use our training set and create K splits of it to choose best degree and corresponding $\\lambda$ and finally evaluate on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for using kth split as validation set to get accuracy\n",
    "# and k-1 splits to train our model\n",
    "def do_cross_validation_reg(k,k_fold_ind,X,Y,lmda=0,degree=1):\n",
    "    \n",
    "    # use one split as val\n",
    "    val_ind = k_fold_ind[k]\n",
    "    # use k-1 split to train\n",
    "    train_splits = [i for i in range(k_fold_ind.shape[0]) if i is not k]\n",
    "    train_ind = k_fold_ind[train_splits,:].reshape(-1)\n",
    "   \n",
    "    #Get train and val \n",
    "    cv_X_train = X[train_ind,:]\n",
    "    cv_Y_train = Y[train_ind]\n",
    "    cv_X_val = X[val_ind,:]\n",
    "    cv_Y_val = Y[val_ind]\n",
    "    \n",
    "   \n",
    "    #expand and normalize for degree d\n",
    "    cv_X_train_poly,mu,std = expand_and_normalize_X(cv_X_train,degree)\n",
    "\n",
    "    #apply the normalization using statistics (mean, std) computed on train data\n",
    "    cv_X_val_poly = expand_X(cv_X_val,degree)\n",
    "    cv_X_val_poly[:,1:] =  (cv_X_val_poly[:,1:]-mu)/std\n",
    "    \n",
    "    #fit on train set using regularised version\n",
    "    w = get_w_analytical_with_regularization(cv_X_train_poly,cv_Y_train,lmda)\n",
    "    \n",
    "    #get loss for val\n",
    "    loss_test = get_loss(w,cv_X_train_poly,cv_Y_train,cv_X_val_poly,cv_Y_val,val=True)\n",
    "    print(loss_test,lmda,degree)\n",
    "    return loss_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do 3-fold CV. We will use same the training data splits as in non regularised case for fairer comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating for {'degree': 1, 'lmda': 0.01} ...\n",
      "The training loss is 9.878734525447705 with std:18.38119705319961. The val loss is 12.097042010817713 with std:17.12823906312638.\n",
      "12.097042010817713 0.01 1\n",
      "The training loss is 11.399154129861566 with std:21.256533730574343. The val loss is 8.643007248451504 with std:13.217445759846907.\n",
      "8.643007248451504 0.01 1\n",
      "The training loss is 9.647516425595219 with std:14.164703865164896. The val loss is 12.362239217481456 with std:29.51765349897477.\n",
      "12.362239217481456 0.01 1\n",
      "Evaluating for {'degree': 1, 'lmda': 0.013043213867190054} ...\n",
      "The training loss is 9.878735361495497 with std:18.382222995873544. The val loss is 12.09680115503232 with std:17.128585342953304.\n",
      "12.09680115503232 0.013043213867190054 1\n",
      "The training loss is 11.399154794290626 with std:21.257689165045775. The val loss is 8.643055262867495 with std:13.217456971916642.\n",
      "8.643055262867495 0.013043213867190054 1\n",
      "The training loss is 9.64751707799271 with std:14.16512228786205. The val loss is 12.362338138238242 with std:29.519369769040463.\n",
      "12.362338138238242 0.013043213867190054 1\n",
      "Evaluating for {'degree': 1, 'lmda': 0.017012542798525893} ...\n",
      "The training loss is 9.87873678324588 with std:18.383561234855193. The val loss is 12.096487560709871 with std:17.129037765448025.\n",
      "12.096487560709871 0.017012542798525893 1\n",
      "The training loss is 11.399155924450753 with std:21.25919634648909. The val loss is 8.643118113493331 with std:13.217471971164224.\n",
      "8.643118113493331 0.017012542798525893 1\n",
      "The training loss is 9.647518187726961 with std:14.165668405289736. The val loss is 12.36246739682643 with std:29.52160822111885.\n",
      "12.36246739682643 0.017012542798525893 1\n",
      "Evaluating for {'degree': 1, 'lmda': 0.02218982341458972} ...\n",
      "The training loss is 9.878739200720693 with std:18.38530686457416. The val loss is 12.096079482795053 with std:17.12962916458133.\n",
      "12.096079482795053 0.02218982341458972 1\n",
      "The training loss is 11.399157846691217 with std:21.261162405490037. The val loss is 8.64320047251388 with std:13.217492172960497.\n",
      "8.64320047251388 0.02218982341458972 1\n",
      "The training loss is 9.64752007531279 with std:14.166381329696465. The val loss is 12.362636389773163 with std:29.524527685280525.\n",
      "12.362636389773163 0.02218982341458972 1\n",
      "Evaluating for {'degree': 1, 'lmda': 0.028942661247167517} ...\n",
      "The training loss is 9.878743310615796 with std:18.387583957838476. The val loss is 12.095548831385633 with std:17.130402737281784.\n",
      "12.095548831385633 0.028942661247167517 1\n",
      "The training loss is 11.399161115915117 with std:21.263727135172374. The val loss is 8.643308544021044 with std:13.2175196074955.\n",
      "8.643308544021044 0.028942661247167517 1\n",
      "The training loss is 9.64752328579067 with std:14.167312252421192. The val loss is 12.362857488097356 with std:29.528335270163602.\n",
      "12.362857488097356 0.028942661247167517 1\n",
      "Evaluating for {'degree': 1, 'lmda': 0.037750532053243954} ...\n",
      "The training loss is 9.878750296295943 with std:18.390554411820105. The val loss is 12.094859430308427 with std:17.13141545412499.\n",
      "12.094859430308427 0.037750532053243954 1\n",
      "The training loss is 11.399166675489381 with std:21.267072973201554. The val loss is 8.643450607165644 with std:13.217557235691286.\n",
      "8.643450607165644 0.037750532053243954 1\n",
      "The training loss is 9.647528745888462 with std:14.168528243119912. The val loss is 12.363147022944636 with std:29.533301015378125.\n",
      "12.363147022944636 0.037750532053243954 1\n",
      "Evaluating for {'degree': 1, 'lmda': 0.04923882631706739} ...\n",
      "The training loss is 9.878762166785355 with std:18.394429504952722. The val loss is 12.093964876495292 with std:17.132742684519812.\n",
      "12.093964876495292 0.04923882631706739 1\n",
      "The training loss is 11.399176128850657 with std:21.271438050720448. The val loss is 8.64363777830268 with std:13.21760945123658.\n",
      "8.64363777830268 0.04923882631706739 1\n",
      "The training loss is 9.647538031044142 with std:14.170117291160455. The val loss is 12.363526627382258 with std:29.539776975103536.\n",
      "12.363526627382258 0.04923882631706739 1\n",
      "Evaluating for {'degree': 1, 'lmda': 0.0642232542222936} ...\n",
      "The training loss is 9.878782330722151 with std:18.39948500271863. The val loss is 12.092805967849811 with std:17.13448452539098.\n",
      "12.092805967849811 0.0642232542222936 1\n",
      "The training loss is 11.399192200600488 with std:21.277133261632436. The val loss is 8.643885096109283 with std:13.217682888066298.\n",
      "8.643885096109283 0.0642232542222936 1\n",
      "The training loss is 9.647553818906733 with std:14.17219502691321. The val loss is 12.364025081923351 with std:29.548222061930858.\n",
      "12.364025081923351 0.0642232542222936 1\n",
      "Evaluating for {'degree': 1, 'lmda': 0.0837677640068292} ...\n",
      "The training loss is 9.87881656675227 with std:18.406080914122562. The val loss is 12.091307717009828 with std:17.136774549689047.\n",
      "12.091307717009828 0.0837677640068292 1\n",
      "The training loss is 11.399219518794181 with std:21.2845646094035. The val loss is 8.644213091091487 with std:13.21778773237121.\n",
      "8.644213091091487 0.0837677640068292 1\n",
      "The training loss is 9.647580659150153 with std:14.174913735039102. The val loss is 12.364680883451282 with std:29.55923437090689.\n",
      "12.364680883451282 0.0837677640068292 1\n",
      "Evaluating for {'degree': 1, 'lmda': 0.10926008611173785} ...\n",
      "The training loss is 9.878874661178413 with std:18.414687350585865. The val loss is 12.089376071520324 with std:17.13979203657799.\n",
      "12.089376071520324 0.10926008611173785 1\n",
      "The training loss is 11.399265941077603 with std:21.29426249374124. The val loss is 8.644650093440072 with std:13.217939872122177.\n",
      "8.644650093440072 0.10926008611173785 1\n",
      "The training loss is 9.647626279358088 with std:14.178474527329751. The val loss is 12.365545869986098 with std:29.57359320420813.\n",
      "12.365545869986098 0.10926008611173785 1\n",
      "Evaluating for {'degree': 1, 'lmda': 0.14251026703029984} ...\n",
      "The training loss is 9.878973165164519 with std:18.425918404953272. The val loss is 12.086894656525619 with std:17.143779272334218.\n",
      "12.086894656525619 0.14251026703029984 1\n",
      "The training loss is 11.399344800384188 with std:21.30692014263285. The val loss is 8.645235683352919 with std:13.218164441819273.\n",
      "8.645235683352919 0.14251026703029984 1\n",
      "The training loss is 9.647703798378037 with std:14.183143922490274. The val loss is 12.366690412280345 with std:29.592313653795504.\n",
      "12.366690412280345 0.14251026703029984 1\n",
      "Evaluating for {'degree': 1, 'lmda': 0.18587918911465645} ...\n",
      "The training loss is 9.879140022895761 with std:18.440576591495876. The val loss is 12.083722206800044 with std:17.149066314538793.\n",
      "12.083722206800044 0.18587918911465645 1\n",
      "The training loss is 11.399478702672981 with std:21.32344412712286. The val loss is 8.646025932739324 with std:13.218501696915546.\n",
      "8.646025932739324 0.18587918911465645 1\n",
      "The training loss is 9.647835473592751 with std:14.189276652816218. The val loss is 12.368210968741554 with std:29.616717403480788.\n",
      "12.368210968741554 0.18587918911465645 1\n",
      "Evaluating for {'degree': 1, 'lmda': 0.24244620170823283} ...\n",
      "The training loss is 9.879422310621583 with std:18.4597112253224. The val loss is 12.079691957763153 with std:17.1561068284557.\n",
      "12.079691957763153 0.24244620170823283 1\n",
      "The training loss is 11.399705938218313 with std:21.34502088780339. The val loss is 8.647101487330062 with std:13.219016782609947.\n",
      "8.647101487330062 0.24244620170823283 1\n",
      "The training loss is 9.648059036109244 with std:14.197347390931036. The val loss is 12.370241260203816 with std:29.648524415895725.\n",
      "12.370241260203816 0.24244620170823283 1\n",
      "Evaluating for {'degree': 1, 'lmda': 0.31622776601683794} ...\n",
      "The training loss is 9.879899115563203 with std:18.484695251013555. The val loss is 12.074615264118032 with std:17.165530419813546.\n",
      "12.074615264118032 0.31622776601683794 1\n",
      "The training loss is 11.400091281213555 with std:21.373204551931796. The val loss is 8.64858019242956 with std:13.219816012646232.\n",
      "8.64858019242956 0.31622776601683794 1\n",
      "The training loss is 9.648438381546084 with std:14.207995433391924. The val loss is 12.37296906607291 with std:29.689971404846435.\n",
      "12.37296906607291 0.31622776601683794 1\n",
      "Evaluating for {'degree': 1, 'lmda': 0.41246263829013524} ...\n",
      "The training loss is 9.880702840881087 with std:18.517326566871407. The val loss is 12.068293312384974 with std:17.178219536503253.\n",
      "12.068293312384974 0.41246263829013524 1\n",
      "The training loss is 11.400744130801044 with std:21.41003316412371. The val loss is 8.650637033433663 with std:13.221074018320992.\n",
      "8.650637033433663 0.41246263829013524 1\n",
      "The training loss is 9.649081568998946 with std:14.222088456591237. The val loss is 12.37666185304966 with std:29.743964480635466.\n",
      "12.37666185304966 0.41246263829013524 1\n",
      "Evaluating for {'degree': 1, 'lmda': 0.5379838403443686} ...\n",
      "The training loss is 9.882054194243398 with std:18.55996198954772. The val loss is 12.060543254242372 with std:17.19542274843204.\n",
      "12.060543254242372 0.5379838403443686 1\n",
      "The training loss is 11.401848878507078 with std:21.458182974202963. The val loss is 8.653535898969642 with std:13.223079006354462.\n",
      "8.653535898969642 0.5379838403443686 1\n",
      "The training loss is 9.650171034788377 with std:14.240814679494214. The val loss is 12.381706412061709 with std:29.81427509224056.\n",
      "12.381706412061709 0.5379838403443686 1\n",
      "Evaluating for {'degree': 1, 'lmda': 0.701703828670383} ...\n",
      "The training loss is 9.884319154636282 with std:18.615694886453703. The val loss is 12.051248723733528 with std:17.218921250700305.\n",
      "12.051248723733528 0.701703828670383 1\n",
      "The training loss is 11.403715525515512 with std:21.521173858373185. The val loss is 8.657680483600997 with std:13.226308075831236.\n",
      "8.657680483600997 0.701703828670383 1\n",
      "The training loss is 9.652014123279706 with std:14.265817733050936. The val loss is 12.388670861508253 with std:29.905790324724123.\n",
      "12.388670861508253 0.701703828670383 1\n",
      "Evaluating for {'degree': 1, 'lmda': 0.9152473108773893} ...\n",
      "The training loss is 9.888100811691087 with std:18.688591491114295. The val loss is 12.040449840357613 with std:17.25127183230645.\n",
      "12.040449840357613 0.9152473108773893 1\n",
      "The training loss is 11.40686361875169 with std:21.60364357627838. The val loss is 8.66369615959223 with std:13.231552176770135.\n",
      "8.66369615959223 0.9152473108773893 1\n",
      "The training loss is 9.655127211082135 with std:14.299396116640795. The val loss is 12.398402505554882 with std:30.024830606098448.\n",
      "12.398402505554882 0.9152473108773893 1\n",
      "Evaluating for {'degree': 1, 'lmda': 1.1937766417144369} ...\n",
      "The training loss is 9.894385854621593 with std:18.78400648087296. The val loss is 12.028494767204858 with std:17.29615715647743.\n",
      "12.028494767204858 1.1937766417144369 1\n",
      "The training loss is 11.412160594798088 with std:21.71171467998793. The val loss is 8.672561842355313 with std:13.240122490287375.\n",
      "8.672561842355313 1.1937766417144369 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 9.660374979645194 with std:14.344800486922558. The val loss is 12.412183247086817 with std:30.17954965849949.\n",
      "12.412183247086817 1.1937766417144369 1\n",
      "Evaluating for {'degree': 1, 'lmda': 1.5570684047537318} ...\n",
      "The training loss is 9.90477579423533 with std:18.909006233502605. The val loss is 12.016284157924519 with std:17.35888285425784.\n",
      "12.016284157924519 1.5570684047537318 1\n",
      "The training loss is 11.421048227041178 with std:21.853485705637496. The val loss is 8.685822268539027 with std:13.254189155038043.\n",
      "8.685822268539027 1.5570684047537318 1\n",
      "The training loss is 9.669199463061226 with std:14.406678578220973. The val loss is 12.431977269491853 with std:30.380432610149537.\n",
      "12.431977269491853 1.5570684047537318 1\n",
      "Evaluating for {'degree': 1, 'lmda': 2.030917620904737} ...\n",
      "The training loss is 9.92184877324301 with std:19.072939271928462. The val loss is 12.00565266829268 with std:17.447071464165457.\n",
      "12.00565266829268 2.030917620904737 1\n",
      "The training loss is 11.435910501367784 with std:22.039687746285647. The val loss is 8.705929068075237 with std:13.27733256978534.\n",
      "8.705929068075237 2.030917620904737 1\n",
      "The training loss is 9.683993851355623 with std:14.49174060258699. The val loss is 12.460826093516244 with std:30.640907812668825.\n",
      "12.460826093516244 2.030917620904737 1\n",
      "Evaluating for {'degree': 1, 'lmda': 2.6489692876105297} ...\n",
      "The training loss is 9.949723016987859 with std:19.28820889944564. The val loss is 11.999951987269263 with std:17.57161964817342.\n",
      "11.999951987269263 2.6489692876105297 1\n",
      "The training loss is 11.46066663558595 with std:22.284557989635942. The val loss is 8.736787302019051 with std:13.315430699120613.\n",
      "8.736787302019051 2.6489692876105297 1\n",
      "The training loss is 9.708707299741537 with std:14.609747801942431. The val loss is 12.503477658517626 with std:30.97808504347909.\n",
      "12.503477658517626 2.6489692876105297 1\n",
      "Evaluating for {'degree': 1, 'lmda': 3.4551072945922217} ...\n",
      "The training loss is 9.994935127939764 with std:19.57132343465739. The val loss is 12.004937238079165 with std:17.748017085984692.\n",
      "12.004937238079165 3.4551072945922217 1\n",
      "The training loss is 11.501719931445228 with std:22.606991535592794. The val loss is 8.784628790953647 with std:13.378065542179598.\n",
      "8.784628790953647 3.4551072945922217 1\n",
      "The training loss is 9.74981449703963 with std:14.774960554051754. The val loss is 12.567384207191077 with std:31.413626211886143.\n",
      "12.567384207191077 3.4551072945922217 1\n",
      "Evaluating for {'degree': 1, 'lmda': 4.506570337745478} ...\n",
      "The training loss is 10.067818202021629 with std:19.94432227174549. The val loss is 12.030132092218217 with std:17.99818668928449.\n",
      "12.030132092218217 4.506570337745478 1\n",
      "The training loss is 11.569465712566947 with std:23.03203794271501. The val loss is 8.859404060743689 with std:13.480704584364718.\n",
      "8.859404060743689 4.506570337745478 1\n",
      "The training loss is 9.817855269988144 with std:15.008209783130456. The val loss is 12.664276113704664 with std:31.974743424793292.\n",
      "12.664276113704664 4.506570337745478 1\n",
      "Evaluating for {'degree': 1, 'lmda': 5.878016072274912} ...\n",
      "The training loss is 10.18468296597491 with std:20.436690835272024. The val loss is 12.090984720871425 with std:18.35309797428469.\n",
      "12.090984720871425 5.878016072274912 1\n",
      "The training loss is 11.680672672063695 with std:23.59280193932544. The val loss is 8.976994719714199 with std:13.647975180643078.\n",
      "8.976994719714199 5.878016072274912 1\n",
      "The training loss is 9.929856100124226 with std:15.339751335202166. The val loss is 12.812625545927457 with std:32.69530312962139.\n",
      "12.812625545927457 5.878016072274912 1\n",
      "Evaluating for {'degree': 1, 'lmda': 7.666822074546214} ...\n",
      "The training loss is 10.371283133727795 with std:21.087859805099722. The val loss is 12.212348549833797 with std:18.856479235897343.\n",
      "12.212348549833797 7.666822074546214 1\n",
      "The training loss is 11.862216309487806 with std:24.332774866487213. The val loss is 9.16271303772791 with std:13.918329048220784.\n",
      "9.16271303772791 7.666822074546214 1\n",
      "The training loss is 10.11310071378945 with std:15.81297835622198. The val loss is 13.041466399667327 with std:33.616995325677365.\n",
      "13.041466399667327 7.666822074546214 1\n",
      "Evaluating for {'degree': 1, 'lmda': 10.0} ...\n",
      "The training loss is 10.668277049273366 with std:21.950293282490183. The val loss is 12.434115407835106 with std:19.56986105148837.\n",
      "12.434115407835106 10.0 1\n",
      "The training loss is 12.15686754798503 with std:25.308543133708348. The val loss is 9.456782447931543 with std:14.350127498388764.\n",
      "9.456782447931543 10.0 1\n",
      "The training loss is 10.410930334578754 with std:16.488827508272816. The val loss is 13.396241380150453 with std:34.7905060952534.\n",
      "13.396241380150453 10.0 1\n",
      "Evaluating for {'degree': 2, 'lmda': 0.01} ...\n",
      "The training loss is 6.7044363554655755 with std:13.045062141644117. The val loss is 15.316215955226586 with std:59.119332756798656.\n",
      "15.316215955226586 0.01 2\n",
      "The training loss is 8.598125258683003 with std:19.212729552451012. The val loss is 7.99156995209688 with std:12.951818877310963.\n",
      "7.99156995209688 0.01 2\n",
      "The training loss is 8.29510948401904 with std:13.781802498065005. The val loss is 8.952883613153553 with std:18.54498325947114.\n",
      "8.952883613153553 0.01 2\n",
      "Evaluating for {'degree': 2, 'lmda': 0.013043213867190054} ...\n",
      "The training loss is 6.704437937514224 with std:13.045719786750915. The val loss is 15.311354741433984 with std:59.096653497127924.\n",
      "15.311354741433984 0.013043213867190054 2\n",
      "The training loss is 8.598126753611997 with std:19.213662312745132. The val loss is 7.991394250651952 with std:12.951375422771381.\n",
      "7.991394250651952 0.013043213867190054 2\n",
      "The training loss is 8.295110642281404 with std:13.781805938124082. The val loss is 8.953091378110521 with std:18.546931640226436.\n",
      "8.953091378110521 0.013043213867190054 2\n",
      "Evaluating for {'degree': 2, 'lmda': 0.017012542798525893} ...\n",
      "The training loss is 6.704440625212566 with std:13.046578293174084. The val loss is 15.305029844778678 with std:59.06717842478045.\n",
      "15.305029844778678 0.017012542798525893 2\n",
      "The training loss is 8.59812929429459 with std:19.214878311724622. The val loss is 7.99116569799717 with std:12.95079805929382.\n",
      "7.99116569799717 0.017012542798525893 2\n",
      "The training loss is 8.295112611262008 with std:13.781810966000101. The val loss is 8.953362692556956 with std:18.549472525095734.\n",
      "8.953362692556956 0.017012542798525893 2\n",
      "Evaluating for {'degree': 2, 'lmda': 0.02218982341458972} ...\n",
      "The training loss is 6.704445189336714 with std:13.047699291959942. The val loss is 15.296806745217541 with std:59.02891288278312.\n",
      "15.296806745217541 0.02218982341458972 2\n",
      "The training loss is 8.598133610949155 with std:19.21646331131684. The val loss is 7.990868641539196 with std:12.950046763531132.\n",
      "7.990868641539196 0.02218982341458972 2\n",
      "The training loss is 8.29511595763712 with std:13.78181844275659. The val loss is 8.95371712087013 with std:18.55278592463591.\n",
      "8.95371712087013 0.02218982341458972 2\n",
      "Evaluating for {'degree': 2, 'lmda': 0.028942661247167517} ...\n",
      "The training loss is 6.7044529356611475 with std:13.049163518431568. The val loss is 15.28612622893057 with std:58.97930567182766.\n",
      "15.28612622893057 0.028942661247167517 2\n",
      "The training loss is 8.598140942085191 with std:19.218528870077876. The val loss is 7.990482964518397 with std:12.949069837927382.\n",
      "7.990482964518397 0.028942661247167517 2\n",
      "The training loss is 8.295121643233793 with std:13.781829754684418. The val loss is 8.954180337776958 with std:18.55710641967093.\n",
      "8.954180337776958 0.028942661247167517 2\n",
      "Evaluating for {'degree': 2, 'lmda': 0.037750532053243954} ...\n",
      "The training loss is 6.704466073510254 with std:13.051076864182203. The val loss is 15.272271595764337 with std:58.91511383599584.\n",
      "15.272271595764337 0.037750532053243954 2\n",
      "The training loss is 8.598153386389699 with std:19.22121998337113. The val loss is 7.989982933587795 with std:12.947800706917976.\n",
      "7.989982933587795 0.037750532053243954 2\n",
      "The training loss is 8.29513129943443 with std:13.781847155544831. The val loss is 8.95478609612696 with std:18.56273962082039.\n",
      "8.95478609612696 0.037750532053243954 2\n",
      "Evaluating for {'degree': 2, 'lmda': 0.04923882631706739} ...\n",
      "The training loss is 6.704488334877049 with std:13.053578430271914. The val loss is 15.25432934321127 with std:58.832248774341764.\n",
      "15.25432934321127 0.04923882631706739 2\n",
      "The training loss is 8.598174495916401 with std:19.224724903443395. The val loss is 7.989335835767365 with std:12.946153975927285.\n",
      "7.989335835767365 0.04923882631706739 2\n",
      "The training loss is 8.295147690769761 with std:13.781874338289175. The val loss is 8.955578867653475 with std:18.570083534231337.\n",
      "8.955578867653475 0.04923882631706739 2\n",
      "Evaluating for {'degree': 2, 'lmda': 0.0642232542222936} ...\n",
      "The training loss is 6.704526010558187 with std:13.056851293535383. The val loss is 15.231143578581479 with std:58.72561202726287.\n",
      "15.231143578581479 0.0642232542222936 2\n",
      "The training loss is 8.59821027331184 with std:19.229287703770808. The val loss is 7.9885004336346706 with std:12.944020687370404.\n",
      "7.9885004336346706 0.0642232542222936 2\n",
      "The training loss is 8.295175496511295 with std:13.78191739089535. The val loss is 8.956617411437492 with std:18.579656255203638.\n",
      "8.956617411437492 0.0642232542222936 2\n",
      "Evaluating for {'degree': 2, 'lmda': 0.0837677640068292} ...\n",
      "The training loss is 6.704589675815154 with std:13.061136976381954. The val loss is 15.201265840830253 with std:58.588939489731295.\n",
      "15.201265840830253 0.0837677640068292 2\n",
      "The training loss is 8.598270842351043 with std:19.235224257739233. The val loss is 7.987425335456509 with std:12.941262789439621.\n",
      "7.987425335456509 0.0837677640068292 2\n",
      "The training loss is 8.295222624866984 with std:13.781986393878816. The val loss is 8.957979634870437 with std:18.592131794900194.\n",
      "8.957979634870437 0.0837677640068292 2\n",
      "Evaluating for {'degree': 2, 'lmda': 0.10926008611173785} ...\n",
      "The training loss is 6.704697046638333 with std:13.066755005413713. The val loss is 15.162904586573335 with std:58.414689708524364.\n",
      "15.162904586573335 0.10926008611173785 2\n",
      "The training loss is 8.59837323364984 with std:19.242942397334534. The val loss is 7.986047499949583 with std:12.937707000116.\n",
      "7.986047499949583 0.10926008611173785 2\n",
      "The training loss is 8.295302415113728 with std:13.782098086293137. The val loss is 8.959769275580925 with std:18.608386312238725.\n",
      "8.959769275580925 0.10926008611173785 2\n",
      "Evaluating for {'degree': 2, 'lmda': 0.14251026703029984} ...\n",
      "The training loss is 6.704877670092471 with std:13.074129500849036. The val loss is 15.11388290046188 with std:58.1940374023319.\n",
      "15.11388290046188 0.14251026703029984 2\n",
      "The training loss is 8.598546002886808 with std:19.252967075240175. The val loss is 7.9842913069898405 with std:12.933138556665767.\n",
      "7.9842913069898405 0.14251026703029984 2\n",
      "The training loss is 8.295437311371542 with std:13.78228030278224. The val loss is 8.962125177461202 with std:18.629557576938858.\n",
      "8.962125177461202 0.14251026703029984 2\n",
      "Evaluating for {'degree': 2, 'lmda': 0.18587918911465645} ...\n",
      "The training loss is 6.705180549817254 with std:13.083825510792447. The val loss is 15.05161971980486 with std:57.91706813719979.\n",
      "15.05161971980486 0.18587918911465645 2\n",
      "The training loss is 8.598836830065697 with std:19.265971325722095. The val loss is 7.982068973885477 with std:12.927295871649923.\n",
      "7.982068973885477 0.18587918911465645 2\n",
      "The training loss is 8.295664957242787 with std:13.7825793280463. The val loss is 8.965234300183123 with std:18.657121121954.\n",
      "8.965234300183123 0.18587918911465645 2\n",
      "Evaluating for {'degree': 2, 'lmda': 0.24244620170823283} ...\n",
      "The training loss is 6.705686389603602 with std:13.096598842743607. The val loss is 14.973159591185643 with std:57.5733104321446.\n",
      "14.973159591185643 0.24244620170823283 2\n",
      "The training loss is 8.599324906551487 with std:19.282813630419515. The val loss is 7.979283645489729 with std:12.919867980228029.\n",
      "7.979283645489729 0.24244620170823283 2\n",
      "The training loss is 8.296048235454302 with std:13.783072018393796. The val loss is 8.969350148036803 with std:18.692987223177383.\n",
      "8.969350148036803 0.24244620170823283 2\n",
      "Evaluating for {'degree': 2, 'lmda': 0.31622776601683794} ...\n",
      "The training loss is 6.706526952844699 with std:13.113464468414213. The val loss is 14.875288607861984 with std:57.1527730930815.\n",
      "14.875288607861984 0.31622776601683794 2\n",
      "The training loss is 8.600140889406243 with std:19.304581837626134. The val loss is 7.975837310677145 with std:12.910497990147109.\n",
      "7.975837310677145 0.31622776601683794 2\n",
      "The training loss is 8.29669166152986 with std:13.783885633349888. The val loss is 8.974819112740116 with std:18.739623503725987.\n",
      "8.974819112740116 0.31622776601683794 2\n",
      "Evaluating for {'degree': 2, 'lmda': 0.41246263829013524} ...\n",
      "The training loss is 6.707915118275364 with std:13.135790115185195. The val loss is 14.75478785370766 with std:56.64764380707672.\n",
      "14.75478785370766 0.41246263829013524 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 8.601498583235031 with std:19.332642931293034. The val loss is 7.971646870662691 with std:12.898797653251734.\n",
      "7.971646870662691 0.41246263829013524 2\n",
      "The training loss is 8.297767867563344 with std:13.785229980783614. The val loss is 8.982118408344386 with std:18.800208479127114.\n",
      "8.982118408344386 0.41246263829013524 2\n",
      "Evaluating for {'degree': 2, 'lmda': 0.5379838403443686} ...\n",
      "The training loss is 6.710190574521063 with std:13.16542315132922. The val loss is 14.608885046403316 with std:56.05468576979093.\n",
      "14.608885046403316 0.5379838403443686 2\n",
      "The training loss is 8.603744377489374 with std:19.368696608286367. The val loss is 7.966673254612952 with std:12.88438070272362.\n",
      "7.966673254612952 0.5379838403443686 2\n",
      "The training loss is 8.299559845204667 with std:13.787448935263425. The val loss is 8.991910988039638 with std:18.87882155870565.\n",
      "8.991910988039638 0.5379838403443686 2\n",
      "Evaluating for {'degree': 2, 'lmda': 0.701703828670383} ...\n",
      "The training loss is 6.713887761903353 with std:13.204859776896418. The val loss is 14.435959333797816 with std:55.37805208875348.\n",
      "14.435959333797816 0.701703828670383 2\n",
      "The training loss is 8.60743287428216 with std:19.414828898762167. The val loss is 7.9609704576380205 with std:12.866925597246828.\n",
      "7.9609704576380205 0.701703828670383 2\n",
      "The training loss is 8.30252735596187 with std:13.791101928295122. The val loss is 9.005125298253208 with std:18.980674687866216.\n",
      "9.005125298253208 0.701703828670383 2\n",
      "Evaluating for {'degree': 2, 'lmda': 0.9152473108773893} ...\n",
      "The training loss is 6.719834847801193 with std:13.257465019005911. The val loss is 14.236516747647782 with std:54.63165500151631.\n",
      "14.236516747647782 0.9152473108773893 2\n",
      "The training loss is 8.613440172449975 with std:19.47356058658114. The val loss is 7.9547638199390995 with std:12.846281390413788.\n",
      "7.9547638199390995 0.9152473108773893 2\n",
      "The training loss is 8.307409741526175 with std:13.797091016822726. The val loss is 9.023071338095823 with std:19.112389841063468.\n",
      "9.023071338095823 0.9152473108773893 2\n",
      "Evaluating for {'degree': 2, 'lmda': 1.1937766417144369} ...\n",
      "The training loss is 6.729296052397797 with std:13.327749377420998. The val loss is 14.014367312628604 with std:53.839469364960394.\n",
      "14.014367312628604 1.1937766417144369 2\n",
      "The training loss is 8.62313033113588 with std:19.547885757367787. The val loss is 7.948570214761011 with std:12.82263319098981.\n",
      "7.948570214761011 1.1937766417144369 2\n",
      "The training loss is 8.31538281513866 with std:13.806856266323944. The val loss is 9.047610073359696 with std:19.28232524917871.\n",
      "9.047610073359696 1.1937766417144369 2\n",
      "Evaluating for {'degree': 2, 'lmda': 1.5570684047537318} ...\n",
      "The training loss is 6.744176820970973 with std:13.421703744431431. The val loss is 13.777796665095241 with std:53.03168263190437.\n",
      "13.777796665095241 1.5570684047537318 2\n",
      "The training loss is 8.638597119502732 with std:19.641302360924996. The val loss is 7.943378792399464 with std:12.796746716354184.\n",
      "7.943378792399464 1.5570684047537318 2\n",
      "The training loss is 8.32829588572267 with std:13.822672850374918. The val loss is 9.081402649712166 with std:19.500952850578653.\n",
      "9.081402649712166 1.5570684047537318 2\n",
      "Evaluating for {'degree': 2, 'lmda': 2.030917620904737} ...\n",
      "The training loss is 6.767326914123069 with std:13.547192421157524. The val loss is 13.540373865860992 with std:52.235347509629214.\n",
      "13.540373865860992 2.030917620904737 2\n",
      "The training loss is 8.66301633253613 with std:19.75785631711967. The val loss is 7.9409241822787395 with std:12.770319000981067.\n",
      "7.9409241822787395 2.030917620904737 2\n",
      "The training loss is 8.349029375344095 with std:13.848100725269541. The val loss is 9.128283013474151 with std:19.781292998198925.\n",
      "9.128283013474151 2.030917620904737 2\n",
      "Evaluating for {'degree': 2, 'lmda': 2.6489692876105297} ...\n",
      "The training loss is 6.803008074610868 with std:13.714414046388718. The val loss is 13.320975051547634 with std:51.46096748358885.\n",
      "13.320975051547634 2.6489692876105297 2\n",
      "The training loss is 8.70117213806165 with std:19.902263869961118. The val loss is 7.944113989787231 with std:12.74648646168807.\n",
      "7.944113989787231 2.6489692876105297 2\n",
      "The training loss is 8.382040468074788 with std:13.88866877925309. The val loss is 9.193829926877731 with std:20.139424855127544.\n",
      "9.193829926877731 2.6489692876105297 2\n",
      "Evaluating for {'degree': 2, 'lmda': 3.4551072945922217} ...\n",
      "The training loss is 6.857649861143691 with std:13.936475434246178. The val loss is 13.142777352177479 with std:50.69051608007423.\n",
      "13.142777352177479 3.4551072945922217 2\n",
      "The training loss is 8.760277346758164 with std:20.08025539154636. The val loss is 7.95773321444713 with std:12.730605465838277.\n",
      "7.95773321444713 3.4551072945922217 2\n",
      "The training loss is 8.4342150051555 with std:13.952931548837432. The val loss is 9.286273492538902 with std:20.59511705767838.\n",
      "9.286273492538902 3.4551072945922217 2\n",
      "Evaluating for {'degree': 2, 'lmda': 4.506570337745478} ...\n",
      "The training loss is 6.9411078811444185 with std:14.230192779782938. The val loss is 13.03148964515796 with std:49.87447668340653.\n",
      "13.03148964515796 4.506570337745478 2\n",
      "The training loss is 8.851306577156334 with std:20.299405729173316. The val loss is 7.989653896279457 with std:12.731554045468412.\n",
      "7.989653896279457 4.506570337745478 2\n",
      "The training loss is 8.516233218722803 with std:14.054131530180483. The val loss is 9.417970437204646 with std:21.172662126316812.\n",
      "9.417970437204646 4.506570337745478 2\n",
      "Evaluating for {'degree': 2, 'lmda': 5.878016072274912} ...\n",
      "The training loss is 7.068768179363775 with std:14.617329383141568. The val loss is 13.013850616504257 with std:48.941995812550864.\n",
      "13.013850616504257 5.878016072274912 2\n",
      "The training loss is 8.991210008336743 with std:20.570879160395528. The val loss is 8.05293251891841 with std:12.764019894504322.\n",
      "8.05293251891841 5.878016072274912 2\n",
      "The training loss is 8.64479750169037 with std:14.212835661091216. The val loss is 9.607829525965546 with std:21.902034172287777.\n",
      "9.607829525965546 5.878016072274912 2\n",
      "Evaluating for {'degree': 2, 'lmda': 7.666822074546214} ...\n",
      "The training loss is 7.265011719156934 with std:15.126561125734293. The val loss is 13.118112200621088 with std:47.820326705783934.\n",
      "13.118112200621088 7.666822074546214 2\n",
      "The training loss is 9.206564859985834 with std:20.91270184678881. The val loss is 8.169365447750833 with std:12.852489295274891.\n",
      "8.169365447750833 7.666822074546214 2\n",
      "The training loss is 8.846263298526495 with std:14.46104664926353. The val loss is 9.88525689450263 with std:22.820488640138272.\n",
      "9.88525689450263 7.666822074546214 2\n",
      "Evaluating for {'degree': 2, 'lmda': 10.0} ...\n",
      "The training loss is 7.568759922675903 with std:15.796462772200105. The val loss is 13.378431853913868 with std:46.454529683150135.\n",
      "13.378431853913868 10.0 2\n",
      "The training loss is 9.539427296079273 with std:21.355329806353268. The val loss is 8.375265066611039 with std:13.03775044973556.\n",
      "8.375265066611039 10.0 2\n",
      "The training loss is 9.162441122419963 with std:14.848268154814525. The val loss is 10.296394150885124 with std:23.974645697036028.\n",
      "10.296394150885124 10.0 2\n",
      "Evaluating for {'degree': 3, 'lmda': 0.01} ...\n",
      "The training loss is 6.017045262090918 with std:12.977011749053265. The val loss is 211.70023397036982 with std:2061.882635625643.\n",
      "211.70023397036982 0.01 3\n",
      "The training loss is 6.6106475783691945 with std:11.60523767958761. The val loss is 9.201695859874079 with std:15.817049563139227.\n",
      "9.201695859874079 0.01 3\n",
      "The training loss is 6.64776755320434 with std:9.467847593182029. The val loss is 8.787491039497837 with std:21.603515296481525.\n",
      "8.787491039497837 0.01 3\n",
      "Evaluating for {'degree': 3, 'lmda': 0.013043213867190054} ...\n",
      "The training loss is 6.017102089895987 with std:12.974343277575517. The val loss is 209.26720850146253 with std:2036.8013330098886.\n",
      "209.26720850146253 0.013043213867190054 3\n",
      "The training loss is 6.610683390531809 with std:11.605794937837373. The val loss is 9.199768613299595 with std:15.817008797096658.\n",
      "9.199768613299595 0.013043213867190054 3\n",
      "The training loss is 6.6477955835305025 with std:9.469563866959213. The val loss is 8.78660264585311 with std:21.596695741896948.\n",
      "8.78660264585311 0.013043213867190054 3\n",
      "Evaluating for {'degree': 3, 'lmda': 0.017012542798525893} ...\n",
      "The training loss is 6.017196587296125 with std:12.970987066510018. The val loss is 206.1516391030608 with std:2004.6841597481523.\n",
      "206.1516391030608 0.017012542798525893 3\n",
      "The training loss is 6.610742855858642 with std:11.606520404414715. The val loss is 9.197326093985186 with std:15.816942032489953.\n",
      "9.197326093985186 0.017012542798525893 3\n",
      "The training loss is 6.647842646060678 with std:9.471820998495982. The val loss is 8.785454179656679 with std:21.58792682493946.\n",
      "8.785454179656679 0.017012542798525893 3\n",
      "Evaluating for {'degree': 3, 'lmda': 0.02218982341458972} ...\n",
      "The training loss is 6.0173527387468395 with std:12.96681075780986. The val loss is 202.18390101514657 with std:1963.7827525747182.\n",
      "202.18390101514657 0.02218982341458972 3\n",
      "The training loss is 6.61084096919843 with std:11.607466438210647. The val loss is 9.194254362197182 with std:15.816832438149255.\n",
      "9.194254362197182 0.02218982341458972 3\n",
      "The training loss is 6.6479213641261214 with std:9.474794058924244. The val loss is 8.783973501523809 with std:21.576697808345948.\n",
      "8.783973501523809 0.02218982341458972 3\n",
      "Evaluating for {'degree': 3, 'lmda': 0.028942661247167517} ...\n",
      "The training loss is 6.017608757351153 with std:12.961685008890697. The val loss is 197.1665768910006 with std:1912.0624160900736.\n",
      "197.1665768910006 0.028942661247167517 3\n",
      "The training loss is 6.611001600345888 with std:11.608704167210531. The val loss is 9.190427715697224 with std:15.816652594073409.\n",
      "9.190427715697224 0.028942661247167517 3\n",
      "The training loss is 6.64805239702544 with std:9.478716123584292. The val loss is 8.782071178277997 with std:21.56239401694151.\n",
      "8.782071178277997 0.028942661247167517 3\n",
      "Evaluating for {'degree': 3, 'lmda': 0.037750532053243954} ...\n",
      "The training loss is 6.018024483245307 with std:12.955504617606321. The val loss is 190.87950323797529 with std:1847.2542510676897.\n",
      "190.87950323797529 0.037750532053243954 3\n",
      "The training loss is 6.611262163846971 with std:11.610332766214631. The val loss is 9.185714686665955 with std:15.816358402549369.\n",
      "9.185714686665955 0.037750532053243954 3\n",
      "The training loss is 6.648269188995564 with std:9.483896778686871. The val loss is 8.779638178804026 with std:21.54429477499786.\n",
      "8.779638178804026 0.037750532053243954 3\n",
      "Evaluating for {'degree': 3, 'lmda': 0.04923882631706739} ...\n",
      "The training loss is 6.018691651172426 with std:12.94822185274969. The val loss is 183.09247200864402 with std:1766.9862573649623.\n",
      "183.09247200864402 0.04923882631706739 3\n",
      "The training loss is 6.61168026349346 with std:11.612494815801233. The val loss is 9.179987669949238 with std:15.815880209829258.\n",
      "9.179987669949238 0.04923882631706739 3\n",
      "The training loss is 6.648625140695741 with std:9.49074490360622. The val loss is 8.77654457664624 with std:21.521585264633252.\n",
      "8.77654457664624 0.04923882631706739 3\n",
      "Evaluating for {'degree': 3, 'lmda': 0.0642232542222936} ...\n",
      "The training loss is 6.019747265977714 with std:12.939894392058454. The val loss is 173.58907766476918 with std:1669.0294285646726.\n",
      "173.58907766476918 0.0642232542222936 3\n",
      "The training loss is 6.612342788684788 with std:11.615401538015332. The val loss is 9.173135999530526 with std:15.815110606771654.\n",
      "9.173135999530526 0.0642232542222936 3\n",
      "The training loss is 6.649204053720465 with std:9.499794987454152. The val loss is 8.772640353282979 with std:21.493391575046473.\n",
      "8.772640353282979 0.0642232542222936 3\n",
      "Evaluating for {'degree': 3, 'lmda': 0.0837677640068292} ...\n",
      "The training loss is 6.021389486456457 with std:12.930748311527331. The val loss is 162.20474864006735 with std:1551.6902335342716.\n",
      "162.20474864006735 0.0837677640068292 3\n",
      "The training loss is 6.613377780125153 with std:11.619372935630532. The val loss is 9.165081149527335 with std:15.813888980472017.\n",
      "9.165081149527335 0.0837677640068292 3\n",
      "The training loss is 6.650134618384989 with std:9.511734537089135. The val loss is 8.767759877650404 with std:21.458850987459325.\n",
      "8.767759877650404 0.0837677640068292 3\n",
      "Evaluating for {'degree': 3, 'lmda': 0.10926008611173785} ...\n",
      "The training loss is 6.023893800481815 with std:12.921253045455629. The val loss is 148.87949029355437 with std:1414.3550529533122.\n",
      "148.87949029355437 0.10926008611173785 3\n",
      "The training loss is 6.614968904694576 with std:11.624898134961539. The val loss is 9.155791490588015 with std:15.811984072444448.\n",
      "9.155791490588015 0.10926008611173785 3\n",
      "The training loss is 6.65160919868823 with std:9.527427647253202. The val loss is 8.76173207199253 with std:21.417231344749233.\n",
      "8.76173207199253 0.10926008611173785 3\n",
      "Evaluating for {'degree': 3, 'lmda': 0.14251026703029984} ...\n",
      "The training loss is 6.027624780738613 with std:12.91220038191477. The val loss is 133.72018442534446 with std:1258.1337022873133.\n",
      "133.72018442534446 0.14251026703029984 3\n",
      "The training loss is 6.617371321019776 with std:11.632719085267532. The val loss is 9.14529321048314 with std:15.809077417178164.\n",
      "9.14529321048314 0.14251026703029984 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 6.6539059145897825 with std:9.547926982341163. The val loss is 8.754398438612766 with std:21.368112354891185.\n",
      "8.754398438612766 0.14251026703029984 3\n",
      "Evaluating for {'degree': 3, 'lmda': 0.18587918911465645} ...\n",
      "The training loss is 6.033035869457525 with std:12.904774386945462. The val loss is 117.05836034048673 with std:1086.4573891879752.\n",
      "117.05836034048673 0.18587918911465645 3\n",
      "The training loss is 6.6209258321664075 with std:11.643934764366545. The val loss is 9.133673970604963 with std:15.804751906047368.\n",
      "9.133673970604963 0.18587918911465645 3\n",
      "The training loss is 6.65741085125739 with std:9.57446456759771. The val loss is 8.745640756134636 with std:21.311635219807346.\n",
      "8.745640756134636 0.18587918911465645 3\n",
      "Evaluating for {'degree': 3, 'lmda': 0.24244620170823283} ...\n",
      "The training loss is 6.04064827968954 with std:12.900595772675697. The val loss is 99.47982780369601 with std:905.3888060973825.\n",
      "99.47982780369601 0.24244620170823283 3\n",
      "The training loss is 6.626065739565154 with std:11.660113250248159. The val loss is 9.121076375681644 with std:15.798489626033792.\n",
      "9.121076375681644 0.24244620170823283 3\n",
      "The training loss is 6.662634506763177 with std:9.608413273562828. The val loss is 8.735419276427598 with std:21.248812580636702.\n",
      "8.735419276427598 0.24244620170823283 3\n",
      "Evaluating for {'degree': 3, 'lmda': 0.31622776601683794} ...\n",
      "The training loss is 6.051003579341295 with std:12.901726201742802. The val loss is 81.79891283696485 with std:723.3655448935742.\n",
      "81.79891283696485 0.31622776601683794 3\n",
      "The training loss is 6.63330923626014 with std:11.683388903774482. The val loss is 9.107678794357758 with std:15.78968029182568.\n",
      "9.107678794357758 0.31622776601683794 3\n",
      "The training loss is 6.670214820477808 with std:9.651218639526721. The val loss is 8.723821104811405 with std:21.181868904043945.\n",
      "8.723821104811405 0.31622776601683794 3\n",
      "Evaluating for {'degree': 3, 'lmda': 0.41246263829013524} ...\n",
      "The training loss is 6.064596181009116 with std:12.910629826086803. The val loss is 64.96040774418944 with std:550.2074007451299.\n",
      "64.96040774418944 0.41246263829013524 3\n",
      "The training loss is 6.643232726558743 with std:11.716518892222048. The val loss is 9.093661858106117 with std:15.777635678980525.\n",
      "9.093661858106117 0.41246263829013524 3\n",
      "The training loss is 6.680901132808371 with std:9.704316320275788. The val loss is 8.711118324246616 with std:21.11455957828043.\n",
      "8.711118324246616 0.41246263829013524 3\n",
      "Evaluating for {'degree': 3, 'lmda': 0.5379838403443686} ...\n",
      "The training loss is 6.081811172417563 with std:12.930113014939135. The val loss is 49.87956065376051 with std:395.50469216186116.\n",
      "49.87956065376051 0.5379838403443686 3\n",
      "The training loss is 6.6564307906957625 with std:11.762885734328316. The val loss is 9.079162536420682 with std:15.761598238429151.\n",
      "9.079162536420682 0.5379838403443686 3\n",
      "The training loss is 6.695522286677666 with std:9.769071098540586. The val loss is 8.697838063310648 with std:21.052410936390505.\n",
      "8.697838063310648 0.5379838403443686 3\n",
      "Evaluating for {'degree': 3, 'lmda': 0.701703828670383} ...\n",
      "The training loss is 6.102911597953306 with std:12.96329245450644. The val loss is 37.26417444422613 with std:266.8675964813886.\n",
      "37.26417444422613 0.701703828670383 3\n",
      "The training loss is 6.673487107285675 with std:11.826462684940095. The val loss is 9.064227209867843 with std:15.740728034820723.\n",
      "9.064227209867843 0.701703828670383 3\n",
      "The training loss is 6.714958379707169 with std:9.846790186340916. The val loss is 8.684853595852113 with std:21.002849800947498.\n",
      "8.684853595852113 0.701703828670383 3\n",
      "Evaluating for {'degree': 3, 'lmda': 0.9152473108773893} ...\n",
      "The training loss is 6.128124684587599 with std:13.013660082187345. The val loss is 27.48128768963449 with std:168.73808533697243.\n",
      "27.48128768963449 0.9152473108773893 3\n",
      "The training loss is 6.69500113528874 with std:11.911795830214828. The val loss is 9.04879042156289 with std:15.714056839785796.\n",
      "9.04879042156289 0.9152473108773893 3\n",
      "The training loss is 6.740154859405374 with std:9.938865622025181. The val loss is 8.673516990290095 with std:20.975250800590935.\n",
      "8.673516990290095 0.9152473108773893 3\n",
      "Evaluating for {'degree': 3, 'lmda': 1.1937766417144369} ...\n",
      "The training loss is 6.157862005837817 with std:13.085303560090287. The val loss is 20.516776972687698 with std:102.31268976976686.\n",
      "20.516776972687698 1.1937766417144369 3\n",
      "The training loss is 6.7217259713471815 with std:12.024079170242508. The val loss is 9.03272046480109 with std:15.680413078521267.\n",
      "9.03272046480109 1.1937766417144369 3\n",
      "The training loss is 6.77223103304223 with std:10.047084434767566. The val loss is 8.66586833438994 with std:20.980995562396412.\n",
      "8.66586833438994 1.1937766417144369 3\n",
      "Evaluating for {'degree': 3, 'lmda': 1.5570684047537318} ...\n",
      "The training loss is 6.1930835987507935 with std:13.183302581139081. The val loss is 16.035073579164244 with std:65.95948478205136.\n",
      "16.035073579164244 1.5570684047537318 3\n",
      "The training loss is 6.75486593632834 with std:12.169390626626148. The val loss is 9.015977673696094 with std:15.638339980367935.\n",
      "9.015977673696094 1.5570684047537318 3\n",
      "The training loss is 6.812736160705705 with std:10.174120362644238. The val loss is 8.66497109286959 with std:21.03366650337603.\n",
      "8.66497109286959 1.5570684047537318 3\n",
      "Evaluating for {'degree': 3, 'lmda': 2.030917620904737} ...\n",
      "The training loss is 6.235802848749974 with std:13.314276898228583. The val loss is 13.504557972475395 with std:52.78415045278603.\n",
      "13.504557972475395 2.030917620904737 3\n",
      "The training loss is 6.796566399582614 with std:12.355127120576661. The val loss is 8.998925464169021 with std:15.58604461055822.\n",
      "8.998925464169021 2.030917620904737 3\n",
      "The training loss is 6.864098515862071 with std:10.32420512343111. The val loss is 8.675434719793284 with std:21.149464304843836.\n",
      "8.675434719793284 2.030917620904737 3\n",
      "Evaluating for {'degree': 3, 'lmda': 2.6489692876105297} ...\n",
      "The training loss is 6.289748362530641 with std:13.487043891039868. The val loss is 12.336853970670896 with std:50.14395768810306.\n",
      "12.336853970670896 2.6489692876105297 3\n",
      "The training loss is 6.8506285284127895 with std:12.590661537499471. The val loss is 8.98283401107857 with std:15.521438000511107.\n",
      "8.98283401107857 2.6489692876105297 3\n",
      "The training loss is 6.930311756443381 with std:10.50398812892455. The val loss is 8.704195799785234 with std:21.347863062996126.\n",
      "8.704195799785234 2.6489692876105297 3\n",
      "Evaluating for {'degree': 3, 'lmda': 3.4551072945922217} ...\n",
      "The training loss is 6.361260455992137 with std:13.713372709415145. The val loss is 11.99880393067873 with std:49.66973326541665.\n",
      "11.99880393067873 3.4551072945922217 3\n",
      "The training loss is 6.923526808590446 with std:12.888279289431544. The val loss is 8.97065171234494 with std:15.442397216011184.\n",
      "8.97065171234494 3.4551072945922217 3\n",
      "The training loss is 7.017932075646394 with std:10.72364514448462. The val loss is 8.76165050351781 with std:21.65245764056516.\n",
      "8.76165050351781 3.4551072945922217 3\n",
      "Evaluating for {'degree': 3, 'lmda': 4.506570337745478} ...\n",
      "The training loss is 6.460604335549625 with std:14.008899494025174. The val loss is 12.079561482641514 with std:49.24054795332804.\n",
      "12.079561482641514 4.506570337745478 3\n",
      "The training loss is 7.025913048157945 with std:13.264538230091109. The val loss is 8.968218862478444 with std:15.347560123257228.\n",
      "8.968218862478444 4.506570337745478 3\n",
      "The training loss is 7.137543293254653 with std:10.998395659330972. The val loss is 8.863291494113174 with std:22.09196690634105.\n",
      "8.863291494113174 4.506570337745478 3\n",
      "Evaluating for {'degree': 3, 'lmda': 5.878016072274912} ...\n",
      "The training loss is 6.60403005970318 with std:14.394379036586399. The val loss is 12.315343367075958 with std:48.7128780247369.\n",
      "12.315343367075958 5.878016072274912 3\n",
      "The training loss is 7.174955543966469 with std:13.742285317482107. The val loss is 8.986273354643656 with std:15.238304388873825.\n",
      "8.986273354643656 5.878016072274912 3\n",
      "The training loss is 7.306003233208494 with std:11.35071705636644. The val loss is 9.032136054524413 with std:22.701443574963537.\n",
      "9.032136054524413 5.878016072274912 3\n",
      "Evaluating for {'degree': 3, 'lmda': 7.666822074546214} ...\n",
      "The training loss is 6.817101048452411 with std:14.897563096600114. The val loss is 12.583921547622014 with std:48.06803866213907.\n",
      "12.583921547622014 7.666822074546214 3\n",
      "The training loss is 7.39807302744819 with std:14.353575883032649. The val loss is 9.04383826703158 with std:15.12306709844279.\n",
      "9.04383826703158 7.666822074546214 3\n",
      "The training loss is 7.550014106364227 with std:11.81362553403233. The val loss is 9.30245815082631 with std:23.52385605638643.\n",
      "9.30245815082631 7.666822074546214 3\n",
      "Evaluating for {'degree': 3, 'lmda': 10.0} ...\n",
      "The training loss is 7.140038268827526 with std:15.556057987785113. The val loss is 12.8821472569622 with std:47.23747190873389.\n",
      "12.8821472569622 10.0 3\n",
      "The training loss is 7.7388567091147324 with std:15.143596176941328. The val loss is 9.173851074991186 with std:15.025719230629319.\n",
      "9.173851074991186 10.0 3\n",
      "The training loss is 7.911832751737521 with std:12.43528083141069. The val loss is 9.725630349508593 with std:24.612257634861386.\n",
      "9.725630349508593 10.0 3\n",
      "Evaluating for {'degree': 4, 'lmda': 0.01} ...\n",
      "The training loss is 5.49453418792616 with std:12.855823407327463. The val loss is 22.079765538121766 with std:97.81628711065353.\n",
      "22.079765538121766 0.01 4\n",
      "The training loss is 5.541014306186811 with std:10.035180847858054. The val loss is 8.683355068840802 with std:15.585395211886555.\n",
      "8.683355068840802 0.01 4\n",
      "The training loss is 4.822608628615276 with std:7.628460871563311. The val loss is 10.469159191105248 with std:29.02062497697947.\n",
      "10.469159191105248 0.01 4\n",
      "Evaluating for {'degree': 4, 'lmda': 0.013043213867190054} ...\n",
      "The training loss is 5.496964378908754 with std:12.869658690462815. The val loss is 40.61326226594881 with std:267.63541426359683.\n",
      "40.61326226594881 0.013043213867190054 4\n",
      "The training loss is 5.548277166287378 with std:10.04863136378275. The val loss is 8.683972853726617 with std:15.587556326301163.\n",
      "8.683972853726617 0.013043213867190054 4\n",
      "The training loss is 4.83680907502573 with std:7.656279581969515. The val loss is 10.37183853800777 with std:28.696070653376754.\n",
      "10.37183853800777 0.013043213867190054 4\n",
      "Evaluating for {'degree': 4, 'lmda': 0.017012542798525893} ...\n",
      "The training loss is 5.50037201543194 with std:12.885909543250174. The val loss is 73.63250855474345 with std:598.7031272368763.\n",
      "73.63250855474345 0.017012542798525893 4\n",
      "The training loss is 5.555009663046193 with std:10.061914372306436. The val loss is 8.686400623817901 with std:15.592619960797217.\n",
      "8.686400623817901 0.017012542798525893 4\n",
      "The training loss is 4.853396747351929 with std:7.69065699386316. The val loss is 10.270484268095545 with std:28.349871995786078.\n",
      "10.270484268095545 0.017012542798525893 4\n",
      "Evaluating for {'degree': 4, 'lmda': 0.02218982341458972} ...\n",
      "The training loss is 5.504999940653214 with std:12.904354742767131. The val loss is 124.30059735687949 with std:1120.4505177505068.\n",
      "124.30059735687949 0.02218982341458972 4\n",
      "The training loss is 5.561271640482526 with std:10.075452853044599. The val loss is 8.689962033165493 with std:15.599392781718429.\n",
      "8.689962033165493 0.02218982341458972 4\n",
      "The training loss is 4.872565468560109 with std:7.732480423077764. The val loss is 10.167525677438718 with std:27.99102196419992.\n",
      "10.167525677438718 0.02218982341458972 4\n",
      "Evaluating for {'degree': 4, 'lmda': 0.028942661247167517} ...\n",
      "The training loss is 5.511060841296885 with std:12.924433840416107. The val loss is 194.04996280338494 with std:1847.7425246132304.\n",
      "194.04996280338494 0.028942661247167517 4\n",
      "The training loss is 5.567185954881802 with std:10.08976809833442. The val loss is 8.693947901600161 with std:15.60643744492315.\n",
      "8.693947901600161 0.028942661247167517 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 4.894352381198367 with std:7.7822851587081265. The val loss is 10.065127722372639 with std:27.628388835172636.\n",
      "10.065127722372639 0.028942661247167517 4\n",
      "Evaluating for {'degree': 4, 'lmda': 0.037750532053243954} ...\n",
      "The training loss is 5.5186970895446725 with std:12.945200897330013. The val loss is 281.4556307295042 with std:2765.931326225595.\n",
      "281.4556307295042 0.037750532053243954 4\n",
      "The training loss is 5.57291413092613 with std:10.105444644082432. The val loss is 8.697641841802874 with std:15.61218244442992.\n",
      "8.697641841802874 0.037750532053243954 4\n",
      "The training loss is 4.918616717271848 with std:7.840139510065997. The val loss is 9.964812543582305 with std:27.26934700178998.\n",
      "9.964812543582305 0.037750532053243954 4\n",
      "Evaluating for {'degree': 4, 'lmda': 0.04923882631706739} ...\n",
      "The training loss is 5.527951012203623 with std:12.965351233316518. The val loss is 381.6036496078282 with std:3823.2947713234967.\n",
      "381.6036496078282 0.04923882631706739 4\n",
      "The training loss is 5.5786415053261695 with std:10.123101185834528. The val loss is 8.700357991085342 with std:15.615084967962344.\n",
      "8.700357991085342 0.04923882631706739 4\n",
      "The training loss is 4.945074624135558 with std:7.905633292503955. The val loss is 9.867263346927286 with std:26.918801256311227.\n",
      "9.867263346927286 0.04923882631706739 4\n",
      "Evaluating for {'degree': 4, 'lmda': 0.0642232542222936} ...\n",
      "The training loss is 5.538758677067888 with std:12.983335786241687. The val loss is 486.3663480250544 with std:4933.669032019014.\n",
      "486.3663480250544 0.0642232542222936 4\n",
      "The training loss is 5.584572669373978 with std:10.143367938080702. The val loss is 8.701485489182854 with std:15.613817009074054.\n",
      "8.701485489182854 0.0642232542222936 4\n",
      "The training loss is 4.973383602088622 with std:7.977993614551524. The val loss is 9.772345547270037 with std:26.578828810743303.\n",
      "9.772345547270037 0.0642232542222936 4\n",
      "Evaluating for {'degree': 4, 'lmda': 0.0837677640068292} ...\n",
      "The training loss is 5.5509732315832885 with std:12.99754665046682. The val loss is 585.641057858018 with std:5989.55078030236.\n",
      "585.641057858018 0.0837677640068292 4\n",
      "The training loss is 5.590933596604311 with std:10.166869470350875. The val loss is 8.70052787937072 with std:15.607425661681889.\n",
      "8.70052787937072 0.0837677640068292 4\n",
      "The training loss is 5.00325843205002 with std:8.056319892130633. The val loss is 9.679313884666467 with std:26.248993115691945.\n",
      "9.679313884666467 0.0837677640068292 4\n",
      "Evaluating for {'degree': 4, 'lmda': 0.10926008611173785} ...\n",
      "The training loss is 5.564415203839597 with std:13.006532493928969. The val loss is 669.1660078856283 with std:6881.471391218834.\n",
      "669.1660078856283 0.10926008611173785 4\n",
      "The training loss is 5.597976023907608 with std:10.194214133510492. The val loss is 8.6971289556369 with std:15.595419271606815.\n",
      "8.6971289556369 0.10926008611173785 4\n",
      "The training loss is 5.034594092787023 with std:8.139888734405476. The val loss is 9.587146191220759 with std:25.92722626787555.\n",
      "9.587146191220759 0.10926008611173785 4\n",
      "Evaluating for {'degree': 4, 'lmda': 0.14251026703029984} ...\n",
      "The training loss is 5.578942181646626 with std:13.009201346062929. The val loss is 728.29790045928 with std:7517.022964417394.\n",
      "728.29790045928 0.14251026703029984 4\n",
      "The training loss is 5.605982286745587 with std:10.225995905946066. The val loss is 8.69108601854128 with std:15.57775226885773.\n",
      "8.69108601854128 0.14251026703029984 4\n",
      "The training loss is 5.067565402543455 with std:8.228440040588481. The val loss is 9.494937885964825 with std:25.611041621013356.\n",
      "9.494937885964825 0.14251026703029984 4\n",
      "Evaluating for {'degree': 4, 'lmda': 0.18587918911465645} ...\n",
      "The training loss is 5.594528890425567 with std:13.004990474409652. The val loss is 757.2606900361499 with std:7834.24895133953.\n",
      "757.2606900361499 0.18587918911465645 4\n",
      "The training loss is 5.61527178648741 with std:10.26281803859419. The val loss is 8.68235750260821 with std:15.554711529094885.\n",
      "8.68235750260821 0.18587918911465645 4\n",
      "The training loss is 5.10266994194846 with std:8.322341454995287. The val loss is 9.402280484061802 with std:25.298733071451142.\n",
      "9.402280484061802 0.18587918911465645 4\n",
      "Evaluating for {'degree': 4, 'lmda': 0.24244620170823283} ...\n",
      "The training loss is 5.61134674464626 with std:12.994010189344936. The val loss is 753.7130832682071 with std:7807.734462176839.\n",
      "753.7130832682071 0.24244620170823283 4\n",
      "The training loss is 5.6262102800927 with std:10.305346662349354. The val loss is 8.671066709335415 with std:15.526730113801708.\n",
      "8.671066709335415 0.24244620170823283 4\n",
      "The training loss is 5.14068811178046 with std:8.422557002916328. The val loss is 9.309531828652455 with std:24.990181461180228.\n",
      "9.309531828652455 0.24244620170823283 4\n",
      "Evaluating for {'degree': 4, 'lmda': 0.31622776601683794} ...\n",
      "The training loss is 5.6298276472056425 with std:12.97718030552941. The val loss is 718.76457759071 with std:7448.772138752831.\n",
      "718.76457759071 0.31622776601683794 4\n",
      "The training loss is 5.639220709695879 with std:10.354397593730486. The val loss is 8.657488914602594 with std:15.49416024151005.\n",
      "8.657488914602594 0.31622776601683794 4\n",
      "The training loss is 5.182557466626313 with std:8.530413160536435. The val loss is 9.217889473858444 with std:24.68697977219486.\n",
      "9.217889473858444 0.31622776601683794 4\n",
      "Evaluating for {'degree': 4, 'lmda': 0.41246263829013524} ...\n",
      "The training loss is 5.650693793760757 with std:12.956369368978269. The val loss is 656.6393119205387 with std:6801.748013674069.\n",
      "656.6393119205387 0.41246263829013524 4\n",
      "The training loss is 5.654794030295245 with std:10.411056457467836. The val loss is 8.641998189898434 with std:15.45702915222238.\n",
      "8.641998189898434 0.41246263829013524 4\n",
      "The training loss is 5.229194881502856 with std:8.647237389181278. The val loss is 9.129226123571115 with std:24.39183595602414.\n",
      "9.129226123571115 0.41246263829013524 4\n",
      "Evaluating for {'degree': 4, 'lmda': 0.5379838403443686} ...\n",
      "The training loss is 5.674939321864456 with std:12.934528827779591. The val loss is 574.0871833274168 with std:5937.842200538598.\n",
      "574.0871833274168 0.5379838403443686 4\n",
      "The training loss is 5.673504326386214 with std:10.47683473988104. The val loss is 8.624957236220357 with std:15.414792699625895.\n",
      "8.624957236220357 0.5379838403443686 4\n",
      "The training loss is 5.2813362995792215 with std:8.774007317000668. The val loss is 9.045735726559764 with std:24.10755384736411.\n",
      "9.045735726559764 0.5379838403443686 4\n",
      "Evaluating for {'degree': 4, 'lmda': 0.701703828670383} ...\n",
      "The training loss is 5.703764497401604 with std:12.915803919433058. The val loss is 479.52935700600506 with std:4945.93936666308.\n",
      "479.52935700600506 0.701703828670383 4\n",
      "The training loss is 5.696042165470491 with std:10.55386990808786. The val loss is 8.606561632344086 with std:15.366105374206759.\n",
      "8.606561632344086 0.701703828670383 4\n",
      "The training loss is 5.339478426227716 with std:8.91116341831448. The val loss is 8.969532423058567 with std:23.836161629526604.\n",
      "8.969532423058567 0.701703828670383 4\n",
      "Evaluating for {'degree': 4, 'lmda': 0.9152473108773893} ...\n",
      "The training loss is 5.738488665617641 with std:12.90561076183983. The val loss is 381.9292402301727 with std:3920.667730223503.\n",
      "381.9292402301727 0.9152473108773893 4\n",
      "The training loss is 5.723287293710031 with std:10.645181775783117. The val loss is 8.586687494517097 with std:15.308644536312475.\n",
      "8.586687494517097 0.9152473108773893 4\n",
      "The training loss is 5.4039838033294245 with std:9.05869349329032. The val loss is 8.902387713482963 with std:23.578777632423144.\n",
      "8.902387713482963 0.9152473108773893 4\n",
      "Evaluating for {'degree': 4, 'lmda': 1.1937766417144369} ...\n",
      "The training loss is 5.780500147891637 with std:12.910694483197437. The val loss is 289.5113253249901 with std:2948.857950831452.\n",
      "289.5113253249901 1.1937766417144369 4\n",
      "The training loss is 5.756445158280604 with std:10.755002156707677. The val loss is 8.564821881850422 with std:15.239052620654387.\n",
      "8.564821881850422 1.1937766417144369 4\n",
      "The training loss is 5.475359535273206 with std:9.21651771977009. The val loss is 8.845759293378526 with std:23.336549284788678.\n",
      "8.845759293378526 1.1937766417144369 4\n",
      "Evaluating for {'degree': 4, 'lmda': 1.5570684047537318} ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 5.831331035167501 with std:12.939217665039806. The val loss is 208.6035295334616 with std:2097.322952239131.\n",
      "208.6035295334616 1.5570684047537318 4\n",
      "The training loss is 5.797279996668684 with std:10.889208807384792. The val loss is 8.540169867108192 with std:15.153083254184693.\n",
      "8.540169867108192 1.5570684047537318 4\n",
      "The training loss is 5.554678476444099 with std:9.38513925656055. The val loss is 8.801189978484551 with std:23.112640471068627.\n",
      "8.801189978484551 1.5570684047537318 4\n",
      "Evaluating for {'degree': 4, 'lmda': 2.030917620904737} ...\n",
      "The training loss is 5.892962104728543 with std:13.000960667105755. The val loss is 142.91758490582092 with std:1405.2688744216366.\n",
      "142.91758490582092 2.030917620904737 4\n",
      "The training loss is 5.848501003447301 with std:11.05591858662683. The val loss is 8.512033804682893 with std:15.046048522175415.\n",
      "8.512033804682893 2.030917620904737 4\n",
      "The training loss is 5.644119387117852 with std:9.56651680939439. The val loss is 8.771098775481752 with std:22.914981070100858.\n",
      "8.771098775481752 2.030917620904737 4\n",
      "Evaluating for {'degree': 4, 'lmda': 2.6489692876105297} ...\n",
      "The training loss is 5.968463352905152 with std:13.107733530717843. The val loss is 93.45204892481595 with std:883.2801597260906.\n",
      "93.45204892481595 2.6489692876105297 4\n",
      "The training loss is 5.914397795011417 with std:11.266312443403583. The val loss is 8.4805562185041 with std:14.913663852446584.\n",
      "8.4805562185041 2.6489692876105297 4\n",
      "The training loss is 5.747666128712367 with std:9.765161229979142. The val loss is 8.75998421068735 with std:22.75939136442403.\n",
      "8.75998421068735 2.6489692876105297 4\n",
      "Evaluating for {'degree': 4, 'lmda': 3.4551072945922217} ...\n",
      "The training loss is 6.063070134578826 with std:13.274092812635713. The val loss is 58.97505190583068 with std:518.4350274149142.\n",
      "58.97505190583068 3.4551072945922217 4\n",
      "The training loss is 6.001868658943307 with std:11.535754691518486. The val loss is 8.447917257658311 with std:14.753388070608107.\n",
      "8.447917257658311 3.4551072945922217 4\n",
      "The training loss is 5.872096387877253 with std:9.989529762855456. The val loss is 8.77609837224171 with std:22.67265989850488.\n",
      "8.77609837224171 3.4551072945922217 4\n",
      "Evaluating for {'degree': 4, 'lmda': 4.506570337745478} ...\n",
      "The training loss is 6.185815192070452 with std:13.518429001472157. The val loss is 36.853343508119075 with std:283.1301100065626.\n",
      "36.853343508119075 4.506570337745478 4\n",
      "The training loss is 6.1220403863987745 with std:11.885230959138777. The val loss is 8.420100535936117 with std:14.566406124397174.\n",
      "8.420100535936117 4.506570337745478 4\n",
      "The training loss is 6.0284780738288415 with std:10.253868571367207. The val loss is 8.83369660604662 with std:22.695078726975737.\n",
      "8.83369660604662 4.506570337745478 4\n",
      "Evaluating for {'degree': 4, 'lmda': 5.878016072274912} ...\n",
      "The training loss is 6.351925098908919 with std:13.864468579972534. The val loss is 23.932514675653255 with std:144.62744491386303.\n",
      "23.932514675653255 5.878016072274912 4\n",
      "The training loss is 6.29276578192878 with std:12.343098637850654. The val loss is 8.409423616563481 with std:14.360586193360014.\n",
      "8.409423616563481 5.878016072274912 4\n",
      "The training loss is 6.234484985612076 with std:10.580732243395234. The val loss is 8.956026546803512 with std:22.881849796776372.\n",
      "8.956026546803512 5.878016072274912 4\n",
      "Evaluating for {'degree': 4, 'lmda': 7.666822074546214} ...\n",
      "The training loss is 6.586375595197952 with std:14.343254940105405. The val loss is 17.238385288122487 with std:73.10919790168244.\n",
      "17.238385288122487 7.666822074546214 4\n",
      "The training loss is 6.542442811464894 with std:12.947170166864597. The val loss is 8.43821217344154 with std:14.15514159728448.\n",
      "8.43821217344154 7.666822074546214 4\n",
      "The training loss is 6.517992114491342 with std:11.004460888116856. The val loss is 9.179354811042838 with std:23.30288184890475.\n",
      "9.179354811042838 7.666822074546214 4\n",
      "Evaluating for {'degree': 4, 'lmda': 10.0} ...\n",
      "The training loss is 6.929277296260825 with std:14.995731029377463. The val loss is 14.394504381796809 with std:45.6660581499365.\n",
      "14.394504381796809 10.0 4\n",
      "The training loss is 6.9158481460162085 with std:13.747244835192715. The val loss is 8.544293683638655 with std:13.988281165741894.\n",
      "8.544293683638655 10.0 4\n",
      "The training loss is 6.922661799549598 with std:11.575813193609257. The val loss is 9.558620855045529 with std:24.041058184234032.\n",
      "9.558620855045529 10.0 4\n",
      "Evaluating for {'degree': 5, 'lmda': 0.01} ...\n",
      "The training loss is 4.936926782988398 with std:11.513206728969474. The val loss is 105.72082716288496 with std:1010.0521578173419.\n",
      "105.72082716288496 0.01 5\n",
      "The training loss is 4.940297743419149 with std:9.49844329772429. The val loss is 8.395646441203317 with std:14.8714831343603.\n",
      "8.395646441203317 0.01 5\n",
      "The training loss is 4.434878635237778 with std:7.368095311731808. The val loss is 10.315785066856357 with std:29.135045405532868.\n",
      "10.315785066856357 0.01 5\n",
      "Evaluating for {'degree': 5, 'lmda': 0.013043213867190054} ...\n",
      "The training loss is 4.952185939816373 with std:11.56051483330267. The val loss is 300.65414009517855 with std:3174.252221275265.\n",
      "300.65414009517855 0.013043213867190054 5\n",
      "The training loss is 4.961818408979613 with std:9.531835897165866. The val loss is 8.356953671611963 with std:14.811509692804275.\n",
      "8.356953671611963 0.013043213867190054 5\n",
      "The training loss is 4.4419157814091825 with std:7.375470209012972. The val loss is 10.277284001950571 with std:28.98226965900228.\n",
      "10.277284001950571 0.013043213867190054 5\n",
      "Evaluating for {'degree': 5, 'lmda': 0.017012542798525893} ...\n",
      "The training loss is 4.967457612775544 with std:11.609890154625608. The val loss is 462.87213863108855 with std:4977.2916344640735.\n",
      "462.87213863108855 0.017012542798525893 5\n",
      "The training loss is 4.982279855761824 with std:9.563445269981345. The val loss is 8.32135570515499 with std:14.75818660515293.\n",
      "8.32135570515499 0.017012542798525893 5\n",
      "The training loss is 4.450079227511827 with std:7.385492383140131. The val loss is 10.228626674212789 with std:28.793115209488747.\n",
      "10.228626674212789 0.017012542798525893 5\n",
      "Evaluating for {'degree': 5, 'lmda': 0.02218982341458972} ...\n",
      "The training loss is 4.983060752534799 with std:11.66225049846415. The val loss is 519.3642975934704 with std:5611.153558668142.\n",
      "519.3642975934704 0.02218982341458972 5\n",
      "The training loss is 5.002152077626161 with std:9.59390099787025. The val loss is 8.288132860699994 with std:14.709874780034053.\n",
      "8.288132860699994 0.02218982341458972 5\n",
      "The training loss is 4.460031997723067 with std:7.399295791373032. The val loss is 10.170065400969444 with std:28.565625218330528.\n",
      "10.170065400969444 0.02218982341458972 5\n",
      "Evaluating for {'degree': 5, 'lmda': 0.028942661247167517} ...\n",
      "The training loss is 4.9995046433136885 with std:11.718539230207757. The val loss is 457.23314281549176 with std:4928.996590623373.\n",
      "457.23314281549176 0.028942661247167517 5\n",
      "The training loss is 5.022198138196182 with std:9.624084882725379. The val loss is 8.25706948993484 with std:14.66518881024333.\n",
      "8.25706948993484 0.028942661247167517 5\n",
      "The training loss is 4.472650299177903 with std:7.418301617229651. The val loss is 10.102183455608357 with std:28.298837703788898.\n",
      "10.102183455608357 0.028942661247167517 5\n",
      "Evaluating for {'degree': 5, 'lmda': 0.037750532053243954} ...\n",
      "The training loss is 5.01741616934315 with std:11.779469299185084. The val loss is 311.3068978655425 with std:3308.2831571856404.\n",
      "311.3068978655425 0.037750532053243954 5\n",
      "The training loss is 5.043396366121254 with std:9.655133352817217. The val loss is 8.22844388914432 with std:14.623029837097322.\n",
      "8.22844388914432 0.037750532053243954 5\n",
      "The training loss is 4.4889962895258515 with std:7.44418611926383. The val loss is 10.025768845063405 with std:27.992831672851885.\n",
      "10.025768845063405 0.037750532053243954 5\n",
      "Evaluating for {'degree': 5, 'lmda': 0.04923882631706739} ...\n",
      "The training loss is 5.037419777182291 with std:11.845252542113117. The val loss is 146.677062943112 with std:1460.2251274063044.\n",
      "146.677062943112 0.04923882631706739 5\n",
      "The training loss is 5.066823279108711 with std:9.688433184743891. The val loss is 8.202887146883546 with std:14.58260254082969.\n",
      "8.202887146883546 0.04923882631706739 5\n",
      "The training loss is 4.510251875293698 with std:7.47879724073958. The val loss is 9.941628216124707 with std:27.648643035910347.\n",
      "9.941628216124707 0.04923882631706739 5\n",
      "Evaluating for {'degree': 5, 'lmda': 0.0642232542222936} ...\n",
      "The training loss is 5.059992710690221 with std:11.915377111985975. The val loss is 37.408861016073345 with std:213.79136744965723.\n",
      "37.408861016073345 0.0642232542222936 5\n",
      "The training loss is 5.0935023903782675 with std:9.725575474574486. The val loss is 8.181134146652077 with std:14.543425703155286.\n",
      "8.181134146652077 0.0642232542222936 5\n",
      "The training loss is 4.537606624831463 with std:7.5240145349380585. The val loss is 9.850401143658338 with std:27.268054208907362.\n",
      "9.850401143658338 0.0642232542222936 5\n",
      "Evaluating for {'degree': 5, 'lmda': 0.0837677640068292} ...\n",
      "The training loss is 5.08533735017133 with std:11.988510541373508. The val loss is 44.482214336716766 with std:240.77957994986838.\n",
      "44.482214336716766 0.0837677640068292 5\n",
      "The training loss is 5.124237251635372 with std:9.76823142748771. The val loss is 8.163725827868438 with std:14.505326300189724.\n",
      "8.163725827868438 0.0837677640068292 5\n",
      "The training loss is 4.572101279153988 with std:7.581554319189056. The val loss is 9.752440389206662 with std:26.853280275614363.\n",
      "9.752440389206662 0.0837677640068292 5\n",
      "Evaluating for {'degree': 5, 'lmda': 0.10926008611173785} ...\n",
      "The training loss is 5.113320990418594 with std:12.062591384682563. The val loss is 198.11130177551323 with std:1781.852641757793.\n",
      "198.11130177551323 0.10926008611173785 5\n",
      "The training loss is 5.159463602134242 with std:9.817945235929432. The val loss is 8.150750515606878 with std:14.468389578485732.\n",
      "8.150750515606878 0.10926008611173785 5\n",
      "The training loss is 4.614447278484123 with std:7.6527402162723295. The val loss is 9.647792060380874 with std:26.406595805024295.\n",
      "9.647792060380874 0.10926008611173785 5\n",
      "Evaluating for {'degree': 5, 'lmda': 0.14251026703029984} ...\n",
      "The training loss is 5.143519338466864 with std:12.135122616380706. The val loss is 489.6984932653599 with std:4866.232108436112.\n",
      "489.6984932653599 0.14251026703029984 5\n",
      "The training loss is 5.199164110443135 with std:9.875896564270972. The val loss is 8.141712733696009 with std:14.432842547284725.\n",
      "8.141712733696009 0.14251026703029984 5\n",
      "The training loss is 4.6648699547430486 with std:7.738290524308955. The val loss is 9.536271241945164 with std:25.929995960614196.\n",
      "9.536271241945164 0.14251026703029984 5\n",
      "Evaluating for {'degree': 5, 'lmda': 0.18587918911465645} ...\n",
      "The training loss is 5.17536385147043 with std:12.203614757625827. The val loss is 875.7068440870811 with std:8995.437007333638.\n",
      "875.7068440870811 0.18587918911465645 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 5.24287879973583 with std:9.942730520397628. The val loss is 8.135579155983926 with std:14.398882435072132.\n",
      "8.135579155983926 0.18587918911465645 5\n",
      "The training loss is 4.723042322389364 with std:7.838201020912573. The val loss is 9.41762023421165 with std:25.4250601412789.\n",
      "9.41762023421165 0.18587918911465645 5\n",
      "Evaluating for {'degree': 5, 'lmda': 0.24244620170823283} ...\n",
      "The training loss is 5.208353071849821 with std:12.266082184136806. The val loss is 1291.1223512468446 with std:13465.393211741719.\n",
      "1291.1223512468446 0.24244620170823283 5\n",
      "The training loss is 5.289814444138068 with std:10.018545442956253. The val loss is 8.130984728102867 with std:14.36650161251128.\n",
      "8.130984728102867 0.24244620170823283 5\n",
      "The training loss is 4.78816235731634 with std:7.951791662438449. The val loss is 9.291753246510806 with std:24.893224933698626.\n",
      "9.291753246510806 0.24244620170823283 5\n",
      "Evaluating for {'degree': 5, 'lmda': 0.31622776601683794} ...\n",
      "The training loss is 5.242267879081881 with std:12.321495971464552. The val loss is 1666.865589313618 with std:17527.558614491547.\n",
      "1666.865589313618 0.31622776601683794 5\n",
      "The training loss is 5.339021999104373 with std:10.103066021761379. The val loss is 8.126519129721096 with std:14.335369535069676.\n",
      "8.126519129721096 0.31622776601683794 5\n",
      "The training loss is 4.859169197288276 with std:8.07792172871468. The val loss is 9.159086345362518 with std:24.33659110016919.\n",
      "9.159086345362518 0.31622776601683794 5\n",
      "Evaluating for {'degree': 5, 'lmda': 0.41246263829013524} ...\n",
      "The training loss is 5.277338447581425 with std:12.370132563235554. The val loss is 1945.3247361624422 with std:20556.115496792678.\n",
      "1945.3247361624422 0.41246263829013524 5\n",
      "The training loss is 5.389593746680785 with std:10.195957294776585. The val loss is 8.120986766599469 with std:14.304797235071492.\n",
      "8.120986766599469 0.41246263829013524 5\n",
      "The training loss is 4.935021265266221 with std:8.215293519149412. The val loss is 9.02089462624812 with std:23.759165563048693.\n",
      "9.02089462624812 0.41246263829013524 5\n",
      "Evaluating for {'degree': 5, 'lmda': 0.5379838403443686} ...\n",
      "The training loss is 5.314334677422931 with std:12.413799662486534. The val loss is 2090.4481684724096 with std:22157.53861268541.\n",
      "2090.4481684724096 0.5379838403443686 5\n",
      "The training loss is 5.440842058424304 with std:10.297206610157998. The val loss is 8.11355444624099 with std:14.27375731308505.\n",
      "8.11355444624099 0.5379838403443686 5\n",
      "The training loss is 5.014927249393819 with std:8.36272810595379. The val loss is 8.879566991659134 with std:23.16817600652939.\n",
      "8.879566991659134 0.5379838403443686 5\n",
      "Evaluating for {'degree': 5, 'lmda': 0.701703828670383} ...\n",
      "The training loss is 5.354574756990354 with std:12.455946234395014. The val loss is 2091.52559262197 with std:22211.871540922984.\n",
      "2091.52559262197 0.701703828670383 5\n",
      "The training loss is 5.492451044683332 with std:10.407525538128645. The val loss is 8.103757071844662 with std:14.240904885625811.\n",
      "8.103757071844662 0.701703828670383 5\n",
      "The training loss is 5.098464037886667 with std:8.519348691556306. The val loss is 8.738621646129712 with std:22.57497228978552.\n",
      "8.738621646129712 0.701703828670383 5\n",
      "Evaluating for {'degree': 5, 'lmda': 0.9152473108773893} ...\n",
      "The training loss is 5.399863091174132 with std:12.501673447914317. The val loss is 1961.4793647298025 with std:20854.518445432655.\n",
      "1961.4793647298025 0.9152473108773893 5\n",
      "The training loss is 5.544620851619825 with std:10.52876714824935. The val loss is 8.091396687178195 with std:14.20456413428361.\n",
      "8.091396687178195 0.9152473108773893 5\n",
      "The training loss is 5.18561070507842 with std:8.684713871800827. The val loss is 8.602444723396015 with std:21.995168528918246.\n",
      "8.602444723396015 0.9152473108773893 5\n",
      "Evaluating for {'degree': 5, 'lmda': 1.1937766417144369} ...\n",
      "The training loss is 5.452387873018987 with std:12.557675512439708. The val loss is 1731.114664752227 with std:18414.31385393714.\n",
      "1731.114664752227 1.1937766417144369 5\n",
      "The training loss is 5.5982376898702215 with std:10.664379357426204. The val loss is 8.07642006904579 with std:14.162698371323778.\n",
      "8.07642006904579 1.1937766417144369 5\n",
      "The training loss is 5.276807512699296 with std:8.859021777891245. The val loss is 8.475876549351607 with std:21.448041114894465.\n",
      "8.475876549351607 1.1937766417144369 5\n",
      "Evaluating for {'degree': 5, 'lmda': 1.5570684047537318} ...\n",
      "The training loss is 5.514641508276262 with std:12.632167904618393. The val loss is 1440.9729857271675 with std:15325.728618903526.\n",
      "1440.9729857271675 1.5570684047537318 5\n",
      "The training loss is 5.6551048556556625 with std:10.819918806399162. The val loss is 8.058885838630818 with std:14.112940434071337.\n",
      "8.058885838630818 1.5570684047537318 5\n",
      "The training loss is 5.3731609786063315 with std:9.043502577124201. The val loss is 8.363897730462616 with std:20.95559056211353.\n",
      "8.363897730462616 1.5570684047537318 5\n",
      "Evaluating for {'degree': 5, 'lmda': 2.030917620904737} ...\n",
      "The training loss is 5.589475526863482 with std:12.734901936534243. The val loss is 1132.6087731406888 with std:12034.879114431067.\n",
      "1132.6087731406888 2.030917620904737 5\n",
      "The training loss is 5.7182805008715345 with std:11.003654475205545. The val loss is 8.039142344571871 with std:14.052804554724439.\n",
      "8.039142344571871 2.030917620904737 5\n",
      "The training loss is 5.476869853973806 with std:9.241055436294769. The val loss is 8.271693847714392 with std:20.54188556263547.\n",
      "8.271693847714392 2.030917620904737 5\n",
      "Evaluating for {'degree': 5, 'lmda': 2.6489692876105297} ...\n",
      "The training loss is 5.680453656276676 with std:12.877400332443523. The val loss is 841.149785276875 with std:8919.369883383006.\n",
      "841.149785276875 2.6489692876105297 5\n",
      "The training loss is 5.792604874727752 with std:11.22731069620866. The val loss is 8.018347611678603 with std:13.980224433831706.\n",
      "8.018347611678603 2.6489692876105297 5\n",
      "The training loss is 5.591905799950307 with std:9.457140989596757. The val loss is 8.205318782383722 with std:20.233226607792247.\n",
      "8.205318782383722 2.6489692876105297 5\n",
      "Evaluating for {'degree': 5, 'lmda': 3.4551072945922217} ...\n",
      "The training loss is 5.792705383084759 with std:13.073559117814773. The val loss is 590.6104255853037 with std:6237.749711667141.\n",
      "590.6104255853037 3.4551072945922217 5\n",
      "The training loss is 5.8855653372385754 with std:11.507021603227612. The val loss is 7.999479809036596 with std:13.894567427257277.\n",
      "7.999479809036596 3.4551072945922217 5\n",
      "The training loss is 5.725007963875333 with std:9.700963962975294. The val loss is 8.173102195684262 with std:20.059324370526355.\n",
      "8.173102195684262 3.4551072945922217 5\n",
      "Evaluating for {'degree': 5, 'lmda': 4.506570337745478} ...\n",
      "The training loss is 5.934503446006391 with std:13.340740026152428. The val loss is 392.5659093276127 with std:4115.094061482219.\n",
      "392.5659093276127 4.506570337745478 5\n",
      "The training loss is 6.008731737909535 with std:11.864565091025376. The val loss is 7.989011471322465 with std:13.798281903439168.\n",
      "7.989011471322465 4.506570337745478 5\n",
      "The training loss is 5.887155433253649 with std:9.98706654561374. The val loss is 8.187915101246631 with std:20.055227214515977.\n",
      "8.187915101246631 4.506570337745478 5\n",
      "Evaluating for {'degree': 5, 'lmda': 5.878016072274912} ...\n",
      "The training loss is 6.119818459671126 with std:13.701436564428187. The val loss is 247.77787697532844 with std:2560.3085012178544.\n",
      "247.77787697532844 5.878016072274912 5\n",
      "The training loss is 6.180096871752953 with std:12.328907910808834. The val loss is 7.99946771063098 with std:13.699409859785511.\n",
      "7.99946771063098 5.878016072274912 5\n",
      "The training loss is 6.095839232871459 with std:10.337560169672377. The val loss is 8.270445911425162 with std:20.263390487430478.\n",
      "8.270445911425162 5.878016072274912 5\n",
      "Evaluating for {'degree': 5, 'lmda': 7.666822074546214} ...\n",
      "The training loss is 6.372196723669994 with std:14.185561777803779. The val loss is 149.61482504753613 with std:1502.7356859240233.\n",
      "149.61482504753613 7.666822074546214 5\n",
      "The training loss is 6.427800434035609 with std:12.938064437824462. The val loss is 8.053203215164762 with std:13.61545091670847.\n",
      "8.053203215164762 7.666822074546214 5\n",
      "The training loss is 6.378645998527821 with std:10.785302224486207. The val loss is 8.453774789697498 with std:20.735340230362763.\n",
      "8.453774789697498 7.666822074546214 5\n",
      "Evaluating for {'degree': 5, 'lmda': 10.0} ...\n",
      "The training loss is 6.730510981206947 with std:14.833392438504879. The val loss is 87.89218405345237 with std:833.3097475489346.\n",
      "87.89218405345237 10.0 5\n",
      "The training loss is 6.795933066574976 with std:13.741309350175. The val loss is 8.1879695816616 with std:13.579546387501836.\n",
      "8.1879695816616 10.0 5\n",
      "The training loss is 6.778897901028327 with std:11.378285487495917. The val loss is 8.789805649838593 with std:21.53296368716455.\n",
      "8.789805649838593 10.0 5\n",
      "Evaluating for {'degree': 6, 'lmda': 0.01} ...\n",
      "The training loss is 4.572805428314505 with std:10.83848159179468. The val loss is 45014.15994069332 with std:495764.6603632011.\n",
      "45014.15994069332 0.01 6\n",
      "The training loss is 4.500877874104301 with std:8.825729034316431. The val loss is 8.998594511563706 with std:17.049929233099405.\n",
      "8.998594511563706 0.01 6\n",
      "The training loss is 3.940603588458181 with std:6.560697688958154. The val loss is 9.54991503865812 with std:24.540392843610956.\n",
      "9.54991503865812 0.01 6\n",
      "Evaluating for {'degree': 6, 'lmda': 0.013043213867190054} ...\n",
      "The training loss is 4.600890758523736 with std:10.942897544049352. The val loss is 26368.895232535157 with std:290062.8638511324.\n",
      "26368.895232535157 0.013043213867190054 6\n",
      "The training loss is 4.537883319819651 with std:8.919019215292295. The val loss is 8.861300483948472 with std:16.604631236486945.\n",
      "8.861300483948472 0.013043213867190054 6\n",
      "The training loss is 3.9699425601157827 with std:6.6239564093293035. The val loss is 9.519354710406576 with std:24.593012948655762.\n",
      "9.519354710406576 0.013043213867190054 6\n",
      "Evaluating for {'degree': 6, 'lmda': 0.017012542798525893} ...\n",
      "The training loss is 4.6316899772544415 with std:11.055701538240523. The val loss is 14186.192063419983 with std:155728.2937911828.\n",
      "14186.192063419983 0.017012542798525893 6\n",
      "The training loss is 4.577382741444029 with std:9.014694850734841. The val loss is 8.730875345277772 with std:16.20544323830147.\n",
      "8.730875345277772 0.017012542798525893 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 4.001807543672504 with std:6.691179633078321. The val loss is 9.483444798034007 with std:24.60209742428702.\n",
      "9.483444798034007 0.017012542798525893 6\n",
      "Evaluating for {'degree': 6, 'lmda': 0.02218982341458972} ...\n",
      "The training loss is 4.664843421462611 with std:11.175755711113553. The val loss is 6899.65738568425 with std:75447.45462359364.\n",
      "6899.65738568425 0.02218982341458972 6\n",
      "The training loss is 4.618900397432003 with std:9.110875123976234. The val loss is 8.608877338737749 with std:15.854834682625258.\n",
      "8.608877338737749 0.02218982341458972 6\n",
      "The training loss is 4.0358942180839925 with std:6.761100843715456. The val loss is 9.440913609000138 with std:24.56001358875525.\n",
      "9.440913609000138 0.02218982341458972 6\n",
      "Evaluating for {'degree': 6, 'lmda': 0.028942661247167517} ...\n",
      "The training loss is 4.700027384573766 with std:11.301659052648931. The val loss is 2982.2021323581025 with std:32344.49810243063.\n",
      "2982.2021323581025 0.028942661247167517 6\n",
      "The training loss is 4.662028233207701 with std:9.205516571765234. The val loss is 8.496275455767273 with std:15.552035190095381.\n",
      "8.496275455767273 0.028942661247167517 6\n",
      "The training loss is 4.071904631446288 with std:6.832454687234618. The val loss is 9.390686899013616 with std:24.462233595351734.\n",
      "9.390686899013616 0.028942661247167517 6\n",
      "Evaluating for {'degree': 6, 'lmda': 0.037750532053243954} ...\n",
      "The training loss is 4.73700981325111 with std:11.431732500784879. The val loss is 1140.7165194563308 with std:12130.90943086361.\n",
      "1140.7165194563308 0.037750532053243954 6\n",
      "The training loss is 4.70650447718312 with std:9.296659471729173. The val loss is 8.393751178264353 with std:15.294139871438347.\n",
      "8.393751178264353 0.037750532053243954 6\n",
      "The training loss is 4.1097186493578315 with std:6.904346181383454. The val loss is 9.3324314022757 with std:24.308462043553785.\n",
      "9.3324314022757 0.037750532053243954 6\n",
      "Evaluating for {'degree': 6, 'lmda': 0.04923882631706739} ...\n",
      "The training loss is 4.775670217370236 with std:11.563998836195196. The val loss is 413.480223578071 with std:4180.800298993403.\n",
      "413.480223578071 0.04923882631706739 6\n",
      "The training loss is 4.752274781481055 with std:9.382771859829676. The val loss is 8.301928281610532 with std:15.07703131983651.\n",
      "8.301928281610532 0.04923882631706739 6\n",
      "The training loss is 4.149554582613336 with std:6.976585662362785. The val loss is 9.267003701237218 with std:24.103014052152908.\n",
      "9.267003701237218 0.04923882631706739 6\n",
      "Evaluating for {'degree': 6, 'lmda': 0.0642232542222936} ...\n",
      "The training loss is 4.815984365393752 with std:11.69619020455289. The val loss is 189.18889060043796 with std:1740.0423191907728.\n",
      "189.18889060043796 0.0642232542222936 6\n",
      "The training loss is 4.799523823265249 with std:9.463121737154681. The val loss is 8.221521546303093 with std:14.896198181539138.\n",
      "8.221521546303093 0.0642232542222936 6\n",
      "The training loss is 4.192058185867704 with std:7.049892021608446. The val loss is 9.19661001549761 with std:23.854411115699776.\n",
      "9.19661001549761 0.0642232542222936 6\n",
      "Evaluating for {'degree': 6, 'lmda': 0.0837677640068292} ...\n",
      "The training loss is 4.857978959974945 with std:11.82580726629666. The val loss is 168.3746185929955 with std:1488.4524665445572.\n",
      "168.3746185929955 0.0837677640068292 6\n",
      "The training loss is 4.848656204408319 with std:9.53806439477966. The val loss is 8.153385045630237 with std:14.747418918371013.\n",
      "8.153385045630237 0.0837677640068292 6\n",
      "The training loss is 4.238275563239966 with std:7.125901114347881. The val loss is 9.124581563009142 with std:23.574655946427125.\n",
      "9.124581563009142 0.0837677640068292 6\n",
      "Evaluating for {'degree': 6, 'lmda': 0.10926008611173785} ...\n",
      "The training loss is 4.901664933649644 with std:11.950242804077277. The val loss is 283.9492198325463 with std:2671.5418097407323.\n",
      "283.9492198325463 0.10926008611173785 6\n",
      "The training loss is 4.900209115249696 with std:9.609129267916279. The val loss is 8.098424165956283 with std:14.627194647675955.\n",
      "8.098424165956283 0.10926008611173785 6\n",
      "The training loss is 4.289497506668862 with std:7.206965569196679. The val loss is 9.054808108197072 with std:23.278805526737028.\n",
      "9.054808108197072 0.10926008611173785 6\n",
      "Evaluating for {'degree': 6, 'lmda': 0.14251026703029984} ...\n",
      "The training loss is 4.946966918871997 with std:12.066974594131626. The val loss is 597.8014433669368 with std:5985.89942254464.\n",
      "597.8014433669368 0.14251026703029984 6\n",
      "The training loss is 4.9547034201400555 with std:9.67884278822215. The val loss is 8.057358696484078 with std:14.532842080073959.\n",
      "8.057358696484078 0.14251026703029984 6\n",
      "The training loss is 4.347002589521559 with std:7.295783505751261. The val loss is 8.99097153898144 with std:22.98496892127075.\n",
      "8.99097153898144 0.14251026703029984 6\n",
      "Evaluating for {'degree': 6, 'lmda': 0.18587918911465645} ...\n",
      "The training loss is 4.99367965747875 with std:12.173821897483224. The val loss is 1195.7582514010444 with std:12395.079725645019.\n",
      "1195.7582514010444 0.18587918911465645 6\n",
      "The training loss is 5.0124749805794995 with std:9.750317533039972. The val loss is 8.03039113257706 with std:14.462255344722555.\n",
      "8.03039113257706 0.18587918911465645 6\n",
      "The training loss is 4.411766667067851 with std:7.39494243725246. The val loss is 8.935747856403282 with std:22.713949594603825.\n",
      "8.935747856403282 0.18587918911465645 6\n",
      "Evaluating for {'degree': 6, 'lmda': 0.24244620170823283} ...\n",
      "The training loss is 5.04148677153964 with std:12.26924085788242. The val loss is 2106.5557924113828 with std:22235.34526528084.\n",
      "2106.5557924113828 0.24244620170823283 6\n",
      "The training loss is 5.073549816042123 with std:9.826734498269206. The val loss is 8.016900327592895 with std:14.413438798272574.\n",
      "8.016900327592895 0.24244620170823283 6\n",
      "The training loss is 4.48423397410718 with std:7.5065024539838365. The val loss is 8.89014038272687 with std:22.487202587738953.\n",
      "8.89014038272687 0.24244620170823283 6\n",
      "Evaluating for {'degree': 6, 'lmda': 0.31622776601683794} ...\n",
      "The training loss is 5.090060721476458 with std:12.352613939986412. The val loss is 3264.1060113653466 with std:34801.931914717425.\n",
      "3264.1060113653466 0.31622776601683794 6\n",
      "The training loss is 5.137614657569343 with std:9.9108942959164. The val loss is 8.015291438461112 with std:14.38396461612841.\n",
      "8.015291438461112 0.31622776601683794 6\n",
      "The training loss is 4.5642390337579055 with std:7.631745932950394. The val loss is 8.853123520176474 with std:22.322354607392644.\n",
      "8.853123520176474 0.31622776601683794 6\n",
      "Evaluating for {'degree': 6, 'lmda': 0.41246263829013524} ...\n",
      "The training loss is 5.139229412886483 with std:12.42448599131922. The val loss is 4516.455271972795 with std:48444.35070722307.\n",
      "4516.455271972795 0.41246263829013524 6\n",
      "The training loss is 5.204092600115146 with std:10.004984442476356. The val loss is 8.023072319373867 with std:14.37051706424567.\n",
      "8.023072319373867 0.41246263829013524 6\n",
      "The training loss is 4.651116342741504 with std:7.771165823819171. The val loss is 8.821788274757592 with std:22.227331460804365.\n",
      "8.821788274757592 0.41246263829013524 6\n",
      "Evaluating for {'degree': 6, 'lmda': 0.5379838403443686} ...\n",
      "The training loss is 5.18916620098285 with std:12.486723641794894. The val loss is 5668.039982392989 with std:61027.7940091629.\n",
      "5668.039982392989 0.5379838403443686 6\n",
      "The training loss is 5.272292024738204 with std:10.110622497123611. The val loss is 8.03713090789039 with std:14.368655743552152.\n",
      "8.03713090789039 0.5379838403443686 6\n",
      "The training loss is 4.743954325227188 with std:7.9246645688210355. The val loss is 8.792088656868117 with std:22.1960986541842.\n",
      "8.792088656868117 0.5379838403443686 6\n",
      "Evaluating for {'degree': 6, 'lmda': 0.701703828670383} ...\n",
      "The training loss is 5.240554244250977 with std:12.542610800995053. The val loss is 6533.388399188271 with std:70521.50105956547.\n",
      "6533.388399188271 0.701703828670383 6\n",
      "The training loss is 5.341587295656739 with std:10.229147469622617. The val loss is 8.054116391600001 with std:14.372866543761804.\n",
      "8.054116391600001 0.701703828670383 6\n",
      "The training loss is 4.841891399340511 with std:8.091854697603159. The val loss is 8.760055469548986 with std:22.209279148969745.\n",
      "8.760055469548986 0.701703828670383 6\n",
      "Evaluating for {'degree': 6, 'lmda': 0.9152473108773893} ...\n",
      "The training loss is 5.294693029770421 with std:12.596914584845702. The val loss is 6982.520648576271 with std:75497.10144701938.\n",
      "6982.520648576271 0.9152473108773893 6\n",
      "The training loss is 5.411615894325634 with std:10.362094098376042. The val loss is 8.070813671470928 with std:14.376879149597999.\n",
      "8.070813671470928 0.9152473108773893 6\n",
      "The training loss is 4.944362074444598 with std:8.272357361547922. The val loss is 8.723096298428425 with std:22.24049883630865.\n",
      "8.723096298428425 0.9152473108773893 6\n",
      "Evaluating for {'degree': 6, 'lmda': 1.1937766417144369} ...\n",
      "The training loss is 5.353543032496987 with std:12.655952345171919. The val loss is 6966.408935368644 with std:75408.92358512647.\n",
      "6966.408935368644 1.1937766417144369 6\n",
      "The training loss is 5.482513074345142 with std:10.511796681742991. The val loss is 8.084440575507154 with std:14.374139829138718.\n",
      "8.084440575507154 1.1937766417144369 6\n",
      "The training loss is 5.051279892202104 with std:8.466087898297536. The val loss is 8.680938586018694 with std:22.265796640860344.\n",
      "8.680938586018694 1.1937766417144369 6\n",
      "Evaluating for {'degree': 6, 'lmda': 1.5570684047537318} ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 5.419738884529931 with std:12.727679209940751. The val loss is 6519.401199589302 with std:70622.11124911689.\n",
      "6519.401199589302 1.5570684047537318 6\n",
      "The training loss is 5.555224912092338 with std:10.682096777949678. The val loss is 8.092864914406693 with std:14.358289714967986.\n",
      "8.092864914406693 1.5570684047537318 6\n",
      "The training loss is 5.163239076495865 with std:8.673622965222936. The val loss is 8.635979295289696 with std:22.27176521072241.\n",
      "8.635979295289696 1.5570684047537318 6\n",
      "Evaluating for {'degree': 6, 'lmda': 2.030917620904737} ...\n",
      "The training loss is 5.49664810441595 with std:12.821826360983692. The val loss is 5741.888059823222 with std:62224.40433032626.\n",
      "5741.888059823222 2.030917620904737 6\n",
      "The training loss is 5.6319440693303715 with std:10.879149646357975. The val loss is 8.094813579976364 with std:14.323561680236573.\n",
      "8.094813579976364 2.030917620904737 6\n",
      "The training loss is 5.281866608729415 with std:8.89678194962575. The val loss is 8.593160915353995 with std:22.25946875828543.\n",
      "8.593160915353995 2.030917620904737 6\n",
      "Evaluating for {'degree': 6, 'lmda': 2.6489692876105297} ...\n",
      "The training loss is 5.58861638079572 with std:12.950165348119658. The val loss is 4770.578674512774 with std:51702.16334919041.\n",
      "4770.578674512774 2.6489692876105297 6\n",
      "The training loss is 5.716727766076838 with std:11.112347323050308. The val loss is 8.090222774266408 with std:14.265172927907928.\n",
      "8.090222774266408 2.6489692876105297 6\n",
      "The training loss is 5.410447442144024 with std:9.13952027118238. The val loss is 8.559791245581703 with std:22.244106639615755.\n",
      "8.559791245581703 2.6489692876105297 6\n",
      "Evaluating for {'degree': 6, 'lmda': 3.4551072945922217} ...\n",
      "The training loss is 5.701614636861673 with std:13.127034955295805. The val loss is 3745.679510022752 with std:40582.219779009065.\n",
      "3745.679510022752 3.4551072945922217 6\n",
      "The training loss is 5.81641486962853 with std:11.395418391577243. The val loss is 8.08095887533713 with std:14.179993962714034.\n",
      "8.08095887533713 3.4551072945922217 6\n",
      "The training loss is 5.554927711314154 with std:9.409192378720922. The val loss is 8.545842645566351 with std:22.25259597357308.\n",
      "8.545842645566351 3.4551072945922217 6\n",
      "Evaluating for {'degree': 6, 'lmda': 4.506570337745478} ...\n",
      "The training loss is 5.844574238594115 with std:13.370312401904979. The val loss is 2783.703624287508 with std:30134.881438800046.\n",
      "2783.703624287508 4.506570337745478 6\n",
      "The training loss is 5.942067097592467 with std:11.74781541053098. The val loss is 8.072221200116498 with std:14.067940346913758.\n",
      "8.072221200116498 4.506570337745478 6\n",
      "The training loss is 5.725446278580533 with std:9.718266748682778. The val loss is 8.565237532503032 with std:22.321682028101808.\n",
      "8.565237532503032 4.506570337745478 6\n",
      "Evaluating for {'degree': 6, 'lmda': 5.878016072274912} ...\n",
      "The training loss is 6.031763006678398 with std:13.703018296442988. The val loss is 1961.639715079298 with std:21200.6805338126.\n",
      "1961.639715079298 5.878016072274912 6\n",
      "The training loss is 6.1113045748754375 with std:12.1965208650948. The val loss is 8.075015268029226 with std:13.934632410710346.\n",
      "8.075015268029226 5.878016072274912 6\n",
      "The training loss is 5.938688834064059 with std:10.086666893702361. The val loss is 8.638564041418242 with std:22.49817524949182.\n",
      "8.638564041418242 5.878016072274912 6\n",
      "Evaluating for {'degree': 6, 'lmda': 7.666822074546214} ...\n",
      "The training loss is 6.2866377518430525 with std:14.155707455429908. The val loss is 1313.8958922486615 with std:14156.397992311608.\n",
      "1313.8958922486615 7.666822074546214 6\n",
      "The training loss is 6.352074131999776 with std:12.778355113505171. The val loss is 8.110165826433114 with std:13.795954342600806.\n",
      "8.110165826433114 7.666822074546214 6\n",
      "The training loss is 6.221579375054011 with std:10.545004554470548. The val loss is 8.797664406169682 with std:22.84135394247527.\n",
      "8.797664406169682 7.666822074546214 6\n",
      "Evaluating for {'degree': 6, 'lmda': 10.0} ...\n",
      "The training loss is 6.647744067090405 with std:14.769706172473827. The val loss is 839.4789018249055 with std:8992.774095723937.\n",
      "839.4789018249055 10.0 6\n",
      "The training loss is 6.708580424096812 with std:13.542759503338871. The val loss is 8.214455951863217 with std:13.685305414259707.\n",
      "8.214455951863217 10.0 6\n",
      "The training loss is 6.617074054229775 with std:11.138953491121606. The val loss is 9.092643413978143 with std:23.42630970555846.\n",
      "9.092643413978143 10.0 6\n",
      "Evaluating for {'degree': 7, 'lmda': 0.01} ...\n",
      "The training loss is 4.4577659242161225 with std:10.76348145751315. The val loss is 884217.6974230363 with std:9803051.377981905.\n",
      "884217.6974230363 0.01 7\n",
      "The training loss is 4.390464426880323 with std:8.652736382430174. The val loss is 9.608632890616423 with std:19.247242235398346.\n",
      "9.608632890616423 0.01 7\n",
      "The training loss is 3.8187686108432195 with std:6.469243705394727. The val loss is 11.291666886388969 with std:33.676484277678284.\n",
      "11.291666886388969 0.01 7\n",
      "Evaluating for {'degree': 7, 'lmda': 0.013043213867190054} ...\n",
      "The training loss is 4.499899505641233 with std:10.879574283809859. The val loss is 619533.6018195528 with std:6867401.079790004.\n",
      "619533.6018195528 0.013043213867190054 7\n",
      "The training loss is 4.432769490451964 with std:8.765327430249101. The val loss is 9.408483486637484 with std:18.43584617263354.\n",
      "9.408483486637484 0.013043213867190054 7\n",
      "The training loss is 3.8535867845629226 with std:6.533294342021749. The val loss is 11.117932393290664 with std:32.61198053422361.\n",
      "11.117932393290664 0.013043213867190054 7\n",
      "Evaluating for {'degree': 7, 'lmda': 0.017012542798525893} ...\n",
      "The training loss is 4.5432224733424516 with std:11.002476140681733. The val loss is 413100.0436317698 with std:4578021.184466999.\n",
      "413100.0436317698 0.017012542798525893 7\n",
      "The training loss is 4.476766671546418 with std:8.880041455317379. The val loss is 9.221155916727927 with std:17.700537896210694.\n",
      "9.221155916727927 0.017012542798525893 7\n",
      "The training loss is 3.890360412825657 with std:6.600119544660245. The val loss is 10.933844175141013 with std:31.481400027146925.\n",
      "10.933844175141013 0.017012542798525893 7\n",
      "Evaluating for {'degree': 7, 'lmda': 0.02218982341458972} ...\n",
      "The training loss is 4.587577412121537 with std:11.13173864499821. The val loss is 261518.87978423925 with std:2897156.950697384.\n",
      "261518.87978423925 0.02218982341458972 7\n",
      "The training loss is 4.522056029584727 with std:8.994984436099418. The val loss is 9.049440029414768 with std:17.057469546125066.\n",
      "9.049440029414768 0.02218982341458972 7\n",
      "The training loss is 3.9287314633558 with std:6.668869623616403. The val loss is 10.744450986519022 with std:30.330569194187984.\n",
      "10.744450986519022 0.02218982341458972 7\n",
      "Evaluating for {'degree': 7, 'lmda': 0.028942661247167517} ...\n",
      "The training loss is 4.632689575280338 with std:11.266253735023689. The val loss is 156899.67916390684 with std:1737227.0214955278.\n",
      "156899.67916390684 0.028942661247167517 7\n",
      "The training loss is 4.568119820888485 with std:9.107828364990922. The val loss is 8.894721451287811 with std:16.513169961377642.\n",
      "8.894721451287811 0.028942661247167517 7\n",
      "The training loss is 3.968200004884775 with std:6.738455002683017. The val loss is 10.554320028031693 with std:29.20458772946768.\n",
      "10.554320028031693 0.028942661247167517 7\n",
      "Evaluating for {'degree': 7, 'lmda': 0.037750532053243954} ...\n",
      "The training loss is 4.678254443998372 with std:11.404360608132171. The val loss is 89210.3767867809 with std:986905.3972179984.\n",
      "89210.3767867809 0.037750532053243954 7\n",
      "The training loss is 4.614464437599012 with std:9.216082555471942. The val loss is 8.757095177121748 with std:16.06543621054884.\n",
      "8.757095177121748 0.037750532053243954 7\n",
      "The training loss is 4.008296497275556 with std:6.807823219817727. The val loss is 10.367324185089661 with std:28.142288153413084.\n",
      "10.367324185089661 0.037750532053243954 7\n",
      "Evaluating for {'degree': 7, 'lmda': 0.04923882631706739} ...\n",
      "The training loss is 4.724016873925974 with std:11.54395570610151. The val loss is 48313.16650912263 with std:533706.2735943468.\n",
      "48313.16650912263 0.04923882631706739 7\n",
      "The training loss is 4.660750117729248 with std:9.317440240695253. The val loss is 8.635706327731432 with std:15.70544641063866.\n",
      "8.635706327731432 0.04923882631706739 7\n",
      "The training loss is 4.048778899884767 with std:6.876280935381534. The val loss is 10.186780735183415 with std:27.173316974473863.\n",
      "10.186780735183415 0.04923882631706739 7\n",
      "Evaluating for {'degree': 7, 'lmda': 0.0642232542222936} ...\n",
      "The training loss is 4.769824392090251 with std:11.682610353870341. The val loss is 25350.759041442205 with std:279358.267760275.\n",
      "25350.759041442205 0.0642232542222936 7\n",
      "The training loss is 4.706895743308775 with std:9.410173166983908. The val loss is 8.529207045691164 with std:15.420481417754072.\n",
      "8.529207045691164 0.0642232542222936 7\n",
      "The training loss is 4.089807713362131 with std:6.943791248341858. The val loss is 10.015736835188452 with std:26.317270904296503.\n",
      "10.015736835188452 0.0642232542222936 7\n",
      "Evaluating for {'degree': 7, 'lmda': 0.0837677640068292} ...\n",
      "The training loss is 4.815648798255492 with std:11.817709361771323. The val loss is 13438.669115989092 with std:147479.26881292276.\n",
      "13438.669115989092 0.0837677640068292 7\n",
      "The training loss is 4.753147785267692 with std:9.493521807820592. The val loss is 8.436231883395662 with std:15.196684609520887.\n",
      "8.436231883395662 0.0837677640068292 7\n",
      "The training loss is 4.1320542740263075 with std:7.011181948320328. The val loss is 9.857133079912865 with std:25.584076811826694.\n",
      "9.857133079912865 0.0837677640068292 7\n",
      "Evaluating for {'degree': 7, 'lmda': 0.10926008611173785} ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 4.861573780472501 with std:11.94661671907315. The val loss is 7797.880103239246 with std:85045.28716745214.\n",
      "7797.880103239246 0.10926008611173785 7\n",
      "The training loss is 4.800094089933887 with std:9.568000388393163. The val loss is 8.355779568124417 with std:15.021293212932994.\n",
      "8.355779568124417 0.10926008611173785 7\n",
      "The training loss is 4.1767109997738165 with std:7.080221756750166. The val loss is 9.713667344602303 with std:24.975398239743107.\n",
      "9.713667344602303 0.10926008611173785 7\n",
      "Evaluating for {'degree': 7, 'lmda': 0.14251026703029984} ...\n",
      "The training loss is 4.9077486286082515 with std:12.066866188629257. The val loss is 5511.862624130117 with std:59695.628274056166.\n",
      "5511.862624130117 0.14251026703029984 7\n",
      "The training loss is 4.848601188910181 with std:9.63552546780426. The val loss is 8.287382297453373 with std:14.883918125191181.\n",
      "8.287382297453373 0.14251026703029984 7\n",
      "The training loss is 4.225389970447353 with std:7.153544714104941. The val loss is 9.587363614915635 with std:24.487163685181002.\n",
      "9.587363614915635 0.14251026703029984 7\n",
      "Evaluating for {'degree': 7, 'lmda': 0.18587918911465645} ...\n",
      "The training loss is 4.954319218496916 with std:12.176373031944111. The val loss is 5073.151279597697 with std:54706.94088385714.\n",
      "5073.151279597697 0.18587918911465645 7\n",
      "The training loss is 4.8996707270823245 with std:9.699300400510138. The val loss is 8.2309912016511 with std:14.776764468208096.\n",
      "8.2309912016511 0.18587918911465645 7\n",
      "The training loss is 4.279914919974638 with std:7.234419521880597. The val loss is 9.47900636975076 with std:24.112553353715647.\n",
      "9.47900636975076 0.18587918911465645 7\n",
      "Evaluating for {'degree': 7, 'lmda': 0.24244620170823283} ...\n",
      "The training loss is 5.001365389627541 with std:12.273661484518918. The val loss is 5856.406126884645 with std:63154.218004383045.\n",
      "5856.406126884645 0.24244620170823283 7\n",
      "The training loss is 4.954242477087994 with std:9.763448788671472. The val loss is 8.186608717125338 with std:14.694019139250779.\n",
      "8.186608717125338 0.24244620170823283 7\n",
      "The training loss is 4.342034724395529 with std:7.3263785121149665. The val loss is 9.387647956742043 with std:23.843605977824037.\n",
      "9.387647956742043 0.24244620170823283 7\n",
      "Evaluating for {'degree': 7, 'lmda': 0.31622776601683794} ...\n",
      "The training loss is 5.048883669875833 with std:12.358093991320512. The val loss is 7634.100510449456 with std:82543.1497922991.\n",
      "7634.100510449456 0.31622776601683794 7\n",
      "The training loss is 5.013000070624849 with std:9.832474806056494. The val loss is 8.153806531388089 with std:14.63082642198613.\n",
      "8.153806531388089 0.31622776601683794 7\n",
      "The training loss is 4.413116610567947 with std:7.432754359745041. The val loss is 9.31035713710882 with std:23.66964875385154.\n",
      "9.31035713710882 0.31622776601683794 7\n",
      "Evaluating for {'degree': 7, 'lmda': 0.41246263829013524} ...\n",
      "The training loss is 5.096842774968552 with std:12.4300797929607. The val loss is 10223.728636871392 with std:110895.53407089636.\n",
      "10223.728636871392 0.41246263829013524 7\n",
      "The training loss is 5.076241947940905 with std:9.910704193840894. The val loss is 8.131321348994833 with std:14.582286365194367.\n",
      "8.131321348994833 0.41246263829013524 7\n",
      "The training loss is 4.493904118387947 with std:7.5562189601027825. The val loss is 9.24233418180242 with std:23.572836066772076.\n",
      "9.24233418180242 0.41246263829013524 7\n",
      "Evaluating for {'degree': 7, 'lmda': 0.5379838403443686} ...\n",
      "The training loss is 5.145310865391761 with std:12.491246836924267. The val loss is 13309.58739587459 with std:144765.84027170195.\n",
      "13309.58739587459 0.5379838403443686 7\n",
      "The training loss is 5.143859426594342 with std:10.001885557484464. The val loss is 8.116889287851441 with std:14.542796277002756.\n",
      "8.116889287851441 0.5379838403443686 7\n",
      "The training loss is 4.584424142729238 with std:7.698447962934578. The val loss is 9.177490316521446 with std:23.523982275617932.\n",
      "9.177490316521446 0.5379838403443686 7\n",
      "Evaluating for {'degree': 7, 'lmda': 0.701703828670383} ...\n",
      "The training loss is 5.19462863381664 with std:12.544584575791514. The val loss is 16426.8604868702 with std:179054.98724760374.\n",
      "16426.8604868702 0.701703828670383 7\n",
      "The training loss is 5.215432079523656 with std:10.109088919798719. The val loss is 8.107381562295798 with std:14.505889499715908.\n",
      "8.107381562295798 0.701703828670383 7\n",
      "The training loss is 4.684085052119495 with std:7.860015470394932. The val loss is 9.109503070745589 with std:23.482992607714053.\n",
      "9.109503070745589 0.701703828670383 7\n",
      "Evaluating for {'degree': 7, 'lmda': 0.9152473108773893} ...\n",
      "The training loss is 5.245598187623722 with std:12.594588715167534. The val loss is 19053.189124104054 with std:208015.63565941222.\n",
      "19053.189124104054 0.9152473108773893 7\n",
      "The training loss is 5.290431827360997 with std:10.234953010057344. The val loss is 8.099194644270552 with std:14.46456196694451.\n",
      "8.099194644270552 0.9152473108773893 7\n",
      "The training loss is 4.791946159355259 with std:8.040565321562465. The val loss is 9.03317701411643 with std:23.405940258154143.\n",
      "9.03317701411643 0.9152473108773893 7\n",
      "Evaluating for {'degree': 7, 'lmda': 1.1937766417144369} ...\n",
      "The training loss is 5.299673476113562 with std:12.647440138436005. The val loss is 20737.9913619748 with std:226678.07839181065.\n",
      "20737.9913619748 1.1937766417144369 7\n",
      "The training loss is 5.368530879574009 with std:10.382253841340837. The val loss is 8.088782856498137 with std:14.41194664436263.\n",
      "8.088782856498137 1.1937766417144369 7\n",
      "The training loss is 4.907105269804344 with std:8.239254908833177. The val loss is 8.94576676165133 with std:23.25663283013586.\n",
      "8.94576676165133 1.1937766417144369 7\n",
      "Evaluating for {'degree': 7, 'lmda': 1.5570684047537318} ...\n",
      "The training loss is 5.359161314774812 with std:12.711225265187192. The val loss is 21210.36836880934 with std:232042.6063322029.\n",
      "21210.36836880934 1.5570684047537318 7\n",
      "The training loss is 5.4500231247919935 with std:10.55472596327212. The val loss is 8.073223568655765 with std:14.342125765824647.\n",
      "8.073223568655765 1.5570684047537318 7\n",
      "The training loss is 5.029171645580606 with std:8.455456069032925. The val loss is 8.847915065580944 with std:23.017733328038872.\n",
      "8.847915065580944 1.5570684047537318 7\n",
      "Evaluating for {'degree': 7, 'lmda': 2.030917620904737} ...\n",
      "The training loss is 5.4274680056806295 with std:12.796179344923013. The val loss is 20430.5159077164 with std:223650.1972117532.\n",
      "20430.5159077164 2.030917620904737 7\n",
      "The training loss is 5.536380737331042 with std:10.758064184845466. The val loss is 8.050764481383796 with std:14.25088995959438.\n",
      "8.050764481383796 2.030917620904737 7\n",
      "The training loss is 5.15884470488642 with std:8.689707796295458. The val loss is 8.744061683842421 with std:22.697072006049616.\n",
      "8.744061683842421 2.030917620904737 7\n",
      "Evaluating for {'degree': 7, 'lmda': 2.6489692876105297} ...\n",
      "The training loss is 5.509465815632589 with std:12.914947764120486. The val loss is 18575.99834086082 with std:203435.34300545562.\n",
      "18575.99834086082 2.6489692876105297 7\n",
      "The training loss is 5.630986600727001 with std:11.001057923914674. The val loss is 8.021398225384317 with std:14.136373780588093.\n",
      "8.021398225384317 2.6489692876105297 7\n",
      "The training loss is 5.298666602209046 with std:8.94491767893317. The val loss is 8.642473315671872 with std:22.327652965596844.\n",
      "8.642473315671872 2.6489692876105297 7\n",
      "Evaluating for {'degree': 7, 'lmda': 3.4551072945922217} ...\n",
      "The training loss is 5.612122411868911 with std:13.082936113068925. The val loss is 15976.21117840515 with std:175007.31782845352.\n",
      "15976.21117840515 3.4551072945922217 7\n",
      "The training loss is 5.740136238098631 with std:11.296859820863201. The val loss is 7.987629122175154 with std:13.999695885327924.\n",
      "7.987629122175154 3.4551072945922217 7\n",
      "The training loss is 5.454057003958716 with std:9.227814401711015. The val loss is 8.5552912566336 with std:21.96274203189144.\n",
      "8.5552912566336 3.4551072945922217 7\n",
      "Evaluating for {'degree': 7, 'lmda': 4.506570337745478} ...\n",
      "The training loss is 5.745642943685161 with std:13.31893844293462. The val loss is 13021.577971862613 with std:142652.05925696038.\n",
      "13021.577971862613 4.506570337745478 7\n",
      "The training loss is 5.874510751952608 with std:11.664456035844395. The val loss is 7.955733299360064 with std:13.845959566474674.\n",
      "7.955733299360064 4.506570337745478 7\n",
      "The training loss is 5.634809554508633 with std:9.550704109202835. The val loss is 8.499142216637798 with std:21.66892687418398.\n",
      "8.499142216637798 4.506570337745478 7\n",
      "Evaluating for {'degree': 7, 'lmda': 5.878016072274912} ...\n",
      "The training loss is 5.92551475151507 with std:13.646338282079087. The val loss is 10076.325321251 with std:110372.29268626332.\n",
      "10076.325321251 5.878016072274912 7\n",
      "The training loss is 6.0514812868209304 with std:12.130455031368005. The val loss is 7.937953924277291 with std:13.686174843588327.\n",
      "7.937953924277291 5.878016072274912 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 5.857370624895472 with std:9.933690481242563. The val loss is 8.496943979494782 with std:21.52003669660339.\n",
      "8.496943979494782 5.878016072274912 7\n",
      "Evaluating for {'degree': 7, 'lmda': 7.666822074546214} ...\n",
      "The training loss is 6.176005999573025 with std:14.095201963915802. The val loss is 7416.498514256367 with std:81204.57860028186.\n",
      "7416.498514256367 7.666822074546214 7\n",
      "The training loss is 6.298797142473818 with std:12.731297579998861. The val loss is 7.9562042579352354 with std:13.540821652964318.\n",
      "7.9562042579352354 7.666822074546214 7\n",
      "The training loss is 6.148434714703725 with std:10.407627997940944. The val loss is 8.581613173685634 with std:21.59377952971129.\n",
      "8.581613173685634 7.666822074546214 7\n",
      "Evaluating for {'degree': 7, 'lmda': 10.0} ...\n",
      "The training loss is 6.535835658290109 with std:14.705477871450903. The val loss is 5202.885713663554 with std:56920.01844525878.\n",
      "5202.885713663554 10.0 7\n",
      "The training loss is 6.660418840558581 with std:13.51589065551816. The val loss is 8.047975684892227 with std:13.445882709214041.\n",
      "8.047975684892227 10.0 7\n",
      "The training loss is 6.5506451568331405 with std:11.018081308797253. The val loss is 8.802483223630482 with std:21.97155339587587.\n",
      "8.802483223630482 10.0 7\n",
      "Evaluating for {'degree': 8, 'lmda': 0.01} ...\n",
      "The training loss is 4.305628075782325 with std:10.349026800787302. The val loss is 7608527.488268324 with std:84615513.54070973.\n",
      "7608527.488268324 0.01 8\n",
      "The training loss is 4.244078749688658 with std:8.373270266865338. The val loss is 11.884020544448758 with std:31.45048453747541.\n",
      "11.884020544448758 0.01 8\n",
      "The training loss is 3.7178452097953616 with std:6.354752712106808. The val loss is 12.868410413483486 with std:47.97300268862425.\n",
      "12.868410413483486 0.01 8\n",
      "Evaluating for {'degree': 8, 'lmda': 0.013043213867190054} ...\n",
      "The training loss is 4.351213813748012 with std:10.493701773181837. The val loss is 5926315.6919772 with std:65903874.07027418.\n",
      "5926315.6919772 0.013043213867190054 8\n",
      "The training loss is 4.293890730979719 with std:8.502901982317109. The val loss is 11.378915965616656 with std:28.397757929826316.\n",
      "11.378915965616656 0.013043213867190054 8\n",
      "The training loss is 3.7512558131012166 with std:6.415477323062106. The val loss is 12.594349228056464 with std:45.808602317521434.\n",
      "12.594349228056464 0.013043213867190054 8\n",
      "Evaluating for {'degree': 8, 'lmda': 0.017012542798525893} ...\n",
      "The training loss is 4.398154823217097 with std:10.644008845787829. The val loss is 4417267.7041778695 with std:49118949.774283625.\n",
      "4417267.7041778695 0.017012542798525893 8\n",
      "The training loss is 4.343112548323491 with std:8.633833453025472. The val loss is 10.929534756324081 with std:25.827692234808673.\n",
      "10.929534756324081 0.017012542798525893 8\n",
      "The training loss is 3.7858274239145944 with std:6.478002119600484. The val loss is 12.311707158835226 with std:43.594044457557104.\n",
      "12.311707158835226 0.017012542798525893 8\n",
      "Evaluating for {'degree': 8, 'lmda': 0.02218982341458972} ...\n",
      "The training loss is 4.446563305695344 with std:10.799819055375538. The val loss is 3148250.945829781 with std:35004430.219311945.\n",
      "3148250.945829781 0.02218982341458972 8\n",
      "The training loss is 4.391892011606454 with std:8.76502349633811. The val loss is 10.533067884689425 with std:23.68919985809813.\n",
      "10.533067884689425 0.02218982341458972 8\n",
      "The training loss is 3.8217816008380505 with std:6.542243128661047. The val loss is 12.025235869385364 with std:41.36006546043552.\n",
      "12.025235869385364 0.02218982341458972 8\n",
      "Evaluating for {'degree': 8, 'lmda': 0.028942661247167517} ...\n",
      "The training loss is 4.4962728754841175 with std:10.96040344524607. The val loss is 2145375.797777539 with std:23850598.399232976.\n",
      "2145375.797777539 0.028942661247167517 8\n",
      "The training loss is 4.440223055099476 with std:8.894927324774919. The val loss is 10.18504186825284 with std:21.923741329396066.\n",
      "10.18504186825284 0.028942661247167517 8\n",
      "The training loss is 3.8591189044302276 with std:6.60782071801557. The val loss is 11.739313062190806 with std:39.14325542416243.\n",
      "11.739313062190806 0.028942661247167517 8\n",
      "Evaluating for {'degree': 8, 'lmda': 0.037750532053243954} ...\n",
      "The training loss is 4.5469216162400405 with std:11.12456424492431. The val loss is 1399269.67769726 with std:15553033.27535276.\n",
      "1399269.67769726 0.037750532053243954 8\n",
      "The training loss is 4.488034976424509 with std:9.021611130098597. The val loss is 9.879973285431676 with std:20.473477219713807.\n",
      "9.879973285431676 0.037750532053243954 8\n",
      "The training loss is 3.897673499481349 with std:6.674140496776282. The val loss is 11.457354839566012 with std:36.98018275093143.\n",
      "11.457354839566012 0.037750532053243954 8\n",
      "Evaluating for {'degree': 8, 'lmda': 0.04923882631706739} ...\n",
      "The training loss is 4.5980780643515615 with std:11.290727636626366. The val loss is 875953.9113362116 with std:9733627.696938487.\n",
      "875953.9113362116 0.04923882631706739 8\n",
      "The training loss is 4.535300493596294 with std:9.142939959849032. The val loss is 9.612103023179003 with std:19.286185857716635.\n",
      "9.612103023179003 0.04923882631706739 8\n",
      "The training loss is 3.9372301150733415 with std:6.740564090802396. The val loss is 11.181710946648943 with std:34.904128574308544.\n",
      "11.181710946648943 0.04923882631706739 8\n",
      "Evaluating for {'degree': 8, 'lmda': 0.0642232542222936} ...\n",
      "The training loss is 4.649360096295854 with std:11.45696688337252. The val loss is 529455.6043096795 with std:5880892.288151941.\n",
      "529455.6043096795 0.0642232542222936 8\n",
      "The training loss is 4.582129558888745 with std:9.25683028840373. The val loss is 9.376052549523715 with std:18.317266159900832.\n",
      "9.376052549523715 0.0642232542222936 8\n",
      "The training loss is 3.9776881735727705 with std:6.806651950822028. The val loss is 10.914088345785 with std:32.9448906399821.\n",
      "10.914088345785 0.0642232542222936 8\n",
      "Evaluating for {'degree': 8, 'lmda': 0.0837677640068292} ...\n",
      "The training loss is 4.700510323614238 with std:11.620985048575228. The val loss is 312607.83376354596 with std:3470092.868978899.\n",
      "312607.83376354596 0.0837677640068292 8\n",
      "The training loss is 4.628833455459285 with std:9.36156616018851. The val loss is 9.167285400908169 with std:17.529623658667223.\n",
      "9.167285400908169 0.0837677640068292 8\n",
      "The training loss is 4.019237124956245 with std:6.872435659298576. The val loss is 10.656279115641668 with std:31.1299991073901.\n",
      "10.656279115641668 0.0837677640068292 8\n",
      "Evaluating for {'degree': 8, 'lmda': 0.10926008611173785} ...\n",
      "The training loss is 4.751418889581035 with std:11.780128673939824. The val loss is 184158.9557010043 with std:2042322.757433162.\n",
      "184158.9557010043 0.10926008611173785 8\n",
      "The training loss is 4.675958505699485 with std:9.456166655929174. The val loss is 8.98234508203346 with std:16.892510020753228.\n",
      "8.98234508203346 0.10926008611173785 8\n",
      "The training loss is 4.0624951735264165 with std:6.9386620219614334. The val loss is 10.410819240883972 with std:29.48496606592374.\n",
      "10.410819240883972 0.10926008611173785 8\n",
      "Evaluating for {'degree': 8, 'lmda': 0.14251026703029984} ...\n",
      "The training loss is 4.802106549583169 with std:11.931501542400612. The val loss is 112065.62170131305 with std:1241122.6997521303.\n",
      "112065.62170131305 0.14251026703029984 8\n",
      "The training loss is 4.724287689487098 with std:9.54075563690707. The val loss is 8.818904281220028 with std:16.380250813714824.\n",
      "8.818904281220028 0.14251026703029984 8\n",
      "The training loss is 4.108566009089484 with std:7.006950135848911. The val loss is 10.181231366029351 with std:28.031314991338153.\n",
      "10.181231366029351 0.14251026703029984 8\n",
      "Evaluating for {'degree': 8, 'lmda': 0.18587918911465645} ...\n",
      "The training loss is 4.852687569848419 with std:12.072207705406141. The val loss is 73868.89350887864 with std:816659.4625254988.\n",
      "73868.89350887864 0.18587918911465645 8\n",
      "The training loss is 4.774797556663858 with std:9.616841784093106. The val loss is 8.67566070589216 with std:15.971288557992722.\n",
      "8.67566070589216 0.18587918911465645 8\n",
      "The training loss is 4.158984689445904 with std:7.0798141866193145. The val loss is 9.971691404943867 with std:26.783399095830323.\n",
      "9.971691404943867 0.18587918911465645 8\n",
      "Evaluating for {'degree': 8, 'lmda': 0.24244620170823283} ...\n",
      "The training loss is 4.903328915998973 with std:12.19970399884131. The val loss is 55302.23090747493 with std:610244.0392318044.\n",
      "55302.23090747493 0.24244620170823283 8\n",
      "The training loss is 4.828555992044559 with std:9.687400851482872. The val loss is 8.552082004634881 with std:15.647472599012751.\n",
      "8.552082004634881 0.24244620170823283 8\n",
      "The training loss is 4.215550646957175 with std:7.160524501727026. The val loss is 9.786214633254204 with std:25.746545917432403.\n",
      "9.786214633254204 0.24244620170823283 8\n",
      "Evaluating for {'degree': 8, 'lmda': 0.31622776601683794} ...\n",
      "The training loss is 4.954220876550679 with std:12.312208335710805. The val loss is 48112.8668083351 with std:530105.079565457.\n",
      "48112.8668083351 0.31622776601683794 8\n",
      "The training loss is 4.886566014771366 with std:9.756686693761083. The val loss is 8.447994617494478 with std:15.39336472572258.\n",
      "8.447994617494478 0.31622776601683794 8\n",
      "The training loss is 4.280077430868867 with std:7.252806464849017. The val loss is 9.627645915995034 with std:24.918106951691993.\n",
      "9.627645915995034 0.31622776601683794 8\n",
      "Evaluating for {'degree': 8, 'lmda': 0.41246263829013524} ...\n",
      "The training loss is 5.005576933006253 with std:12.409097753145666. The val loss is 47812.79606065045 with std:526379.006408223.\n",
      "47812.79606065045 0.41246263829013524 8\n",
      "The training loss is 4.949593571614706 with std:9.829782992384574. The val loss is 8.363055688556145 with std:15.195464933485647.\n",
      "8.363055688556145 0.41246263829013524 8\n",
      "The training loss is 4.354117251587576 with std:7.360413922262756. The val loss is 9.496775408717788 with std:24.290035064288467.\n",
      "9.496775408717788 0.41246263829013524 8\n",
      "Evaluating for {'degree': 8, 'lmda': 0.5379838403443686} ...\n",
      "The training loss is 5.057679265816281 with std:12.491237486964005. The val loss is 51827.81993699485 with std:570521.4835276847.\n",
      "51827.81993699485 0.5379838403443686 8\n",
      "The training loss is 5.018039981082365 with std:9.912015808493054. The val loss is 8.296217451962418 with std:15.041443953118304.\n",
      "8.296217451962418 0.5379838403443686 8\n",
      "The training loss is 4.438732485819219 with std:7.486657016100717. The val loss is 9.391824464952846 with std:23.84961979342646.\n",
      "9.391824464952846 0.5379838403443686 8\n",
      "Evaluating for {'degree': 8, 'lmda': 0.701703828670383} ...\n",
      "The training loss is 5.110976515648196 with std:12.561208440219874. The val loss is 58278.833079531454 with std:641741.8732194564.\n",
      "58278.833079531454 0.701703828670383 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 5.0919156911205015 with std:10.00842966628779. The val loss is 8.245338063966372 with std:14.919528235821183.\n",
      "8.245338063966372 0.701703828670383 8\n",
      "The training loss is 4.534377953858708 with std:7.633998181389676. The val loss is 9.308462294861876 with std:23.57651622754776.\n",
      "9.308462294861876 0.701703828670383 8\n",
      "Evaluating for {'degree': 8, 'lmda': 0.9152473108773893} ...\n",
      "The training loss is 5.166230561158825 with std:12.62344093539841. The val loss is 65410.74852760662 with std:720633.4117657129.\n",
      "65410.74852760662 0.9152473108773893 8\n",
      "The training loss is 5.170948262418415 with std:10.123541612292263. The val loss is 8.20707783852655 with std:14.818143548694625.\n",
      "8.20707783852655 0.9152473108773893 8\n",
      "The training loss is 4.640933185332271 with std:7.8038385884276975. The val loss is 9.24045453309197 with std:23.43834918873345.\n",
      "9.24045453309197 0.9152473108773893 8\n",
      "Evaluating for {'degree': 8, 'lmda': 1.1937766417144369} ...\n",
      "The training loss is 5.224709835668035 with std:12.684301209757207. The val loss is 71520.77477890914 with std:788354.6369273774.\n",
      "71520.77477890914 1.1937766417144369 8\n",
      "The training loss is 5.25483708289535 with std:10.261518599975561. The val loss is 8.177151695451599 with std:14.725885993484066.\n",
      "8.177151695451599 1.1937766417144369 8\n",
      "The training loss is 4.7579039599699495 with std:7.99660204732604. The val loss is 9.180940038048231 with std:23.39009093253489.\n",
      "9.180940038048231 1.1937766417144369 8\n",
      "Evaluating for {'degree': 8, 'lmda': 1.5570684047537318} ...\n",
      "The training loss is 5.288438042920965 with std:12.752191711513557. The val loss is 75162.18286049658 with std:828877.7776096041.\n",
      "75162.18286049658 1.5570684047537318 8\n",
      "The training loss is 5.343664406159124 with std:10.426809544586316. The val loss is 8.150921709908843 with std:14.631874995874115.\n",
      "8.150921709908843 1.5570684047537318 8\n",
      "The training loss is 4.884813457329401 with std:8.212197362691967. The val loss is 9.124143048910309 with std:23.38082449779168.\n",
      "9.124143048910309 1.5570684047537318 8\n",
      "Evaluating for {'degree': 8, 'lmda': 2.030917620904737} ...\n",
      "The training loss is 5.360523461566862 with std:12.837710485542116. The val loss is 75406.57342098436 with std:831895.2488257339.\n",
      "75406.57342098436 2.030917620904737 8\n",
      "The training loss is 5.438481621978321 with std:10.62516167982575. The val loss is 8.124246592768305 with std:14.526490443139522.\n",
      "8.124246592768305 2.030917620904737 8\n",
      "The training loss is 5.021818534881067 with std:8.450905701695552. The val loss is 9.067153867574747 with std:23.366370948920178.\n",
      "9.067153867574747 2.030917620904737 8\n",
      "Evaluating for {'degree': 8, 'lmda': 2.6489692876105297} ...\n",
      "The training loss is 5.445621196022353 with std:12.953894062945425. The val loss is 72007.331181696 with std:794637.1063698478.\n",
      "72007.331181696 2.6489692876105297 8\n",
      "The training loss is 5.542106445485461 with std:10.864905354017782. The val loss is 8.094493623997957 with std:14.402397068176086.\n",
      "8.094493623997957 2.6489692876105297 8\n",
      "The training loss is 5.170598479495686 with std:8.714691581760832. The val loss is 9.01141431684285 with std:23.322132088614293.\n",
      "9.01141431684285 2.6489692876105297 8\n",
      "Evaluating for {'degree': 8, 'lmda': 3.4551072945922217} ...\n",
      "The training loss is 5.550639060963258 with std:13.116587634417149. The val loss is 65390.48287780764 with std:721778.633902037.\n",
      "65390.48287780764 3.4551072945922217 8\n",
      "The training loss is 5.6602084850447305 with std:11.158417408606027. The val loss is 8.06169624846441 with std:14.255730230923062.\n",
      "8.06169624846441 3.4551072945922217 8\n",
      "The training loss is 5.335578650569082 with std:9.00889508765467. The val loss is 8.963788107726748 with std:23.250709111092018.\n",
      "8.963788107726748 3.4551072945922217 8\n",
      "Evaluating for {'degree': 8, 'lmda': 4.506570337745478} ...\n",
      "The training loss is 5.685905194705342 with std:13.34507233243178. The val loss is 56482.34695497973 with std:623539.5116302094.\n",
      "56482.34695497973 4.506570337745478 8\n",
      "The training loss is 5.802857396050663 with std:11.523756756466414. The val loss is 8.030020008812489 with std:14.087496435579467.\n",
      "8.030020008812489 4.506570337745478 8\n",
      "The training loss is 5.525611018351009 with std:9.344279336178733. The val loss is 8.93749433740102 with std:23.182499908300766.\n",
      "8.93749433740102 4.506570337745478 8\n",
      "Evaluating for {'degree': 8, 'lmda': 5.878016072274912} ...\n",
      "The training loss is 5.86717175221874 with std:13.663204825136807. The val loss is 46442.88401186307 with std:512736.17626703135.\n",
      "46442.88401186307 5.878016072274912 8\n",
      "The training loss is 5.986871926071841 with std:11.986562521410978. The val loss is 8.009948978269813 with std:13.9056283556018.\n",
      "8.009948978269813 5.878016072274912 8\n",
      "The training loss is 5.756380489551 with std:9.739506949032943. The val loss is 8.953583232052633 with std:23.171349358458553.\n",
      "8.953583232052633 5.878016072274912 8\n",
      "Evaluating for {'degree': 8, 'lmda': 7.666822074546214} ...\n",
      "The training loss is 6.119016766122128 with std:14.101410276676976. The val loss is 36394.07679665398 with std:401776.0211679416.\n",
      "36394.07679665398 7.666822074546214 8\n",
      "The training loss is 6.2395222572527755 with std:12.582347276639322. The val loss is 8.021855854630079 with std:13.728560486782134.\n",
      "8.021855854630079 7.666822074546214 8\n",
      "The training loss is 6.054044819206493 with std:10.224247848337518. The val loss is 9.04392863208188 with std:23.28911846492842.\n",
      "9.04392863208188 7.666822074546214 8\n",
      "Evaluating for {'degree': 8, 'lmda': 10.0} ...\n",
      "The training loss is 6.480400230262932 with std:14.699821627922056. The val loss is 27216.30555345632 with std:300401.7086893113.\n",
      "27216.30555345632 10.0 8\n",
      "The training loss is 6.604370404363355 with std:13.359245589464104. The val loss is 8.101811389748017 with std:13.591432711210528.\n",
      "8.101811389748017 10.0 8\n",
      "The training loss is 6.460906951321327 with std:10.843146883445192. The val loss is 9.256879798189882 with std:23.622408230668718.\n",
      "9.256879798189882 10.0 8\n",
      "Evaluating for {'degree': 9, 'lmda': 0.01} ...\n",
      "The training loss is 4.222867445300734 with std:10.240278943965428. The val loss is 41566412.06289636 with std:463173219.3025653.\n",
      "41566412.06289636 0.01 9\n",
      "The training loss is 4.181062750299289 with std:8.314191612293445. The val loss is 11.629884203183984 with std:31.314988068994822.\n",
      "11.629884203183984 0.01 9\n",
      "The training loss is 3.6517465682044645 with std:6.286448522825049. The val loss is 14.712370729109104 with std:69.73686290234359.\n",
      "14.712370729109104 0.01 9\n",
      "Evaluating for {'degree': 9, 'lmda': 0.013043213867190054} ...\n",
      "The training loss is 4.272051626062625 with std:10.390036485716067. The val loss is 35206415.93582717 with std:392298186.98308486.\n",
      "35206415.93582717 0.013043213867190054 9\n",
      "The training loss is 4.234317771562113 with std:8.447417190771073. The val loss is 11.204856146789133 with std:28.601392305697253.\n",
      "11.204856146789133 0.013043213867190054 9\n",
      "The training loss is 3.6859734714633468 with std:6.353617207687049. The val loss is 14.564451793380272 with std:68.07079128149817.\n",
      "14.564451793380272 0.013043213867190054 9\n",
      "Evaluating for {'degree': 9, 'lmda': 0.017012542798525893} ...\n",
      "The training loss is 4.322516267476479 with std:10.544702953725702. The val loss is 28487979.88383282 with std:317429318.2958842.\n",
      "28487979.88383282 0.017012542798525893 9\n",
      "The training loss is 4.286421399294389 with std:8.582543770308451. The val loss is 10.827829492391016 with std:26.273537198353868.\n",
      "10.827829492391016 0.017012542798525893 9\n",
      "The training loss is 3.720726122023496 with std:6.421620878253541. The val loss is 14.360234289035615 with std:65.91116359655369.\n",
      "14.360234289035615 0.017012542798525893 9\n",
      "Evaluating for {'degree': 9, 'lmda': 0.02218982341458972} ...\n",
      "The training loss is 4.37449941703412 with std:10.70502254302783. The val loss is 22018245.178553317 with std:245332772.12451738.\n",
      "22018245.178553317 0.02218982341458972 9\n",
      "The training loss is 4.337669592902594 with std:8.71875012780591. The val loss is 10.49490830456322 with std:24.298599609940588.\n",
      "10.49490830456322 0.02218982341458972 9\n",
      "The training loss is 3.7563899941058234 with std:6.490157510039547. The val loss is 14.104744123991317 with std:63.27736935926015.\n",
      "14.104744123991317 0.02218982341458972 9\n",
      "Evaluating for {'degree': 9, 'lmda': 0.028942661247167517} ...\n",
      "The training loss is 4.428042368493511 with std:10.87090905608277. The val loss is 16256646.664508179 with std:181128427.62976867.\n",
      "16256646.664508179 0.028942661247167517 9\n",
      "The training loss is 4.388222044850719 with std:8.854647540942748. The val loss is 10.201607443086658 with std:22.633950596022615.\n",
      "10.201607443086658 0.028942661247167517 9\n",
      "The training loss is 3.793192015311009 with std:6.558785845444494. The val loss is 13.805007595988872 with std:60.21951091139884.\n",
      "13.805007595988872 0.028942661247167517 9\n",
      "Evaluating for {'degree': 9, 'lmda': 0.037750532053243954} ...\n",
      "The training loss is 4.482966115733306 with std:11.04144972413015. The val loss is 11475078.094975213 with std:127846042.93706703.\n",
      "11475078.094975213 0.037750532053243954 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 4.438109236636538 with std:8.988317202576278. The val loss is 9.942990883740874 with std:21.235096993341802.\n",
      "9.942990883740874 0.037750532053243954 9\n",
      "The training loss is 3.8311992559466796 with std:6.627003549675465. The val loss is 13.469772887647945 with std:56.818926022565144.\n",
      "13.469772887647945 0.037750532053243954 9\n",
      "Evaluating for {'degree': 9, 'lmda': 0.04923882631706739} ...\n",
      "The training loss is 4.538900936078196 with std:11.214999489103649. The val loss is 7759036.21865681 with std:86438102.15048456.\n",
      "7759036.21865681 0.04923882631706739 9\n",
      "The training loss is 4.487301175853387 with std:9.117477836829828. The val loss is 9.713968744701157 with std:20.06069044719085.\n",
      "9.713968744701157 0.04923882631706739 9\n",
      "The training loss is 3.8703669622583865 with std:6.694352407931447. The val loss is 13.108882176131901 with std:53.18198876642945.\n",
      "13.108882176131901 0.04923882631706739 9\n",
      "Evaluating for {'degree': 9, 'lmda': 0.0642232542222936} ...\n",
      "The training loss is 4.595361932981139 with std:11.389327280423759. The val loss is 5044465.744708966 with std:56190496.75563991.\n",
      "5044465.744708966 0.0642232542222936 9\n",
      "The training loss is 4.535806015398739 with std:9.23975313361124. The val loss is 9.509717416692117 with std:19.075195884407993.\n",
      "9.509717416692117 0.0642232542222936 9\n",
      "The training loss is 3.9106376914066847 with std:6.76055049379467. The val loss is 12.732629256726689 with std:49.43090879360859.\n",
      "12.732629256726689 0.0642232542222936 9\n",
      "Evaluating for {'degree': 9, 'lmda': 0.0837677640068292} ...\n",
      "The training loss is 4.651849226787076 with std:11.56177925471258. The val loss is 3174137.035779712 with std:35350843.13261173.\n",
      "3174137.035779712 0.0837677640068292 9\n",
      "The training loss is 4.583764359607107 with std:9.352997809829022. The val loss is 9.326100953515585 with std:18.24966257937647.\n",
      "9.326100953515585 0.0837677640068292 9\n",
      "The training loss is 3.9520825524054577 with std:6.825653046015499. The val loss is 12.35136830222981 with std:45.69490945373897.\n",
      "12.35136830222981 0.0837677640068292 9\n",
      "Evaluating for {'degree': 9, 'lmda': 0.10926008611173785} ...\n",
      "The training loss is 4.707945404497958 with std:11.729443769106258. The val loss is 1954376.923817541 with std:21760698.270574383.\n",
      "1954376.923817541 0.10926008611173785 9\n",
      "The training loss is 4.6315193558823 with std:9.455650648411629. The val loss is 9.159968760470063 with std:17.561019553321344.\n",
      "9.159968760470063 0.10926008611173785 9\n",
      "The training loss is 3.995058233526714 with std:6.890234770469339. The val loss is 11.975397330216204 with std:42.102832087965304.\n",
      "11.975397330216204 0.10926008611173785 9\n",
      "Evaluating for {'degree': 9, 'lmda': 0.14251026703029984} ...\n",
      "The training loss is 4.7633858049304605 with std:11.889325259835777. The val loss is 1198355.9492341666 with std:13337937.84282802.\n",
      "1198355.9492341666 0.14251026703029984 9\n",
      "The training loss is 4.679655385868219 with std:9.54708850205001. The val loss is 9.009272330617014 with std:16.99058770713973.\n",
      "9.009272330617014 0.14251026703029984 9\n",
      "The training loss is 4.040336275922207 with std:6.955566941524592. The val loss is 11.614878967201978 with std:38.775666803066066.\n",
      "11.614878967201978 0.14251026703029984 9\n",
      "Evaluating for {'degree': 9, 'lmda': 0.18587918911465645} ...\n",
      "The training loss is 4.818086374685811 with std:12.038545170737985. The val loss is 750943.1935744272 with std:8353720.645839756.\n",
      "750943.1935744272 0.18587918911465645 9\n",
      "The training loss is 4.728998471352584 with std:9.627940393487131. The val loss is 8.873007583488565 with std:16.522577789700843.\n",
      "8.873007583488565 0.18587918911465645 9\n",
      "The training loss is 4.089156209598422 with std:7.0237441618522105. The val loss is 11.279461762741173 with std:35.81694326170098.\n",
      "11.279461762741173 0.18587918911465645 9\n",
      "Evaluating for {'degree': 9, 'lmda': 0.24244620170823283} ...\n",
      "The training loss is 4.872127891991377 with std:12.174586284050179. The val loss is 497172.6890088478 with std:5526832.0518640755.\n",
      "497172.6890088478 0.24244620170823283 9\n",
      "The training loss is 4.7805664274152315 with std:9.700293777272865. The val loss is 8.751004676332403 with std:16.142988397447574.\n",
      "8.751004676332403 0.24244620170823283 9\n",
      "The training loss is 4.143169213686296 with std:7.09770893359191. The val loss is 10.977419771136121 with std:33.301235355396464.\n",
      "10.977419771136121 0.24244620170823283 9\n",
      "Evaluating for {'degree': 9, 'lmda': 0.31622776601683794} ...\n",
      "The training loss is 4.9257131632393545 with std:12.295586717157864. The val loss is 359610.1221105684 with std:3994347.579336945.\n",
      "359610.1221105684 0.31622776601683794 9\n",
      "The training loss is 4.835462093874232 with std:9.767712955780205. The val loss is 8.643575777645351 with std:15.838846712532092.\n",
      "8.643575777645351 0.31622776601683794 9\n",
      "The training loss is 4.204270023492748 with std:7.1811303805286215. The val loss is 10.714451932627087 with std:31.26440932894573.\n",
      "10.714451932627087 0.31622776601683794 9\n",
      "Evaluating for {'degree': 9, 'lmda': 0.41246263829013524} ...\n",
      "The training loss is 4.979128706116936 with std:12.400672248734319. The val loss is 290132.3596926696 with std:3220041.000445402.\n",
      "290132.3596926696 0.41246263829013524 9\n",
      "The training loss is 4.894724705550895 with std:9.835018229400754. The val loss is 8.551039672293156 with std:15.597558356284361.\n",
      "8.551039672293156 0.41246263829013524 9\n",
      "The training loss is 4.274349842539111 with std:7.278116598878915. The val loss is 10.492553857624078 with std:29.700007302071953.\n",
      "10.492553857624078 0.41246263829013524 9\n",
      "Evaluating for {'degree': 9, 'lmda': 0.5379838403443686} ...\n",
      "The training loss is 5.032744215933121 with std:12.490293920552888. The val loss is 260392.66032505885 with std:2888140.934524106.\n",
      "260392.66032505885 0.5379838403443686 9\n",
      "The training loss is 4.959180989115314 with std:9.907848981274187. The val loss is 8.47318595937968 with std:15.406293087961634.\n",
      "8.47318595937968 0.5379838403443686 9\n",
      "The training loss is 4.3550290734434824 with std:7.39278049737405. The val loss is 10.309414030503635 with std:28.56282525026302.\n",
      "10.309414030503635 0.5379838403443686 9\n",
      "Evaluating for {'degree': 9, 'lmda': 0.701703828670383} ...\n",
      "The training loss is 5.087069536358397 with std:12.566526701221147. The val loss is 253631.06498960184 with std:2812062.1371732545.\n",
      "253631.06498960184 0.701703828670383 9\n",
      "The training loss is 5.029351162659457 with std:9.992131742994676. The val loss is 8.408796289869208 with std:15.251558256727481.\n",
      "8.408796289869208 0.701703828670383 9\n",
      "The training loss is 4.447437964647785 with std:7.5287350568759495. The val loss is 10.158614760343095 with std:27.7768362922662.\n",
      "10.158614760343095 0.701703828670383 9\n",
      "Evaluating for {'degree': 9, 'lmda': 0.9152473108773893} ...\n",
      "The training loss is 5.1428722328121905 with std:12.63330037444849. The val loss is 259006.96201506717 with std:2871224.057435338.\n",
      "259006.96201506717 0.9152473108773893 9\n",
      "The training loss is 5.105460804004379 with std:10.093651536021445. The val loss is 8.355369862710148 with std:15.11917245839166.\n",
      "8.355369862710148 0.9152473108773893 9\n",
      "The training loss is 4.552108236252688 with std:7.6886521874891445. The val loss is 10.030697542041366 with std:27.244710898192807.\n",
      "10.030697542041366 0.9152473108773893 9\n",
      "Evaluating for {'degree': 9, 'lmda': 1.1937766417144369} ...\n",
      "The training loss is 5.201355469408566 with std:12.696572732984686. The val loss is 268505.8504193612 with std:2976541.364612042.\n",
      "268505.8504193612 1.1937766417144369 9\n",
      "The training loss is 5.187599645844261 with std:10.217939378060747. The val loss is 8.30918594160304 with std:14.994750362317314.\n",
      "8.30918594160304 1.1937766417144369 9\n",
      "The training loss is 4.669032191005444 with std:7.87405879010465. The val loss is 9.914985309636572 with std:26.859412593802748.\n",
      "9.914985309636572 1.1937766417144369 9\n",
      "Evaluating for {'degree': 9, 'lmda': 1.5570684047537318} ...\n",
      "The training loss is 5.264409279989706 with std:12.7644919998716. The val loss is 275809.2032563282 with std:3057787.2239084984.\n",
      "275809.2032563282 1.5570684047537318 9\n",
      "The training loss is 5.276065017461475 with std:10.370627081049049. The val loss is 8.265778902706465 with std:14.864677442390464.\n",
      "8.265778902706465 1.5570684047537318 9\n",
      "The training loss is 4.797948658119923 with std:8.085546718742208. The val loss is 9.801910569868122 with std:26.519493185852067.\n",
      "9.801910569868122 1.5570684047537318 9\n",
      "Evaluating for {'degree': 9, 'lmda': 2.030917620904737} ...\n",
      "The training loss is 5.334969116906095 with std:12.84760388472568. The val loss is 276286.8461064181 with std:3063473.8930439283.\n",
      "276286.8461064181 2.030917620904737 9\n",
      "The training loss is 5.371928992016657 with std:10.558307726087374. The val loss is 8.220818598459472 with std:14.717466505155517.\n",
      "8.220818598459472 2.030917620904737 9\n",
      "The training loss is 4.938921009980831 with std:8.323529554185495. The val loss is 9.685449972788543 with std:26.146706530319854.\n",
      "9.685449972788543 2.030917620904737 9\n",
      "Evaluating for {'degree': 9, 'lmda': 2.6489692876105297} ...\n",
      "The training loss is 5.417539985629742 with std:12.959140921548547. The val loss is 267343.3561871748 with std:2964695.920136989.\n",
      "267343.3561871748 2.6489692876105297 9\n",
      "The training loss is 5.477870378927318 with std:10.789828864617723. The val loss is 8.171317659645226 with std:14.54533341718938.\n",
      "8.171317659645226 2.6489692876105297 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 5.093265655857452 with std:8.589590378302177. The val loss is 9.565201873514322 with std:25.700903642958817.\n",
      "9.565201873514322 2.6489692876105297 9\n",
      "Evaluating for {'degree': 9, 'lmda': 3.4551072945922217} ...\n",
      "The training loss is 5.518987856636455 with std:13.115426293574577. The val loss is 248583.6276934344 with std:2756979.720096986.\n",
      "248583.6276934344 3.4551072945922217 9\n",
      "The training loss is 5.599333417120826 with std:11.077889594573852. The val loss is 8.117085390367631 with std:14.345814737076987.\n",
      "8.117085390367631 3.4551072945922217 9\n",
      "The training loss is 5.264878504070749 with std:8.888364901360903. The val loss is 9.447799585940217 with std:25.186897575712813.\n",
      "9.447799585940217 3.4551072945922217 9\n",
      "Evaluating for {'degree': 9, 'lmda': 4.506570337745478} ...\n",
      "The training loss is 5.649787143728206 with std:13.336478407061168. The val loss is 221558.2774020017 with std:2457470.16523799.\n",
      "221558.2774020017 4.506570337745478 9\n",
      "The training loss is 5.746152371232023 with std:11.44084001020741. The val loss is 8.062467288579969 with std:14.12336413411021.\n",
      "8.062467288579969 4.506570337745478 9\n",
      "The training loss is 5.462041975803321 with std:9.229861493593269. The val loss is 9.347757322096557 with std:24.651327341873895.\n",
      "9.347757322096557 4.506570337745478 9\n",
      "Evaluating for {'degree': 9, 'lmda': 5.878016072274912} ...\n",
      "The training loss is 5.826057500813148 with std:13.647023675576314. The val loss is 189123.680613446 with std:2097833.749240679.\n",
      "189123.680613446 5.878016072274912 9\n",
      "The training loss is 5.934941605449299 with std:11.904672411440085. The val loss is 8.018654541197808 with std:13.8911881075265.\n",
      "8.018654541197808 5.878016072274912 9\n",
      "The training loss is 5.699933862030883 with std:9.632185241413033. The val loss is 9.288374675573092 with std:24.171871416900476.\n",
      "9.288374675573092 5.878016072274912 9\n",
      "Evaluating for {'degree': 9, 'lmda': 7.666822074546214} ...\n",
      "The training loss is 6.072919045802585 with std:14.078252842518435. The val loss is 154625.32552126722 with std:1715193.1476598766.\n",
      "154625.32552126722 7.666822074546214 9\n",
      "The training loss is 6.192783529226227 with std:12.505279694629536. The val loss is 8.007160829411042 with std:13.674053457450764.\n",
      "8.007160829411042 7.666822074546214 9\n",
      "The training loss is 6.0043135113399835 with std:10.124770623941703. The val loss is 9.303799322357037 with std:23.84396887303013.\n",
      "9.303799322357037 7.666822074546214 9\n",
      "Evaluating for {'degree': 9, 'lmda': 10.0} ...\n",
      "The training loss is 6.4299201764987455 with std:14.67067862469095. The val loss is 121149.40892273597 with std:1343813.4213334867.\n",
      "121149.40892273597 10.0 9\n",
      "The training loss is 6.563010217079742 with std:13.29105153668507. The val loss is 8.065355088610032 with std:13.513178981131816.\n",
      "8.065355088610032 10.0 9\n",
      "The training loss is 6.4171808057093775 with std:10.752295802739782. The val loss is 9.443647914555436 with std:23.77057450995154.\n",
      "9.443647914555436 10.0 9\n",
      "Evaluating for {'degree': 10, 'lmda': 0.01} ...\n",
      "The training loss is 4.155287064935343 with std:10.043667300989702. The val loss is 165487757.80640158 with std:1846247736.7156873.\n",
      "165487757.80640158 0.01 10\n",
      "The training loss is 4.08330756464546 with std:8.1297651124425. The val loss is 14.164371860305604 with std:54.3985431082515.\n",
      "14.164371860305604 0.01 10\n",
      "The training loss is 3.596715928053969 with std:6.212751737258113. The val loss is 15.929316074067337 with std:80.61816952463461.\n",
      "15.929316074067337 0.01 10\n",
      "Evaluating for {'degree': 10, 'lmda': 0.013043213867190054} ...\n",
      "The training loss is 4.201652618162301 with std:10.186414395126965. The val loss is 153176510.20132712 with std:1708898140.8574958.\n",
      "153176510.20132712 0.013043213867190054 10\n",
      "The training loss is 4.138366943829808 with std:8.265585843818153. The val loss is 13.367221853940373 with std:47.74829730502888.\n",
      "13.367221853940373 0.013043213867190054 10\n",
      "The training loss is 3.630898828277409 with std:6.282265747524938. The val loss is 16.044046680978894 with std:82.17034973641825.\n",
      "16.044046680978894 0.013043213867190054 10\n",
      "Evaluating for {'degree': 10, 'lmda': 0.017012542798525893} ...\n",
      "The training loss is 4.248817982651863 with std:10.33475111434455. The val loss is 134820521.3814584 with std:1504105193.4904687.\n",
      "134820521.3814584 0.017012542798525893 10\n",
      "The training loss is 4.192943178704105 with std:8.406517476549896. The val loss is 12.64939338854155 with std:41.77472323494852.\n",
      "12.64939338854155 0.017012542798525893 10\n",
      "The training loss is 3.6651099537439156 with std:6.35287108373045. The val loss is 16.060551012687 with std:82.7545656740104.\n",
      "16.060551012687 0.017012542798525893 10\n",
      "Evaluating for {'degree': 10, 'lmda': 0.02218982341458972} ...\n",
      "The training loss is 4.297347156331836 with std:10.490684891335452. The val loss is 112995488.097398 with std:1260607290.6623623.\n",
      "112995488.097398 0.02218982341458972 10\n",
      "The training loss is 4.247162026868308 with std:8.551489024037277. The val loss is 12.018111596661177 with std:36.610173393297146.\n",
      "12.018111596661177 0.02218982341458972 10\n",
      "The training loss is 3.699754368201038 with std:6.42409993230666. The val loss is 15.981968405340508 with std:82.34959166345185.\n",
      "15.981968405340508 0.02218982341458972 10\n",
      "Evaluating for {'degree': 10, 'lmda': 0.028942661247167517} ...\n",
      "The training loss is 4.347729417208874 with std:10.655245690421543. The val loss is 90251646.87742034 with std:1006858430.2267325.\n",
      "90251646.87742034 0.028942661247167517 10\n",
      "The training loss is 4.301005700338141 with std:8.698727330292762. The val loss is 11.474548630964922 with std:32.296011870544305.\n",
      "11.474548630964922 0.028942661247167517 10\n",
      "The training loss is 3.735155340887185 with std:6.495333367685156. The val loss is 15.81337076095338 with std:80.96551581698091.\n",
      "15.81337076095338 0.028942661247167517 10\n",
      "Evaluating for {'degree': 10, 'lmda': 0.037750532053243954} ...\n",
      "The training loss is 4.400252281296831 with std:10.828273044445012. The val loss is 68754833.05332094 with std:767023338.1230444.\n",
      "68754833.05332094 0.037750532053243954 10\n",
      "The training loss is 4.3543182880493205 with std:8.845841983619662. The val loss is 11.01362569439621 with std:28.792389648258727.\n",
      "11.01362569439621 0.037750532053243954 10\n",
      "The training loss is 3.7715367756725704 with std:6.565904610756664. The val loss is 15.561902562926266 with std:78.65120268713206.\n",
      "15.561902562926266 0.037750532053243954 10\n",
      "Evaluating for {'degree': 10, 'lmda': 0.04923882631706739} ...\n",
      "The training loss is 4.454917395005372 with std:11.008404044046497. The val loss is 50031412.774829194 with std:558131726.7516032.\n",
      "50031412.774829194 0.04923882631706739 10\n",
      "The training loss is 4.406877748293191 with std:8.990034012700933. The val loss is 10.62542299096538 with std:26.002022349393474.\n",
      "10.62542299096538 0.04923882631706739 10\n",
      "The training loss is 3.8090402592178267 with std:6.635226765952268. The val loss is 15.236753325426912 with std:75.4962106934318.\n",
      "15.236753325426912 0.04923882631706739 10\n",
      "Evaluating for {'degree': 10, 'lmda': 0.0642232542222936} ...\n",
      "The training loss is 4.511428673650027 with std:11.193245039462775. The val loss is 34868202.41085068 with std:388961860.80520195.\n",
      "34868202.41085068 0.0642232542222936 10\n",
      "The training loss is 4.458506945477238 with std:9.128392160125898. The val loss is 10.29740412585204 with std:23.799393452243855.\n",
      "10.29740412585204 0.0642232542222936 10\n",
      "The training loss is 3.8477747565735068 with std:6.702923181176186. The val loss is 14.849026044466333 with std:71.62809028862439.\n",
      "14.849026044466333 0.0642232542222936 10\n",
      "Evaluating for {'degree': 10, 'lmda': 0.0837677640068292} ...\n",
      "The training loss is 4.569258528669631 with std:11.379663067768018. The val loss is 23378360.84268897 with std:260775892.4245309.\n",
      "23378360.84268897 0.0837677640068292 10\n",
      "The training loss is 4.509185061102941 with std:9.258222956931103. The val loss is 10.0167141665941 with std:22.057315499320147.\n",
      "10.0167141665941 0.0837677640068292 10\n",
      "The training loss is 3.887897844821563 with std:6.768950225172073. The val loss is 14.411661493619308 with std:67.20760388474615.\n",
      "14.411661493619308 0.0837677640068292 10\n",
      "Evaluating for {'degree': 10, 'lmda': 0.10926008611173785} ...\n",
      "The training loss is 4.627770726452181 with std:11.564113600391465. The val loss is 15188250.538587015 with std:169404790.34941337.\n",
      "15188250.538587015 0.10926008611173785 10\n",
      "The training loss is 4.559128888981103 with std:9.377371087176188. The val loss is 9.772018004913834 with std:20.665299461080437.\n",
      "9.772018004913834 0.10926008611173785 10\n",
      "The training loss is 3.9297248382334917 with std:6.833717522073651. The val loss is 13.93948993033415 with std:62.423580624833804.\n",
      "13.93948993033415 0.10926008611173785 10\n",
      "Evaluating for {'degree': 10, 'lmda': 0.14251026703029984} ...\n",
      "The training loss is 4.686361428400096 with std:11.742940850130184. The val loss is 9666771.970407536 with std:107807074.71799895.\n",
      "9666771.970407536 0.14251026703029984 10\n",
      "The training loss is 4.608832126570786 with std:9.484513633591057. The val loss is 9.554602263908794 with std:19.537901544414265.\n",
      "9.554602263908794 0.14251026703029984 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 3.973849407636224 with std:6.898214005805149. The val loss is 13.449259703809457 with std:57.486612194949124.\n",
      "13.449259703809457 0.14251026703029984 10\n",
      "Evaluating for {'degree': 10, 'lmda': 0.18587918911465645} ...\n",
      "The training loss is 4.744578388988059 with std:11.912631510442054. The val loss is 6125301.937999427 with std:68299533.20115776.\n",
      "6125301.937999427 0.18587918911465645 10\n",
      "The training loss is 4.659064263965804 with std:9.579429581373386. The val loss is 9.358709391498916 with std:18.614642321665457.\n",
      "9.358709391498916 0.18587918911465645 10\n",
      "The training loss is 4.021240820244318 with std:6.964132956744074. The val loss is 12.95928279536222 with std:52.618489972890984.\n",
      "12.95928279536222 0.18587918911465645 10\n",
      "Evaluating for {'degree': 10, 'lmda': 0.24244620170823283} ...\n",
      "The training loss is 4.802190540119507 with std:12.070045342741482. The val loss is 3949676.6302920682 with std:44029750.147323556.\n",
      "3949676.6302920682 0.24244620170823283 10\n",
      "The training loss is 4.710831440209494 with std:9.663235084588917. The val loss is 9.181254085554084 with std:17.854902826312703.\n",
      "9.181254085554084 0.24244620170823283 10\n",
      "The training loss is 4.07327430483692 with std:7.033964581005057. The val loss is 12.488326285441355 with std:48.03439564935827.\n",
      "12.488326285441355 0.24244620170823283 10\n",
      "Evaluating for {'degree': 10, 'lmda': 0.31622776601683794} ...\n",
      "The training loss is 4.859202164803335 with std:12.212665486980468. The val loss is 2660666.4517872822 with std:29650834.977677114.\n",
      "2660666.4517872822 0.31622776601683794 10\n",
      "The training loss is 4.765296676199358 with std:9.73854085262956. The val loss is 9.021150539760937 with std:17.231180304092593.\n",
      "9.021150539760937 0.31622776601683794 10\n",
      "The training loss is 4.131664619964329 with std:7.111006529729138. The val loss is 12.053671768650005 with std:43.91852113404101.\n",
      "12.053671768650005 0.31622776601683794 10\n",
      "Evaluating for {'degree': 10, 'lmda': 0.41246263829013524} ...\n",
      "The training loss is 4.91583002054754 with std:12.33890055874562. The val loss is 1920588.5119473953 with std:21395229.799289696.\n",
      "1920588.5119473953 0.41246263829013524 10\n",
      "The training loss is 4.823660010288367 with std:9.809459519722228. The val loss is 8.878461218577161 with std:16.722976046263543.\n",
      "8.878461218577161 0.41246263829013524 10\n",
      "The training loss is 4.19830496701451 with std:7.199241890986631. The val loss is 11.668746901302242 with std:40.39929166445971.\n",
      "11.668746901302242 0.41246263829013524 10\n",
      "Evaluating for {'degree': 10, 'lmda': 0.5379838403443686} ...\n",
      "The training loss is 4.972477535713042 with std:12.448436521734182. The val loss is 1509665.1737025392 with std:16810978.67374311.\n",
      "1509665.1737025392 0.5379838403443686 10\n",
      "The training loss is 4.887016076298936 with std:9.88140231478893. The val loss is 8.753518075418386 with std:16.31233906482924.\n",
      "8.753518075418386 0.5379838403443686 10\n",
      "The training loss is 4.275046619365536 with std:7.303054968308877. The val loss is 11.341083798868196 with std:37.53323382500305.\n",
      "11.341083798868196 0.5379838403443686 10\n",
      "Evaluating for {'degree': 10, 'lmda': 0.701703828670383} ...\n",
      "The training loss is 5.029740848310499 with std:12.542599699958817. The val loss is 1292186.901893865 with std:14384222.820281807.\n",
      "1292186.901893865 0.701703828670383 10\n",
      "The training loss is 4.956229327452528 with std:9.960672997865071. The val loss is 8.646124811923343 with std:15.981250936398768.\n",
      "8.646124811923343 0.701703828670383 10\n",
      "The training loss is 4.36347300136305 with std:7.426799511978755. The val loss is 11.071313620988645 with std:35.30338551832618.\n",
      "11.071313620988645 0.701703828670383 10\n",
      "Evaluating for {'degree': 10, 'lmda': 0.9152473108773893} ...\n",
      "The training loss is 5.088471349112055 with std:12.62467921379151. The val loss is 1185391.8872280656 with std:13191967.809368346.\n",
      "1185391.8872280656 0.9152473108773893 10\n",
      "The training loss is 5.031882878843345 with std:10.053977095402884. The val loss is 8.554948055430103 with std:15.710638385402769.\n",
      "8.554948055430103 0.9152473108773893 10\n",
      "The training loss is 4.464725252312108 with std:7.574300635806963. The val loss is 10.85352726363239 with std:33.63120838500434.\n",
      "10.85352726363239 0.9152473108773893 10\n",
      "Evaluating for {'degree': 10, 'lmda': 1.1937766417144369} ...\n",
      "The training loss is 5.149911589174518 with std:12.700180042063321. The val loss is 1136917.0646861023 with std:12650442.057415226.\n",
      "1136917.0646861023 1.1937766417144369 10\n",
      "The training loss is 5.1143622230148695 with std:10.168065665961054. The val loss is 8.477227346124959 with std:15.480643180081133.\n",
      "8.477227346124959 1.1937766417144369 10\n",
      "The training loss is 4.579440920373339 with std:7.748449214807549. The val loss is 10.6769320082361 with std:32.39655596599491.\n",
      "10.6769320082361 1.1937766417144369 10\n",
      "Evaluating for {'degree': 10, 'lmda': 1.5570684047537318} ...\n",
      "The training loss is 5.215925632749192 with std:12.777022490377208. The val loss is 1111486.3224102468 with std:12366491.61182951.\n",
      "1111486.3224102468 1.5570684047537318 10\n",
      "The training loss is 5.204134631705298 with std:10.30976232488237. The val loss is 8.40893649378045 with std:15.271739377132471.\n",
      "8.40893649378045 1.5570684047537318 10\n",
      "The training loss is 4.7078875512446485 with std:7.951109284743767. The val loss is 10.528518379950508 with std:31.461158467869193.\n",
      "10.528518379950508 1.5570684047537318 10\n",
      "Evaluating for {'degree': 10, 'lmda': 2.030917620904737} ...\n",
      "The training loss is 5.289359799598236 with std:12.8657395298787. The val loss is 1084686.3507565544 with std:12068053.904689385.\n",
      "1084686.3507565544 2.030917620904737 10\n",
      "The training loss is 5.3022813714747725 with std:10.486547356330565. The val loss is 8.345478261959673 with std:15.066342058617598.\n",
      "8.345478261959673 2.030917620904737 10\n",
      "The training loss is 4.850398118287427 with std:8.18356607597957. The val loss is 10.396341077231838 with std:30.693318252128456.\n",
      "10.396341077231838 2.030917620904737 10\n",
      "Evaluating for {'degree': 10, 'lmda': 2.6489692876105297} ...\n",
      "The training loss is 5.374591491893005 with std:12.979732478930574. The val loss is 1040851.675030416 with std:11580503.311593963.\n",
      "1040851.675030416 2.6489692876105297 10\n",
      "The training loss is 5.411334279489123 with std:10.707724829844146. The val loss is 8.282898338151675 with std:14.850655843433291.\n",
      "8.282898338151675 2.6489692876105297 10\n",
      "The training loss is 5.008216824327447 with std:8.447667544940002. The val loss is 10.272880386730227 with std:29.990822709779998.\n",
      "10.272880386730227 2.6489692876105297 10\n",
      "Evaluating for {'degree': 10, 'lmda': 3.4551072945922217} ...\n",
      "The training loss is 5.478362235044876 with std:13.135644848353142. The val loss is 972457.6651489947 with std:10819861.529036716.\n",
      "972457.6651489947 3.4551072945922217 10\n",
      "The training loss is 5.536477655692482 with std:10.98606166730083. The val loss is 8.219529076824365 with std:14.61660061822274.\n",
      "8.219529076824365 3.4551072945922217 10\n",
      "The training loss is 5.18483749396677 with std:8.747677628944231. The val loss is 10.157883729833474 with std:29.296992820801908.\n",
      "10.157883729833474 3.4551072945922217 10\n",
      "Evaluating for {'degree': 10, 'lmda': 4.506570337745478} ...\n",
      "The training loss is 5.611070351824645 with std:13.353946999602986. The val loss is 879327.9403249181 with std:9783979.698955562.\n",
      "879327.9403249181 4.506570337745478 10\n",
      "The training loss is 5.687238495750373 with std:11.339740966903197. The val loss is 8.158013571003737 with std:14.363756009837715.\n",
      "8.158013571003737 4.506570337745478 10\n",
      "The training loss is 5.387914237880035 with std:9.092747002962025. The val loss is 10.06033715106873 with std:28.60546927111398.\n",
      "10.06033715106873 4.506570337745478 10\n",
      "Evaluating for {'degree': 10, 'lmda': 5.878016072274912} ...\n",
      "The training loss is 5.78883892226819 with std:13.659918640595752. The val loss is 766976.1331274739 with std:8534094.473926842.\n",
      "766976.1331274739 5.878016072274912 10\n",
      "The training loss is 5.879937189042927 with std:11.794531605661547. The val loss is 8.10787214875766 with std:14.101518774312312.\n",
      "8.10787214875766 5.878016072274912 10\n",
      "The training loss is 5.6319234333082955 with std:9.499902890844902. The val loss is 9.999848140689297 with std:27.953619512894953.\n",
      "9.999848140689297 5.878016072274912 10\n",
      "Evaluating for {'degree': 10, 'lmda': 7.666822074546214} ...\n",
      "The training loss is 6.036874654942354 with std:14.085339826424615. The val loss is 644229.606454154 with std:7168385.169659648.\n",
      "644229.606454154 7.666822074546214 10\n",
      "The training loss is 6.141406878276825 with std:12.386179037978533. The val loss is 8.089121241754015 with std:13.852128361974874.\n",
      "8.089121241754015 7.666822074546214 10\n",
      "The training loss is 5.941993509146925 with std:9.997560993073067. The val loss is 10.008489173338901 with std:27.40933545919273.\n",
      "10.008489173338901 7.666822074546214 10\n",
      "Evaluating for {'degree': 10, 'lmda': 10.0} ...\n",
      "The training loss is 6.394868749122544 with std:14.671251286969378. The val loss is 520720.5076313126 with std:5794042.489020432.\n",
      "520720.5076313126 10.0 10\n",
      "The training loss is 6.5147717679231185 with std:13.163062638476832. The val loss is 8.137826726263006 with std:13.655687221813128.\n",
      "8.137826726263006 10.0 10\n",
      "The training loss is 6.3596528651237625 with std:10.629636330977808. The val loss is 10.13472045392533 with std:27.05840784064152.\n",
      "10.13472045392533 10.0 10\n",
      "Evaluating for {'degree': 11, 'lmda': 0.01} ...\n",
      "The training loss is 4.107580032100943 with std:9.981541306187786. The val loss is 514093507.6689721 with std:5739879242.710038.\n",
      "514093507.6689721 0.01 11\n",
      "The training loss is 4.049657099774255 with std:8.113785703759984. The val loss is 13.439587471891507 with std:51.72779811746422.\n",
      "13.439587471891507 0.01 11\n",
      "The training loss is 3.561740945295461 with std:6.163947172338833. The val loss is 15.305063092909133 with std:77.99941708293802.\n",
      "15.305063092909133 0.01 11\n",
      "Evaluating for {'degree': 11, 'lmda': 0.013043213867190054} ...\n",
      "The training loss is 4.153660783424772 with std:10.126210559090529. The val loss is 529644113.1337412 with std:5913554052.111032.\n",
      "529644113.1337412 0.013043213867190054 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 4.1047832499701435 with std:8.246789559667404. The val loss is 12.744011626052586 with std:45.86257664878375.\n",
      "12.744011626052586 0.013043213867190054 11\n",
      "The training loss is 3.5964448144152548 with std:6.236100290175237. The val loss is 16.008348617731876 with std:85.22097219946333.\n",
      "16.008348617731876 0.013043213867190054 11\n",
      "Evaluating for {'degree': 11, 'lmda': 0.017012542798525893} ...\n",
      "The training loss is 4.200592501748394 with std:10.274654626214554. The val loss is 511031041.36581564 with std:5705763968.613176.\n",
      "511031041.36581564 0.017012542798525893 11\n",
      "The training loss is 4.159512755122866 with std:8.385618438195829. The val loss is 12.11629926447872 with std:40.456404435422705.\n",
      "12.11629926447872 0.017012542798525893 11\n",
      "The training loss is 3.6308473538996497 with std:6.30966075940972. The val loss is 16.57985771944045 with std:91.22475368563764.\n",
      "16.57985771944045 0.017012542798525893 11\n",
      "Evaluating for {'degree': 11, 'lmda': 0.02218982341458972} ...\n",
      "The training loss is 4.248877922237609 with std:10.429272632642721. The val loss is 464684100.9727516 with std:5188300684.082562.\n",
      "464684100.9727516 0.02218982341458972 11\n",
      "The training loss is 4.213901869046252 with std:8.529362747353403. The val loss is 11.565023416470325 with std:35.6916995435636.\n",
      "11.565023416470325 0.02218982341458972 11\n",
      "The training loss is 3.665295664357909 with std:6.384147339757998. The val loss is 17.00678224303904 with std:95.80270349602715.\n",
      "17.00678224303904 0.02218982341458972 11\n",
      "Evaluating for {'degree': 11, 'lmda': 0.028942661247167517} ...\n",
      "The training loss is 4.299043939023561 with std:10.591769691264265. The val loss is 399722072.3914343 with std:4462981933.804748.\n",
      "399722072.3914343 0.028942661247167517 11\n",
      "The training loss is 4.267968950054812 with std:8.676475123432962. The val loss is 11.093587090856774 with std:31.65869219786764.\n",
      "11.093587090856774 0.028942661247167517 11\n",
      "The training loss is 3.700112907887739 with std:6.458858609974568. The val loss is 17.282132261928208 with std:98.81157894496164.\n",
      "17.282132261928208 0.028942661247167517 11\n",
      "Evaluating for {'degree': 11, 'lmda': 0.037750532053243954} ...\n",
      "The training loss is 4.351519295916858 with std:10.76278885596458. The val loss is 326085787.4762252 with std:3640804807.501348.\n",
      "326085787.4762252 0.037750532053243954 11\n",
      "The training loss is 4.3216530805775095 with std:8.824780051518255. The val loss is 10.699403328780782 with std:28.361210649595893.\n",
      "10.699403328780782 0.037750532053243954 11\n",
      "The training loss is 3.7355676836583047 with std:6.532975904096592. The val loss is 17.40365511143975 with std:100.17082220394452.\n",
      "17.40365511143975 0.037750532053243954 11\n",
      "Evaluating for {'degree': 11, 'lmda': 0.04923882631706739} ...\n",
      "The training loss is 4.4065142139367905 with std:10.941710847929967. The val loss is 252821246.49292254 with std:2822774907.577625.\n",
      "252821246.49292254 0.04923882631706739 11\n",
      "The training loss is 4.3748210671503704 with std:8.97159536041712. The val loss is 10.374597416874794 with std:25.73640932944222.\n",
      "10.374597416874794 0.04923882631706739 11\n",
      "The training loss is 3.771881452605116 with std:6.605714797207256. The val loss is 17.373489219123304 with std:99.86461652442007.\n",
      "17.373489219123304 0.04923882631706739 11\n",
      "Evaluating for {'degree': 11, 'lmda': 0.0642232542222936} ...\n",
      "The training loss is 4.4639373436238134 with std:11.126659070250252. The val loss is 186780125.12224904 with std:2085396464.804671.\n",
      "186780125.12224904 0.0642232542222936 11\n",
      "The training loss is 4.427325508665763 with std:9.113971207096334. The val loss is 10.1076802430215 with std:23.6807493214491.\n",
      "10.1076802430215 0.0642232542222936 11\n",
      "The training loss is 3.8092680455882952 with std:6.676492243679965. The val loss is 17.19848969664166 with std:97.94709847834736.\n",
      "17.19848969664166 0.0642232542222936 11\n",
      "Evaluating for {'degree': 11, 'lmda': 0.0837677640068292} ...\n",
      "The training loss is 4.523384779492228 with std:11.31470615195751. The val loss is 131981072.75002956 with std:1473540896.1441858.\n",
      "131981072.75002956 0.0837677640068292 11\n",
      "The training loss is 4.479095172135532 with std:9.249013864289147. The val loss is 9.885637927622867 with std:22.07576064763753.\n",
      "9.885637927622867 0.0837677640068292 11\n",
      "The training loss is 3.8479953467863317 with std:6.745078711284577. The val loss is 16.89094503837499 with std:94.54870805068505.\n",
      "16.89094503837499 0.0837677640068292 11\n",
      "Evaluating for {'degree': 11, 'lmda': 0.10926008611173785} ...\n",
      "The training loss is 4.584214704577865 with std:11.50222930600137. The val loss is 89702560.45051824 with std:1001483621.4210771.\n",
      "89702560.45051824 0.10926008611173785 11\n",
      "The training loss is 4.530227378873129 with std:9.374237382349648. The val loss is 9.695971618585125 with std:20.808978236111066.\n",
      "9.695971618585125 0.10926008611173785 11\n",
      "The training loss is 3.8884629819729937 with std:6.811722876133241. The val loss is 16.469275729146975 with std:89.88060284073472.\n",
      "16.469275729146975 0.10926008611173785 11\n",
      "Evaluating for {'degree': 11, 'lmda': 0.14251026703029984} ...\n",
      "The training loss is 4.6456891373975 with std:11.685326972696405. The val loss is 59136366.33450791 with std:660200670.9187665.\n",
      "59136366.33450791 0.14251026703029984 11\n",
      "The training loss is 4.5810552865114325 with std:9.487891694864603. The val loss is 9.528317224600313 with std:19.787471137428444.\n",
      "9.528317224600313 0.14251026703029984 11\n",
      "The training loss is 3.9312906036263975 with std:6.877255634780207. The val loss is 15.958240062253736 with std:84.2330307935468.\n",
      "15.958240062253736 0.14251026703029984 11\n",
      "Evaluating for {'degree': 11, 'lmda': 0.18587918911465645} ...\n",
      "The training loss is 4.707138114390037 with std:11.860210838721875. The val loss is 38277187.09168924 with std:427301721.777208.\n",
      "38277187.09168924 0.18587918911465645 11\n",
      "The training loss is 4.632174865492837 with std:9.589242938377783. The val loss is 9.375387342291308 with std:18.943415298499453.\n",
      "9.375387342291308 0.18587918911465645 11\n",
      "The training loss is 3.9774013514656943 with std:6.943184628377328. The val loss is 15.388176325273308 with std:77.96306725016304.\n",
      "15.388176325273308 0.18587918911465645 11\n",
      "Evaluating for {'degree': 11, 'lmda': 0.24244620170823283} ...\n",
      "The training loss is 4.768094816356387 with std:12.023527590942402. The val loss is 24737208.990919515 with std:276125267.63011515.\n",
      "24737208.990919515 0.24244620170823283 11\n",
      "The training loss is 4.684427523395453 with std:9.67880372985279. The val loss is 9.233139505545964 with std:18.232899199701254.\n",
      "9.233139505545964 0.24244620170823283 11\n",
      "The training loss is 4.02807048084528 with std:7.011776189496404. The val loss is 14.792925831458236 with std:71.46795353185883.\n",
      "14.792925831458236 0.24244620170823283 11\n",
      "Evaluating for {'degree': 11, 'lmda': 0.31622776601683794} ...\n",
      "The training loss is 4.828367031058384 with std:12.17262031795009. The val loss is 16306071.905679656 with std:181991015.67552042.\n",
      "16306071.905679656 0.31622776601683794 11\n",
      "The training loss is 4.738840780852703 with std:9.758505823898473. The val loss is 9.100265880739071 with std:17.63030140040142.\n",
      "9.100265880739071 0.31622776601683794 11\n",
      "The training loss is 4.084905883858603 with std:7.086097578061795. The val loss is 14.206391392854837 with std:65.14353768491452.\n",
      "14.206391392854837 0.31622776601683794 11\n",
      "Evaluating for {'degree': 11, 'lmda': 0.41246263829013524} ...\n",
      "The training loss is 4.8880435513074785 with std:12.305776082059525. The val loss is 11224606.993440244 with std:125256472.99154927.\n",
      "11224606.993440244 0.41246263829013524 11\n",
      "The training loss is 4.796534507165407 with std:9.831779253916949. The val loss is 8.97724540011576 with std:17.12094112066816.\n",
      "8.97724540011576 0.41246263829013524 11\n",
      "The training loss is 4.1497445691987345 with std:7.169972671291948. The val loss is 13.658222617822613 with std:59.333445666149565.\n",
      "13.658222617822613 0.41246263829013524 11\n",
      "Evaluating for {'degree': 11, 'lmda': 0.5379838403443686} ...\n",
      "The training loss is 4.947466219883414 with std:12.422502675554947. The val loss is 8236380.31360728 with std:91892638.25475706.\n",
      "8236380.31360728 0.5379838403443686 11\n",
      "The training loss is 4.858608701371182 with std:9.90348162570509. The val loss is 8.86524192803345 with std:16.69428426450869.\n",
      "8.86524192803345 0.5379838403443686 11\n",
      "The training loss is 4.224479362922796 with std:7.267801766575978. The val loss is 13.169684974751776 with std:54.28127077551786.\n",
      "13.169684974751776 0.5379838403443686 11\n",
      "Evaluating for {'degree': 11, 'lmda': 0.701703828670383} ...\n",
      "The training loss is 5.007212699746684 with std:12.523841889969002. The val loss is 6512423.966355003 with std:72643719.98852646.\n",
      "6512423.966355003 0.701703828670383 11\n",
      "The training loss is 4.926040617210118 with std:9.979644601888406. The val loss is 8.76508410499471 with std:16.339096507801717.\n",
      "8.76508410499471 0.701703828670383 11\n",
      "The training loss is 4.3108555157875585 with std:7.384221246072949. The val loss is 12.750964816851237 with std:50.100412325559546.\n",
      "12.750964816851237 0.701703828670383 11\n",
      "Evaluating for {'degree': 11, 'lmda': 0.9152473108773893} ...\n",
      "The training loss is 5.0681292768534485 with std:12.612690825713367. The val loss is 5532172.824596001 with std:61697765.79499446.\n",
      "5532172.824596001 0.9152473108773893 11\n",
      "The training loss is 4.999630979173704 with std:10.067084920864279. The val loss is 8.67648886870044 with std:16.041012005733037.\n",
      "8.67648886870044 0.9152473108773893 11\n",
      "The training loss is 4.410288604067757 with std:7.523637908214518. The val loss is 12.400761578955352 with std:46.7712137006132.\n",
      "12.400761578955352 0.9152473108773893 11\n",
      "Evaluating for {'degree': 11, 'lmda': 1.1937766417144369} ...\n",
      "The training loss is 5.131440972699738 with std:12.694097605519694. The val loss is 4973158.152113988 with std:55454948.43850517.\n",
      "4973158.152113988 1.1937766417144369 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 5.080051199281327 with std:10.173038181714233. The val loss is 8.597646869381652 with std:15.78231969665774.\n",
      "8.597646869381652 1.1937766417144369 11\n",
      "The training loss is 4.52376380591867 with std:7.689761437361881. The val loss is 12.108270456727046 with std:44.16543329073526.\n",
      "12.108270456727046 1.1937766417144369 11\n",
      "Evaluating for {'degree': 11, 'lmda': 1.5570684047537318} ...\n",
      "The training loss is 5.198964468863792 with std:12.775526834856404. The val loss is 4631696.9065977195 with std:51641846.940930635.\n",
      "4631696.9065977195 1.5570684047537318 11\n",
      "The training loss is 5.168058257343004 with std:10.305055451653184. The val loss is 8.525278008846438 with std:15.543422462420432.\n",
      "8.525278008846438 1.5570684047537318 11\n",
      "The training loss is 4.651900136726487 with std:7.885352304295174. The val loss is 11.857046183484567 with std:42.089895537720906.\n",
      "11.857046183484567 1.5570684047537318 11\n",
      "Evaluating for {'degree': 11, 'lmda': 2.030917620904737} ...\n",
      "The training loss is 5.273462388716179 with std:12.867128049577502. The val loss is 4375798.238313187 with std:48785422.032777324.\n",
      "4375798.238313187 2.030917620904737 11\n",
      "The training loss is 5.264957002865939 with std:10.471395630189214. The val loss is 8.455248267812845 with std:15.305335621534818.\n",
      "8.455248267812845 2.030917620904737 11\n",
      "The training loss is 4.795298473898439 with std:8.112454703881662. The val loss is 11.629956978440491 with std:40.33769837398188.\n",
      "11.629956978440491 2.030917620904737 11\n",
      "Evaluating for {'degree': 11, 'lmda': 2.6489692876105297} ...\n",
      "The training loss is 5.359200884969405 with std:12.98206006982665. The val loss is 4121464.3939652867 with std:45948238.273309514.\n",
      "4121464.3939652867 2.6489692876105297 11\n",
      "The training loss is 5.373392326121524 with std:10.682036499565845. The val loss is 8.383778478856351 with std:15.052658215949549.\n",
      "8.383778478856351 2.6489692876105297 11\n",
      "The training loss is 4.955308775918332 with std:8.373346785901946. The val loss is 11.414317782531684 with std:38.73521795274391.\n",
      "11.414317782531684 2.6489692876105297 11\n",
      "Evaluating for {'degree': 11, 'lmda': 3.4551072945922217} ...\n",
      "The training loss is 5.4628102405565615 with std:13.136927780223443. The val loss is 3821675.455783415 with std:42605388.43924237.\n",
      "3821675.455783415 3.4551072945922217 11\n",
      "The training loss is 5.498553968726877 with std:10.950269239064454. The val loss is 8.309194629165496 with std:14.776579230615573.\n",
      "8.309194629165496 3.4551072945922217 11\n",
      "The training loss is 5.135331135911529 with std:8.6723092813758. The val loss is 11.206237028168104 with std:37.17452455665931.\n",
      "11.206237028168104 3.4551072945922217 11\n",
      "Evaluating for {'degree': 11, 'lmda': 4.506570337745478} ...\n",
      "The training loss is 5.594620840601194 with std:13.352411407405036. The val loss is 3459715.690397353 with std:38569990.38039723.\n",
      "3459715.690397353 4.506570337745478 11\n",
      "The training loss is 5.649913650951078 with std:11.29472670611231. The val loss is 8.234155810825177 with std:14.477650380741602.\n",
      "8.234155810825177 4.506570337745478 11\n",
      "The training loss is 5.342747129333188 with std:9.0181591193624. The val loss is 11.013450548718938 with std:35.62447091374179.\n",
      "11.013450548718938 4.506570337745478 11\n",
      "Evaluating for {'degree': 11, 'lmda': 5.878016072274912} ...\n",
      "The training loss is 5.770777292028807 with std:13.654248153231059. The val loss is 3042238.5198191414 with std:33915846.50356988.\n",
      "3042238.5198191414 5.878016072274912 11\n",
      "The training loss is 5.843736672770799 with std:11.741684597534737. The val loss is 8.168441249210682 with std:14.168345619243016.\n",
      "8.168441249210682 5.878016072274912 11\n",
      "The training loss is 5.59164821606926 with std:9.427421640912335. The val loss is 10.856671303024807 with std:34.12044265048973.\n",
      "10.856671303024807 5.878016072274912 11\n",
      "Evaluating for {'degree': 11, 'lmda': 7.666822074546214} ...\n",
      "The training loss is 6.016626598951371 with std:14.074847875950974. The val loss is 2590908.2767312583 with std:28884255.053044613.\n",
      "2590908.2767312583 7.666822074546214 11\n",
      "The training loss is 6.106836453368365 with std:12.327544515536673. The val loss is 8.132696449340068 with std:13.875905564423386.\n",
      "8.132696449340068 7.666822074546214 11\n",
      "The training loss is 5.906745009677811 with std:9.928060690470643. The val loss is 10.770555204753265 with std:32.74028759806859.\n",
      "10.770555204753265 7.666822074546214 11\n",
      "Evaluating for {'degree': 11, 'lmda': 10.0} ...\n",
      "The training loss is 6.372112670234473 with std:14.655897840691889. The val loss is 2133665.5197004513 with std:23786615.08366159.\n",
      "2133665.5197004513 10.0 11\n",
      "The training loss is 6.4823370975275845 with std:13.10146783738277. The val loss is 8.163944069354686 with std:13.646495564279922.\n",
      "8.163944069354686 10.0 11\n",
      "The training loss is 6.329177152272928 with std:10.563745424401478. The val loss is 10.806262565057725 with std:31.57842305534936.\n",
      "10.806262565057725 10.0 11\n",
      "Evaluating for {'degree': 12, 'lmda': 0.01} ...\n",
      "The training loss is 4.0752619402722345 with std:9.899877269072382. The val loss is 1167786596.1754885 with std:13044708222.242357.\n",
      "1167786596.1754885 0.01 12\n",
      "The training loss is 4.002528159860153 with std:8.018645307566059. The val loss is 16.145224313280007 with std:78.01617058979667.\n",
      "16.145224313280007 0.01 12\n",
      "The training loss is 3.5329749281952654 with std:6.11649550904213. The val loss is 13.053905347397754 with std:51.79938355738785.\n",
      "13.053905347397754 0.01 12\n",
      "Evaluating for {'degree': 12, 'lmda': 0.013043213867190054} ...\n",
      "The training loss is 4.1192613587642155 with std:10.04047928717459. The val loss is 1410888340.3357544 with std:15760516024.96133.\n",
      "1410888340.3357544 0.013043213867190054 12\n",
      "The training loss is 4.0549910219130565 with std:8.146625072205174. The val loss is 15.07511061446737 with std:68.60963297591364.\n",
      "15.07511061446737 0.013043213867190054 12\n",
      "The training loss is 3.567581609033122 with std:6.188970613265576. The val loss is 14.219211451373472 with std:63.79288459204413.\n",
      "14.219211451373472 0.013043213867190054 12\n",
      "Evaluating for {'degree': 12, 'lmda': 0.017012542798525893} ...\n",
      "The training loss is 4.163727299605519 with std:10.183283652182968. The val loss is 1539015439.881083 with std:17191945360.06897.\n",
      "1539015439.881083 0.017012542798525893 12\n",
      "The training loss is 4.107782026892308 with std:8.282562086533638. The val loss is 14.072822182970585 with std:59.61880304183672.\n",
      "14.072822182970585 0.017012542798525893 12\n",
      "The training loss is 3.6016755245325074 with std:6.2630407155277. The val loss is 15.34778207110582 with std:75.9402638572283.\n",
      "15.34778207110582 0.017012542798525893 12\n",
      "Evaluating for {'degree': 12, 'lmda': 0.02218982341458972} ...\n",
      "The training loss is 4.20911908416201 with std:10.330986653466043. The val loss is 1547366730.4989707 with std:17285340802.587196.\n",
      "1547366730.4989707 0.02218982341458972 12\n",
      "The training loss is 4.161015151935798 with std:8.425858197415142. The val loss is 13.165853630348943 with std:51.394969419264235.\n",
      "13.165853630348943 0.02218982341458972 12\n",
      "The training loss is 3.6355753226206207 with std:6.338390103491586. The val loss is 16.389654423877122 with std:87.48522144168764.\n",
      "16.389654423877122 0.02218982341458972 12\n",
      "Evaluating for {'degree': 12, 'lmda': 0.028942661247167517} ...\n",
      "The training loss is 4.256010984814267 with std:10.485940094699647. The val loss is 1451084694.592727 with std:16209850300.990196.\n",
      "1451084694.592727 0.028942661247167517 12\n",
      "The training loss is 4.214704325820163 with std:8.575086169368333. The val loss is 12.374218472446 with std:44.186311996398544.\n",
      "12.374218472446 0.028942661247167517 12\n",
      "The training loss is 3.6696007584486474 with std:6.414439764153957. The val loss is 17.302441511985297 with std:97.8010267620706.\n",
      "17.302441511985297 0.028942661247167517 12\n",
      "Evaluating for {'degree': 12, 'lmda': 0.037750532053243954} ...\n",
      "The training loss is 4.304994997931261 with std:10.649691219195844. The val loss is 1278391438.879208 with std:14280743018.80999.\n",
      "1278391438.879208 0.037750532053243954 12\n",
      "The training loss is 4.268724255047103 with std:8.72799964009124. The val loss is 11.706738524553902 with std:38.11565442808491.\n",
      "11.706738524553902 0.037750532053243954 12\n",
      "The training loss is 3.70403612438824 with std:6.490411689308674. The val loss is 18.051143040338435 with std:106.38289294781116.\n",
      "18.051143040338435 0.037750532053243954 12\n",
      "Evaluating for {'degree': 12, 'lmda': 0.04923882631706739} ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 4.3565558531247035 with std:10.822619860089757. The val loss is 1063138326.6216185 with std:11876177139.51597.\n",
      "1063138326.6216185 0.04923882631706739 12\n",
      "The training loss is 4.322825150637203 with std:8.881674283400555. The val loss is 11.160570692311433 with std:33.18597763321743.\n",
      "11.160570692311433 0.04923882631706739 12\n",
      "The training loss is 3.739130895739078 with std:6.565459207450366. The val loss is 18.607776696686088 with std:112.84135310389836.\n",
      "18.607776696686088 0.04923882631706739 12\n",
      "Evaluating for {'degree': 12, 'lmda': 0.0642232542222936} ...\n",
      "The training loss is 4.410943001581105 with std:11.003751278859601. The val loss is 837824662.1422096 with std:9359208586.81613.\n",
      "837824662.1422096 0.0642232542222936 12\n",
      "The training loss is 4.376702893601735 with std:9.032780959426153. The val loss is 10.723322333739702 with std:29.305309130494233.\n",
      "10.723322333739702 0.0642232542222936 12\n",
      "The training loss is 3.775135890589088 with std:6.63883904497432. The val loss is 18.951802808756653 with std:116.9028233642998.\n",
      "18.951802808756653 0.0642232542222936 12\n",
      "Evaluating for {'degree': 12, 'lmda': 0.0837677640068292} ...\n",
      "The training loss is 4.468081669810135 with std:11.190796626922094. The val loss is 628208452.8706115 with std:7017586132.092859.\n",
      "628208452.8706115 0.0837677640068292 12\n",
      "The training loss is 4.430105717172833 with std:9.177952907198916. The val loss is 10.376550138807385 with std:26.320506190068514.\n",
      "10.376550138807385 0.0837677640068292 12\n",
      "The training loss is 3.8123624560669467 with std:6.710088777014898. The val loss is 19.071828644598128 with std:118.42210462599799.\n",
      "19.071828644598128 0.0837677640068292 12\n",
      "Evaluating for {'degree': 12, 'lmda': 0.10926008611173785} ...\n",
      "The training loss is 4.527564138916357 with std:11.380419212926656. The val loss is 450447878.1480407 with std:5031817937.013607.\n",
      "450447878.1480407 0.10926008611173785 12\n",
      "The training loss is 4.4829464917403286 with std:9.314181625890065. The val loss is 10.099523264104453 with std:24.051691907094384.\n",
      "10.099523264104453 0.10926008611173785 12\n",
      "The training loss is 3.8512518382133103 with std:6.779181382123733. The val loss is 18.968300903566654 with std:117.40493731556059.\n",
      "18.968300903566654 0.10926008611173785 12\n",
      "Evaluating for {'degree': 12, 'lmda': 0.14251026703029984} ...\n",
      "The training loss is 4.588736307701859 with std:11.568659627312778. The val loss is 311025167.69798625 with std:3474320847.5009465.\n",
      "311025167.69798625 0.14251026703029984 12\n",
      "The training loss is 4.535390084264196 with std:9.439176154112205. The val loss is 9.872497052055824 with std:22.32195237869026.\n",
      "9.872497052055824 0.14251026703029984 12\n",
      "The training loss is 3.892446950605451 with std:6.846648832328997. The val loss is 18.65616224469086 with std:114.03069776004722.\n",
      "18.65616224469086 0.14251026703029984 12\n",
      "Evaluating for {'degree': 12, 'lmda': 0.18587918911465645} ...\n",
      "The training loss is 4.650855782995311 with std:11.751411816114343. The val loss is 208818834.3164953 with std:2332569146.9085526.\n",
      "208818834.3164953 0.18587918911465645 12\n",
      "The training loss is 4.587895749487008 with std:9.55164951365877. The val loss is 9.679076805808307 with std:20.978793624873262.\n",
      "9.679076805808307 0.18587918911465645 12\n",
      "The training loss is 3.9368588771836257 with std:6.913682672847001. The val loss is 18.166085011940446 with std:108.66055313423895.\n",
      "18.166085011940446 0.18587918911465645 12\n",
      "Evaluating for {'degree': 12, 'lmda': 0.24244620170823283} ...\n",
      "The training loss is 4.713268680630554 with std:11.924853490591145. The val loss is 138141481.51809773 with std:1543029907.7694597.\n",
      "138141481.51809773 0.24244620170823283 12\n",
      "The training loss is 4.641208302361817 with std:9.651533834578753. The val loss is 9.507473643772617 with std:19.905503564073463.\n",
      "9.507473643772617 0.24244620170823283 12\n",
      "The training loss is 3.9857122282154016 with std:6.982219776603604. The val loss is 17.54305796953699 with std:101.81744428294493.\n",
      "17.54305796953699 0.24244620170823283 12\n",
      "Evaluating for {'degree': 12, 'lmda': 0.31622776601683794} ...\n",
      "The training loss is 4.775549869833131 with std:12.085791942399979. The val loss is 91604258.59819546 with std:1023162042.7747638.\n",
      "91604258.59819546 0.31622776601683794 12\n",
      "The training loss is 4.696303068110316 with std:9.740142599251087. The val loss is 9.350642213742733 with std:19.02261728574395.\n",
      "9.350642213742733 0.31622776601683794 12\n",
      "The training loss is 4.040545507439676 with std:7.055005445439432. The val loss is 16.841755268022528 with std:94.13116615397615.\n",
      "16.841755268022528 0.31622776601683794 12\n",
      "Evaluating for {'degree': 12, 'lmda': 0.41246263829013524} ...\n",
      "The training loss is 4.837576393725256 with std:12.231951217198876. The val loss is 62132628.380303934 with std:693934902.6780554.\n",
      "62132628.380303934 0.41246263829013524 12\n",
      "The training loss is 4.754296872271551 with std:9.820280224497205. The val loss is 9.205493305559548 with std:18.281748853080234.\n",
      "9.205493305559548 0.41246263829013524 12\n",
      "The training loss is 4.103147058887082 with std:7.135602320699309. The val loss is 16.119138103386167 with std:86.25326432988741.\n",
      "16.119138103386167 0.41246263829013524 12\n",
      "Evaluating for {'degree': 12, 'lmda': 0.5379838403443686} ...\n",
      "The training loss is 4.899541097436828 with std:12.362255666920545. The val loss is 43989475.27493186 with std:491258009.60311604.\n",
      "43989475.27493186 0.5379838403443686 12\n",
      "The training loss is 4.816342479928476 with std:9.896261981902631. The val loss is 9.071540253858727 with std:17.655372240452724.\n",
      "9.071540253858727 0.5379838403443686 12\n",
      "The training loss is 4.175426307751944 with std:7.228295608024739. The val loss is 15.425876626140226 with std:78.75949981287599.\n",
      "15.425876626140226 0.5379838403443686 12\n",
      "Evaluating for {'degree': 12, 'lmda': 0.701703828670383} ...\n",
      "The training loss is 4.961942624114836 with std:12.477145218451664. The val loss is 33015669.17374861 with std:368668950.4224069.\n",
      "33015669.17374861 0.701703828670383 12\n",
      "The training loss is 4.883529404597261 with std:9.973790598799614. The val loss is 8.949387122320545 with std:17.126266896613846.\n",
      "8.949387122320545 0.701703828670383 12\n",
      "The training loss is 4.259243358149691 with std:7.337848903003458. The val loss is 14.798917694869786 with std:72.06622684956449.\n",
      "14.798917694869786 0.701703828670383 12\n",
      "Evaluating for {'degree': 12, 'lmda': 0.9152473108773893} ...\n",
      "The training loss is 5.025597951264354 with std:12.578915335924998. The val loss is 26421353.921813548 with std:295002198.5934954.\n",
      "26421353.921813548 0.9152473108773893 12\n",
      "The training loss is 4.956822377290558 with std:10.059676931616014. The val loss is 8.839397751535849 with std:16.679392668165736.\n",
      "8.839397751535849 0.9152473108773893 12\n",
      "The training loss is 4.356236615585924 with std:7.469105542794939. The val loss is 14.257316850633647 with std:66.38472761182548.\n",
      "14.257316850633647 0.9152473108773893 12\n",
      "Evaluating for {'degree': 12, 'lmda': 1.1937766417144369} ...\n",
      "The training loss is 5.091720121978891 with std:12.672050383686829. The val loss is 22419943.40854286 with std:250300511.82502195.\n",
      "22419943.40854286 1.1937766417144369 12\n",
      "The training loss is 5.037083307385792 with std:10.161494968088535. The val loss is 8.740776173226068 with std:16.2974716092276.\n",
      "8.740776173226068 1.1937766417144369 12\n",
      "The training loss is 4.467702121515042 with std:7.626513968100651. The val loss is 13.802310569162854 with std:61.72409070547495.\n",
      "13.802310569162854 1.1937766417144369 12\n",
      "Evaluating for {'degree': 12, 'lmda': 1.5570684047537318} ...\n",
      "The training loss is 5.162101260816585 with std:12.763532713654998. The val loss is 19885742.066138502 with std:221989954.6598481.\n",
      "19885742.066138502 1.5570684047537318 12\n",
      "The training loss is 5.1252445613338615 with std:10.287380454217933. The val loss is 8.651212311216856 with std:15.960177334076327.\n",
      "8.651212311216856 1.5570684047537318 12\n",
      "The training loss is 4.59460572608976 with std:7.813766438874916. The val loss is 13.421242525259055 with std:57.93655184520985.\n",
      "13.421242525259055 1.5570684047537318 12\n",
      "Evaluating for {'degree': 12, 'lmda': 2.030917620904737} ...\n",
      "The training loss is 5.2394486066335935 with std:12.863146233936293. The val loss is 18107397.500920527 with std:202125306.43441477.\n",
      "18107397.500920527 2.030917620904737 12\n",
      "The training loss is 5.222723654409318 with std:10.446239447989516. The val loss is 8.567202377922385 with std:15.64598139114916.\n",
      "8.567202377922385 2.030917620904737 12\n",
      "The training loss is 4.737852151145861 with std:8.033832402143734. The val loss is 13.094102550999496 with std:54.788274864764425.\n",
      "13.094102550999496 2.030917620904737 12\n",
      "Evaluating for {'degree': 12, 'lmda': 2.6489692876105297} ...\n",
      "The training loss is 5.327939780769496 with std:12.983823004885396. The val loss is 16638114.638216576 with std:185716398.73990253.\n",
      "16638114.638216576 2.6489692876105297 12\n",
      "The training loss is 5.332179371659655 with std:10.648572087056312. The val loss is 8.485093782854827 with std:15.335454815165376.\n",
      "8.485093782854827 2.6489692876105297 12\n",
      "The training loss is 4.898965801231279 with std:8.289677906403712. The val loss is 12.801150837927116 with std:52.035403502928894.\n",
      "12.801150837927116 2.6489692876105297 12\n",
      "Evaluating for {'degree': 12, 'lmda': 3.4551072945922217} ...\n",
      "The training loss is 5.434097237353448 with std:13.142095212020081. The val loss is 15213114.892471505 with std:169805784.02603602.\n",
      "15213114.892471505 3.4551072945922217 12\n",
      "The training loss is 5.458709515345941 with std:10.907959091887982. The val loss is 8.402817321869986 with std:15.015001262163542.\n",
      "8.402817321869986 3.4551072945922217 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 5.081329569552401 with std:8.5858589241645. The val loss is 12.530051952943985 with std:49.48662914598462.\n",
      "12.530051952943985 3.4551072945922217 12\n",
      "Evaluating for {'degree': 12, 'lmda': 4.506570337745478} ...\n",
      "The training loss is 5.568149600783602 with std:13.358737949381151. The val loss is 13702053.207829205 with std:152937117.82610217.\n",
      "13702053.207829205 4.506570337745478 12\n",
      "The training loss is 5.611616524404924 with std:11.243098607231762. The val loss is 8.322224944172675 with std:14.68038802625036.\n",
      "8.322224944172675 4.506570337745478 12\n",
      "The training loss is 5.292105829751046 with std:8.931007109064407. The val loss is 12.28114092243692 with std:47.03803360199734.\n",
      "12.28114092243692 4.506570337745478 12\n",
      "Evaluating for {'degree': 12, 'lmda': 5.878016072274912} ...\n",
      "The training loss is 5.746173384788921 with std:13.659756495545315. The val loss is 12073926.840414258 with std:134763274.0076449.\n",
      "12073926.840414258 5.878016072274912 12\n",
      "The training loss is 5.806972551237027 with std:11.680212429710052. The val loss is 8.252056596320708 with std:14.339938361143876.\n",
      "8.252056596320708 5.878016072274912 12\n",
      "The training loss is 5.545011754937507 with std:9.341100002923735. The val loss is 12.070241156683363 with std:44.67486632459523.\n",
      "12.070241156683363 5.878016072274912 12\n",
      "Evaluating for {'degree': 12, 'lmda': 7.666822074546214} ...\n",
      "The training loss is 5.993500625225766 with std:14.077982578597233. The val loss is 10364221.541338336 with std:115679572.9661028.\n",
      "10364221.541338336 7.666822074546214 12\n",
      "The training loss is 6.071429251314523 with std:12.255675578885215. The val loss is 8.211860465993322 with std:14.017847226584518.\n",
      "8.211860465993322 7.666822074546214 12\n",
      "The training loss is 5.864311955518902 with std:9.843391739092427. The val loss is 11.929852315441439 with std:42.44679135193588.\n",
      "11.929852315441439 7.666822074546214 12\n",
      "Evaluating for {'degree': 12, 'lmda': 10.0} ...\n",
      "The training loss is 6.350117526188108 with std:14.655620895295092. The val loss is 8643332.120056156 with std:96471270.6644354.\n",
      "8643332.120056156 10.0 12\n",
      "The training loss is 6.448014276747612 with std:13.01878077016159. The val loss is 8.237616815417041 with std:13.758668701526586.\n",
      "8.237616815417041 10.0 12\n",
      "The training loss is 6.29071511031736 with std:10.480901800703458. The val loss is 11.910896325898754 with std:40.43232580031783.\n",
      "11.910896325898754 10.0 12\n",
      "Evaluating for {'degree': 13, 'lmda': 0.01} ...\n",
      "The training loss is 4.053513103649818 with std:9.8698042921712. The val loss is 1643593408.4632585 with std:18365061723.867325.\n",
      "1643593408.4632585 0.01 13\n",
      "The training loss is 3.989144776985466 with std:8.021020197178922. The val loss is 15.482447751184942 with std:74.09353825232594.\n",
      "15.482447751184942 0.01 13\n",
      "The training loss is 3.5116896959643142 with std:6.082660304965941. The val loss is 9.708437156847467 with std:25.69897511260262.\n",
      "9.708437156847467 0.01 13\n",
      "Evaluating for {'degree': 13, 'lmda': 0.013043213867190054} ...\n",
      "The training loss is 4.096351913536543 with std:10.012759148870293. The val loss is 2751601513.6345606 with std:30746577924.323517.\n",
      "2751601513.6345606 0.013043213867190054 13\n",
      "The training loss is 4.040768883506429 with std:8.1459762206999. The val loss is 14.52066658663522 with std:65.67563271298236.\n",
      "14.52066658663522 0.013043213867190054 13\n",
      "The training loss is 3.5470082772373557 with std:6.157066567732176. The val loss is 10.912983487070468 with std:34.3695464039166.\n",
      "10.912983487070468 0.013043213867190054 13\n",
      "Evaluating for {'degree': 13, 'lmda': 0.017012542798525893} ...\n",
      "The training loss is 4.139526817135396 with std:10.156730035054759. The val loss is 3684413087.228266 with std:41170520599.29542.\n",
      "3684413087.228266 0.017012542798525893 13\n",
      "The training loss is 4.092727846522006 with std:8.278984109331045. The val loss is 13.597000741028667 with std:57.44349542425274.\n",
      "13.597000741028667 0.017012542798525893 13\n",
      "The training loss is 3.581624398272245 with std:6.232772273549707. The val loss is 12.336520808917351 with std:47.41273961287018.\n",
      "12.336520808917351 0.017012542798525893 13\n",
      "Evaluating for {'degree': 13, 'lmda': 0.02218982341458972} ...\n",
      "The training loss is 4.18345930059385 with std:10.30424697813666. The val loss is 4282225685.7715516 with std:47851066625.02592.\n",
      "4282225685.7715516 0.02218982341458972 13\n",
      "The training loss is 4.145120449169954 with std:8.41959861606463. The val loss is 12.746938350381978 with std:49.785512743517046.\n",
      "12.746938350381978 0.02218982341458972 13\n",
      "The training loss is 3.6157984397876337 with std:6.309609582505365. The val loss is 13.88430606050385 with std:63.099201963646486.\n",
      "13.88430606050385 0.02218982341458972 13\n",
      "Evaluating for {'degree': 13, 'lmda': 0.028942661247167517} ...\n",
      "The training loss is 4.228717215759042 with std:10.457708879257462. The val loss is 4486732122.155926 with std:50136588519.43599.\n",
      "4486732122.155926 0.028942661247167517 13\n",
      "The training loss is 4.1979709486234675 with std:8.566584755929998. The val loss is 11.99735573689847 with std:42.98542922435239.\n",
      "11.99735573689847 0.028942661247167517 13\n",
      "The training loss is 3.6498251674595914 with std:6.387175875524002. The val loss is 15.461413988521407 with std:79.839348066053.\n",
      "15.461413988521407 0.028942661247167517 13\n",
      "Evaluating for {'degree': 13, 'lmda': 0.037750532053243954} ...\n",
      "The training loss is 4.275945622264954 with std:10.618929895541317. The val loss is 4324169679.198132 with std:48320236395.52607.\n",
      "4324169679.198132 0.037750532053243954 13\n",
      "The training loss is 4.251207803766648 with std:8.71793187784738. The val loss is 11.36299353848911 with std:37.20013402053455.\n",
      "11.36299353848911 0.037750532053243954 13\n",
      "The training loss is 3.6839928921486393 with std:6.464858100572754. The val loss is 16.97825339677058 with std:96.31428113017881.\n",
      "16.97825339677058 0.037750532053243954 13\n",
      "Evaluating for {'degree': 13, 'lmda': 0.04923882631706739} ...\n",
      "The training loss is 4.3257552120266825 with std:10.788727555514196. The val loss is 3879094328.723468 with std:43346869626.34181.\n",
      "3879094328.723468 0.04923882631706739 13\n",
      "The training loss is 4.304666689757116 with std:8.87095678431024. The val loss is 10.846011462575536 with std:32.46543553187627.\n",
      "10.846011462575536 0.04923882631706739 13\n",
      "The training loss is 3.7185729895433575 with std:6.5419201144953325. The val loss is 18.353236141760963 with std:111.42067123841339.\n",
      "18.353236141760963 0.04923882631706739 13\n",
      "Evaluating for {'degree': 13, 'lmda': 0.0642232542222936} ...\n",
      "The training loss is 4.378582998446307 with std:10.96664310493641. The val loss is 3264401601.4767284 with std:36478045193.487366.\n",
      "3264401601.4767284 0.0642232542222936 13\n",
      "The training loss is 4.3581290762792255 with std:9.022515481066085. The val loss is 10.437984934106936 with std:28.72076218478241.\n",
      "10.437984934106936 0.0642232542222936 13\n",
      "The training loss is 3.7538458871533136 with std:6.617643914177856. The val loss is 19.514272902854067 with std:124.23614335401044.\n",
      "19.514272902854067 0.0642232542222936 13\n",
      "Evaluating for {'degree': 13, 'lmda': 0.0837677640068292} ...\n",
      "The training loss is 4.43456445622525 with std:11.150872097862221. The val loss is 2592877492.351821 with std:28974105019.438126.\n",
      "2592877492.351821 0.0837677640068292 13\n",
      "The training loss is 4.411395828567647 with std:9.169316978658252. The val loss is 10.1231985385408 with std:25.841538309811618.\n",
      "10.1231985385408 0.0837677640068292 13\n",
      "The training loss is 3.790155326207296 with std:6.691496736968232. The val loss is 20.40124449436237 with std:134.01986187777268.\n",
      "20.40124449436237 0.0837677640068292 13\n",
      "Evaluating for {'degree': 13, 'lmda': 0.10926008611173785} ...\n",
      "The training loss is 4.493469958515068 with std:11.338439448868577. The val loss is 1955474289.6573994 with std:21851413408.415623.\n",
      "1955474289.6573994 0.10926008611173785 13\n",
      "The training loss is 4.464380863255201 with std:9.308297645000366. The val loss is 9.88221085064429 with std:23.67071411616525.\n",
      "9.88221085064429 0.10926008611173785 13\n",
      "The training loss is 3.827976391692993 with std:6.76329394957808. The val loss is 20.970689121628013 with std:140.24848486523163.\n",
      "20.970689121628013 0.10926008611173785 13\n",
      "Evaluating for {'degree': 13, 'lmda': 0.14251026703029984} ...\n",
      "The training loss is 4.554743184989176 with std:11.525583495154399. The val loss is 1410002993.3156803 with std:15755996644.673504.\n",
      "1410002993.3156803 0.14251026703029984 13\n",
      "The training loss is 4.517199112964285 with std:9.436992135366955. The val loss is 9.69506067689901 with std:22.045039612261213.\n",
      "9.69506067689901 0.14251026703029984 13\n",
      "The training loss is 3.8679846907468742 with std:6.833341251687869. The val loss is 21.20211894168931 with std:142.67604030133.\n",
      "21.20211894168931 0.14251026703029984 13\n",
      "Evaluating for {'degree': 13, 'lmda': 0.18587918911465645} ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 4.617637922280376 with std:11.708250252362726. The val loss is 980697662.6380763 with std:10958672296.996569.\n",
      "980697662.6380763 0.18587918911465645 13\n",
      "The training loss is 4.57022222263043 with std:9.553845634903928. The val loss is 9.5438159313577 with std:20.814679736932604.\n",
      "9.5438159313577 0.18587918911465645 13\n",
      "The training loss is 3.9111166484494833 with std:6.902555896710664. The val loss is 21.103651343786716 with std:141.39046829880056.\n",
      "21.103651343786716 0.18587918911465645 13\n",
      "Evaluating for {'degree': 13, 'lmda': 0.24244620170823283} ...\n",
      "The training loss is 4.681406652222974 with std:11.882581467759028. The val loss is 665684291.2149054 with std:7438508718.568493.\n",
      "665684291.2149054 0.24244620170823283 13\n",
      "The training loss is 4.624085477547021 with std:9.658447297418011. The val loss is 9.414309237030276 with std:19.85607709625044.\n",
      "9.414309237030276 0.24244620170823283 13\n",
      "The training loss is 3.9586084708755354 with std:6.972572476288631. The val loss is 20.713995853434454 with std:136.83270309105652.\n",
      "20.713995853434454 0.24244620170823283 13\n",
      "Evaluating for {'degree': 13, 'lmda': 0.31622776601683794} ...\n",
      "The training loss is 4.745476655172915 with std:12.045320768066933. The val loss is 447674765.918267 with std:5002324355.08303.\n",
      "447674765.918267 0.31622776601683794 13\n",
      "The training loss is 4.67964548591295 with std:9.751697662296046. The val loss is 9.296943788836717 with std:19.078115687288587.\n",
      "9.296943788836717 0.31622776601683794 13\n",
      "The training loss is 4.011996842507391 with std:7.045829903884914. The val loss is 20.098625270121982 with std:129.75237062788614.\n",
      "20.098625270121982 0.31622776601683794 13\n",
      "Evaluating for {'degree': 13, 'lmda': 0.41246263829013524} ...\n",
      "The training loss is 4.809566306709223 with std:12.194132067247846. The val loss is 303759060.12060195 with std:3394111387.181628.\n",
      "303759060.12060195 0.41246263829013524 13\n",
      "The training loss is 4.737903190066546 with std:9.835925633035895. The val loss is 9.186537362858836 with std:18.42167926665935.\n",
      "9.186537362858836 0.41246263829013524 13\n",
      "The training loss is 4.0730662106451 with std:7.125616423543758. The val loss is 19.33982794527271 with std:121.09481113869833.\n",
      "19.33982794527271 0.41246263829013524 13\n",
      "Evaluating for {'degree': 13, 'lmda': 0.5379838403443686} ...\n",
      "The training loss is 4.873732456707193 with std:12.327875036270772. The val loss is 212064848.16040194 with std:2369455206.8378916.\n",
      "212064848.16040194 0.5379838403443686 13\n",
      "The training loss is 4.799915618408778 with std:9.914942540873902. The val loss is 9.081351589341855 with std:17.853503800408934.\n",
      "9.081351589341855 0.5379838403443686 13\n",
      "The training loss is 4.143739928800934 with std:7.216028581702031. The val loss is 18.522588494182024 with std:111.83990241323501.\n",
      "18.522588494182024 0.5379838403443686 13\n",
      "Evaluating for {'degree': 13, 'lmda': 0.701703828670383} ...\n",
      "The training loss is 4.938376060078555 with std:12.446885778122194. The val loss is 154978493.2817003 with std:1731529240.0268269.\n",
      "154978493.2817003 0.701703828670383 13\n",
      "The training loss is 4.86672115139148 with std:9.993992291027565. The val loss is 8.981640390679212 with std:17.356614728154867.\n",
      "8.981640390679212 0.701703828670383 13\n",
      "The training loss is 4.225930838010278 with std:7.321793667315358. The val loss is 17.72002163497881 with std:102.8342970924537.\n",
      "17.72002163497881 0.701703828670383 13\n",
      "Evaluating for {'degree': 13, 'lmda': 0.9152473108773893} ...\n",
      "The training loss is 5.00425211692657 with std:12.553279822793007. The val loss is 119793412.80959861 with std:1338341652.565964.\n",
      "119793412.80959861 0.9152473108773893 13\n",
      "The training loss is 4.939301987933598 with std:10.07956919192168. The val loss is 8.888131590530584 with std:16.920541036511825.\n",
      "8.888131590530584 0.9152473108773893 13\n",
      "The training loss is 4.321383231628602 with std:7.447931974796118. The val loss is 16.982495541142203 with std:94.66458435329336.\n",
      "16.982495541142203 0.9152473108773893 13\n",
      "Evaluating for {'degree': 13, 'lmda': 1.1937766417144369} ...\n",
      "The training loss is 5.072532560368317 with std:12.65126690597555. The val loss is 97978274.73229417 with std:1094558583.9522789.\n",
      "97978274.73229417 1.1937766417144369 13\n",
      "The training loss is 5.0186148604884995 with std:10.17915080796447. The val loss is 8.800809605451834 with std:16.534126365520816.\n",
      "8.800809605451834 1.1937766417144369 13\n",
      "The training loss is 4.43155415188781 with std:7.599308108212659. The val loss is 16.33325743606624 with std:87.60457639983949.\n",
      "16.33325743606624 1.1937766417144369 13\n",
      "Evaluating for {'degree': 13, 'lmda': 1.5570684047537318} ...\n",
      "The training loss is 5.14496967797645 with std:12.747469236391126. The val loss is 84030528.25156204 with std:938692829.0796038.\n",
      "84030528.25156204 1.5570684047537318 13\n",
      "The training loss is 5.1057455170675485 with std:10.301009251337227. The val loss is 8.718265954363511 with std:16.182354389020244.\n",
      "8.718265954363511 1.5570684047537318 13\n",
      "The training loss is 4.557610347075197 with std:7.780231682214632. The val loss is 15.771032806150435 with std:81.64257969839778.\n",
      "15.771032806150435 1.5570684047537318 13\n",
      "Evaluating for {'degree': 13, 'lmda': 2.030917620904737} ...\n",
      "The training loss is 5.224214355120935 with std:12.851261276095174. The val loss is 74459056.48097476 with std:831734771.7580107.\n",
      "74459056.48097476 2.030917620904737 13\n",
      "The training loss is 5.202278256082688 with std:10.454352979366. The val loss is 8.637790623543959 with std:15.847040071228681.\n",
      "8.637790623543959 2.030917620904737 13\n",
      "The training loss is 4.700662268727746 with std:7.994377789274544. The val loss is 15.278039593876613 with std:76.56909016180511.\n",
      "15.278039593876613 2.030917620904737 13\n",
      "Evaluating for {'degree': 13, 'lmda': 2.6489692876105297} ...\n",
      "The training loss is 5.31436139208417 with std:12.975174201475623. The val loss is 67074405.7625008 with std:749220122.9489144.\n",
      "67074405.7625008 2.6489692876105297 13\n",
      "The training loss is 5.31099900376416 with std:10.650039787637777. The val loss is 8.556288601137902 with std:15.510218354622811.\n",
      "8.556288601137902 2.6489692876105297 13\n",
      "The training loss is 4.862395136033763 with std:8.245342631842616. The val loss is 14.830862089187603 with std:72.09196839702427.\n",
      "14.830862089187603 2.6489692876105297 13\n",
      "Evaluating for {'degree': 13, 'lmda': 3.4551072945922217} ...\n",
      "The training loss is 5.421827379829335 with std:13.135420020946562. The val loss is 60554858.62428017 with std:676380250.4040331.\n",
      "60554858.62428017 3.4551072945922217 13\n",
      "The training loss is 5.437059153638027 with std:10.901975154778784. The val loss is 8.472004147877334 with std:15.158776000259868.\n",
      "8.472004147877334 3.4551072945922217 13\n",
      "The training loss is 5.0462594004171075 with std:8.538081390988078. The val loss is 14.411424470147251 with std:67.94457606692576.\n",
      "14.411424470147251 3.4551072945922217 13\n",
      "Evaluating for {'degree': 13, 'lmda': 4.506570337745478} ...\n",
      "The training loss is 5.5567312990711235 with std:13.352608774787807. The val loss is 54186706.4555907 with std:605239508.5961411.\n",
      "54186706.4555907 4.506570337745478 13\n",
      "The training loss is 5.589744755120139 with std:11.22913697748688. The val loss is 8.386964725607022 with std:14.78914090188194.\n",
      "8.386964725607022 4.506570337745478 13\n",
      "The training loss is 5.259364497426303 with std:8.881314836879973. The val loss is 14.01558731604631 with std:63.958697161697.\n",
      "14.01558731604631 4.506570337745478 13\n",
      "Evaluating for {'degree': 13, 'lmda': 5.878016072274912} ...\n",
      "The training loss is 5.735067657649251 with std:13.652792204124525. The val loss is 47688247.08206787 with std:532648593.0228471.\n",
      "47688247.08206787 5.878016072274912 13\n",
      "The training loss is 5.785079081790055 with std:11.65804975452566. The val loss is 8.310119217352403 with std:14.411427573035201.\n",
      "8.310119217352403 5.878016072274912 13\n",
      "The training loss is 5.51525814385487 with std:9.290833574184433. The val loss is 13.65790233543183 with std:60.087237071317574.\n",
      "13.65790233543183 5.878016072274912 13\n",
      "Evaluating for {'degree': 13, 'lmda': 7.666822074546214} ...\n",
      "The training loss is 5.982147586618888 with std:14.069072228389087. The val loss is 41066210.837652706 with std:458680584.5227817.\n",
      "41066210.837652706 7.666822074546214 13\n",
      "The training loss is 6.049679832339586 with std:12.225514681663057. The val loss is 8.261411333961156 with std:14.053187441411303.\n",
      "8.261411333961156 7.666822074546214 13\n",
      "The training loss is 5.837948731504751 with std:9.793558505367397. The val loss is 13.372941827806066 with std:56.38008525497887.\n",
      "13.372941827806066 7.666822074546214 13\n",
      "Evaluating for {'degree': 13, 'lmda': 10.0} ...\n",
      "The training loss is 6.33801933785102 with std:14.64409483307058. The val loss is 34489376.00961797 with std:385219191.44232905.\n",
      "34489376.00961797 10.0 13\n",
      "The training loss is 6.426583434634587 with std:12.98143741312226. The val loss is 8.277456827324407 with std:13.76364581492332.\n",
      "8.277456827324407 10.0 13\n",
      "The training loss is 6.267843138687839 with std:10.432193118101003. The val loss is 13.215682777615301 with std:52.933162126274475.\n",
      "13.215682777615301 10.0 13\n",
      "Evaluating for {'degree': 14, 'lmda': 0.01} ...\n",
      "The training loss is 4.039739988317937 with std:9.832694362452015. The val loss is 450457940.0751199 with std:5033493795.715482.\n",
      "450457940.0751199 0.01 14\n",
      "The training loss is 3.966221435490778 with std:7.971497541953082. The val loss is 18.130787827537937 with std:98.87028389368362.\n",
      "18.130787827537937 0.01 14\n",
      "The training loss is 3.4936183079775214 with std:6.047417836497711. The val loss is 8.749597000904147 with std:22.491476782106695.\n",
      "8.749597000904147 0.01 14\n",
      "Evaluating for {'degree': 14, 'lmda': 0.013043213867190054} ...\n",
      "The training loss is 4.081396636829182 with std:9.974996432554855. The val loss is 2858847725.31186 with std:31950603834.063618.\n",
      "2858847725.31186 0.013043213867190054 14\n",
      "The training loss is 4.0161410865914755 with std:8.092827592145678. The val loss is 16.830640232786674 with std:87.28415648300621.\n",
      "16.830640232786674 0.013043213867190054 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 3.529157320904516 with std:6.123207211372504. The val loss is 8.801006135463544 with std:22.513843970121496.\n",
      "8.801006135463544 0.013043213867190054 14\n",
      "Evaluating for {'degree': 14, 'lmda': 0.017012542798525893} ...\n",
      "The training loss is 4.123004587387384 with std:10.117216804006272. The val loss is 6155619666.438954 with std:68797691162.84921.\n",
      "6155619666.438954 0.017012542798525893 14\n",
      "The training loss is 4.066447559869879 with std:8.222869944980078. The val loss is 15.556466344165937 with std:75.86033103774696.\n",
      "15.556466344165937 0.017012542798525893 14\n",
      "The training loss is 3.563852148834713 with std:6.199920710478627. The val loss is 9.470870349796686 with std:23.75747095326903.\n",
      "9.470870349796686 0.017012542798525893 14\n",
      "Evaluating for {'degree': 14, 'lmda': 0.02218982341458972} ...\n",
      "The training loss is 4.164960470547131 with std:10.261767026393365. The val loss is 9255955762.635576 with std:103449712261.89589.\n",
      "9255955762.635576 0.02218982341458972 14\n",
      "The training loss is 4.11741719977945 with std:8.361566712741787. The val loss is 14.3610144102903 with std:65.09963671783296.\n",
      "14.3610144102903 0.02218982341458972 14\n",
      "The training loss is 3.597943569551172 with std:6.277497745181566. The val loss is 10.679505911458794 with std:30.975218633226195.\n",
      "10.679505911458794 0.02218982341458972 14\n",
      "Evaluating for {'degree': 14, 'lmda': 0.028942661247167517} ...\n",
      "The training loss is 4.207814444402306 with std:10.411089464297332. The val loss is 11463160063.380177 with std:128119612043.58345.\n",
      "11463160063.380177 0.028942661247167517 14\n",
      "The training loss is 4.169183637845819 with std:8.507945609630958. The val loss is 13.287763080571084 with std:55.40213839895551.\n",
      "13.287763080571084 0.028942661247167517 14\n",
      "The training loss is 3.6317168696509805 with std:6.355696733119216. The val loss is 12.310877441594968 with std:45.78448683200814.\n",
      "12.310877441594968 0.028942661247167517 14\n",
      "Evaluating for {'degree': 14, 'lmda': 0.037750532053243954} ...\n",
      "The training loss is 4.252232887286941 with std:10.56725047158469. The val loss is 12469168543.08506 with std:139364052818.36914.\n",
      "12469168543.08506 0.037750532053243954 14\n",
      "The training loss is 4.221722384159867 with std:8.660132642855423. The val loss is 12.364628890502168 with std:47.020583125740515.\n",
      "12.364628890502168 0.037750532053243954 14\n",
      "The training loss is 3.6654627629309084 with std:6.434091025941782. The val loss is 14.225351139264143 with std:65.68377370917952.\n",
      "14.225351139264143 0.037750532053243954 14\n",
      "Evaluating for {'degree': 14, 'lmda': 0.04923882631706739} ...\n",
      "The training loss is 4.298916039101596 with std:10.731522604309642. The val loss is 12292835536.955729 with std:137393658486.24792.\n",
      "12292835536.955729 0.04923882631706739 14\n",
      "The training loss is 4.274866293927016 with std:8.815463830849447. The val loss is 11.601915562844244 with std:40.0510373478574.\n",
      "11.601915562844244 0.04923882631706739 14\n",
      "The training loss is 3.699463351884667 with std:6.512115744154892. The val loss is 16.27174381457026 with std:87.94673340862376.\n",
      "16.27174381457026 0.04923882631706739 14\n",
      "Evaluating for {'degree': 14, 'lmda': 0.0642232542222936} ...\n",
      "The training loss is 4.348473145408461 with std:10.904042629322538. The val loss is 11179824985.180784 with std:124954109052.97571.\n",
      "11179824985.180784 0.0642232542222936 14\n",
      "The training loss is 4.328357144787736 with std:8.970709154413269. The val loss is 10.994160912073728 with std:34.45477186992413.\n",
      "10.994160912073728 0.0642232542222936 14\n",
      "The training loss is 3.7340118394461577 with std:6.589168543253673. The val loss is 18.296696937051326 with std:110.34895884706941.\n",
      "18.296696937051326 0.0642232542222936 14\n",
      "Evaluating for {'degree': 14, 'lmda': 0.0837677640068292} ...\n",
      "The training loss is 4.401282058239438 with std:11.083636818990975. The val loss is 9487736031.534908 with std:106042198830.88634.\n",
      "9487736031.534908 0.0837677640068292 14\n",
      "The training loss is 4.381930012299854 with std:9.12240100942329. The val loss is 10.524367346365102 with std:30.097133600317417.\n",
      "10.524367346365102 0.0837677640068292 14\n",
      "The training loss is 3.769461719545064 with std:6.664747617425428. The val loss is 20.152592327724967 with std:131.00500177952787.\n",
      "20.152592327724967 0.0837677640068292 14\n",
      "Evaluating for {'degree': 14, 'lmda': 0.10926008611173785} ...\n",
      "The training loss is 4.457383296787399 with std:11.267878855740777. The val loss is 7579821728.412578 with std:84717939912.90561.\n",
      "7579821728.412578 0.10926008611173785 14\n",
      "The training loss is 4.435416920559318 with std:9.26722844899173. The val loss is 10.168910080406638 with std:26.789139536001816.\n",
      "10.168910080406638 0.10926008611173785 14\n",
      "The training loss is 3.806292875479366 with std:6.738601713634801. The val loss is 21.70686765491541 with std:148.32325874168478.\n",
      "21.70686765491541 0.10926008611173785 14\n",
      "Evaluating for {'degree': 14, 'lmda': 0.14251026703029984} ...\n",
      "The training loss is 4.516459092285785 with std:11.453383315446406. The val loss is 5747710381.810414 with std:64240824844.902954.\n",
      "5747710381.810414 0.14251026703029984 14\n",
      "The training loss is 4.488846146598094 with std:9.402433135242639. The val loss is 9.901936941251195 with std:24.32272518726804.\n",
      "9.901936941251195 0.14251026703029984 14\n",
      "The training loss is 3.8451809395139147 with std:6.810873687609877. The val loss is 22.854428317069644 with std:161.0792538755033.\n",
      "22.854428317069644 0.14251026703029984 14\n",
      "Evaluating for {'degree': 14, 'lmda': 0.18587918911465645} ...\n",
      "The training loss is 4.577917689991582 with std:11.636263075809442. The val loss is 4174434430.9497166 with std:46656599720.52324.\n",
      "4174434430.9497166 0.18587918911465645 14\n",
      "The training loss is 4.5425096086104055 with std:9.526144099107297. The val loss is 9.698757866665662 with std:22.49641666485656.\n",
      "9.698757866665662 0.18587918911465645 14\n",
      "The training loss is 3.8870579868411963 with std:6.882232790693196. The val loss is 23.5316506419368 with std:168.5426413930815.\n",
      "23.5316506419368 0.18587918911465645 14\n",
      "Evaluating for {'degree': 14, 'lmda': 0.24244620170823283} ...\n",
      "The training loss is 4.641057534766333 with std:11.812636146560262. The val loss is 2935727801.9037037 with std:32811756912.214985.\n",
      "2935727801.9037037 0.24244620170823283 14\n",
      "The training loss is 4.59697824862009 with std:9.637620936148583. The val loss is 9.538175580376194 with std:21.132094572747597.\n",
      "9.538175580376194 0.24244620170823283 14\n",
      "The training loss is 3.933152229201814 with std:6.9539991157459236. The val loss is 23.727451694890483 with std:170.587713889543.\n",
      "23.727451694890483 0.24244620170823283 14\n",
      "Evaluating for {'degree': 14, 'lmda': 0.31622776601683794} ...\n",
      "The training loss is 4.705253805609559 with std:11.97908440926207. The val loss is 2026485937.2154434 with std:22649267429.96471.\n",
      "2026485937.2154434 0.31622776601683794 14\n",
      "The training loss is 4.653062585231261 with std:9.737416450337328. The val loss is 9.403842521349633 with std:20.084860875013412.\n",
      "9.403842521349633 0.31622776601683794 14\n",
      "The training loss is 3.9849925186685446 with std:7.028259138318009. The val loss is 23.48614366506473 with std:167.71976915752145.\n",
      "23.48614366506473 0.31622776601683794 14\n",
      "Evaluating for {'degree': 14, 'lmda': 0.41246263829013524} ...\n",
      "The training loss is 4.770110499988809 with std:12.133028332282079. The val loss is 1395657285.2514687 with std:15598552855.262388.\n",
      "1395657285.2514687 0.41246263829013524 14\n",
      "The training loss is 4.71173459644305 with std:9.827487205441216. The val loss is 9.284689968480322 with std:19.24728078891705.\n",
      "9.284689968480322 0.41246263829013524 14\n",
      "The training loss is 4.04436525813451 with std:7.1079540822318386. The val loss is 22.89889042539742 with std:160.97682336440093.\n",
      "22.89889042539742 0.41246263829013524 14\n",
      "Evaluating for {'degree': 14, 'lmda': 0.5379838403443686} ...\n",
      "The training loss is 4.835550779506932 with std:12.273041293466097. The val loss is 976515022.0350336 with std:10913826500.930464.\n",
      "976515022.0350336 0.5379838403443686 14\n",
      "The training loss is 4.774038890896769 with std:9.911260868827007. The val loss is 9.174468524792452 with std:18.54804412513359.\n",
      "9.174468524792452 0.5379838403443686 14\n",
      "The training loss is 4.113220241519369 with std:7.196899781140034. The val loss is 22.084588584470335 with std:151.71290810719918.\n",
      "22.084588584470335 0.5379838403443686 14\n",
      "Evaluating for {'degree': 14, 'lmda': 0.701703828670383} ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 4.901855925210489 with std:12.399147114383458. The val loss is 706210870.7616135 with std:7892645414.545912.\n",
      "706210870.7616135 0.701703828670383 14\n",
      "The training loss is 4.841021709996989 with std:9.993633343733729. The val loss is 9.070554997966909 with std:17.945206109403696.\n",
      "9.070554997966909 0.701703828670383 14\n",
      "The training loss is 4.193536072616777 with std:7.29968245698977. The val loss is 21.16512955198098 with std:141.31731721250551.\n",
      "21.16512955198098 0.701703828670383 14\n",
      "Evaluating for {'degree': 14, 'lmda': 0.9152473108773893} ...\n",
      "The training loss is 4.969690287929895 with std:12.513125887748545. The val loss is 534604084.31993073 with std:5974594019.774961.\n",
      "534604084.31993073 0.9152473108773893 14\n",
      "The training loss is 4.913700563115487 with std:10.080858671339499. The val loss is 8.972350083441288 with std:17.415801701796056.\n",
      "8.972350083441288 0.9152473108773893 14\n",
      "The training loss is 4.287170814620577 with std:7.421390408702961. The val loss is 20.24232767972694 with std:130.95189519712147.\n",
      "20.24232767972694 0.9152473108773893 14\n",
      "Evaluating for {'degree': 14, 'lmda': 1.1937766417144369} ...\n",
      "The training loss is 5.040161510155193 with std:12.618828653524673. The val loss is 425614327.02196115 with std:4756408120.224165.\n",
      "425614327.02196115 1.1937766417144369 14\n",
      "The training loss is 4.993098852930349 with std:10.180345341356128. The val loss is 8.87969372360996 with std:16.945160703939706.\n",
      "8.87969372360996 1.1937766417144369 14\n",
      "The training loss is 4.39574111471044 with std:7.567204827921775. The val loss is 19.38304913058849 with std:121.3808769079205.\n",
      "19.38304913058849 1.1937766417144369 14\n",
      "Evaluating for {'degree': 14, 'lmda': 1.5570684047537318} ...\n",
      "The training loss is 5.1149701215528225 with std:12.72250044422901. The val loss is 354894192.1720049 with std:3965962435.845833.\n",
      "354894192.1720049 1.5570684047537318 14\n",
      "The training loss is 5.08039191857026 with std:10.30048064936708. The val loss is 8.79171383230761 with std:16.519203122768925.\n",
      "8.79171383230761 1.5570684047537318 14\n",
      "The training loss is 4.5206034055174085 with std:7.741982531267097. The val loss is 18.615659258382507 with std:112.92884721891637.\n",
      "18.615659258382507 1.5570684047537318 14\n",
      "Evaluating for {'degree': 14, 'lmda': 2.030917620904737} ...\n",
      "The training loss is 5.1967107895437685 with std:12.833133658503453. The val loss is 306601725.4762719 with std:3426197302.228817.\n",
      "306601725.4762719 2.030917620904737 14\n",
      "The training loss is 5.177251349058166 with std:10.450714519271077. The val loss is 8.706431944951792 with std:16.12141016232694.\n",
      "8.706431944951792 2.030917620904737 14\n",
      "The training loss is 4.663057769604958 with std:7.950085678465907. The val loss is 17.93686960455982 with std:105.55493723908619.\n",
      "17.93686960455982 2.030917620904737 14\n",
      "Evaluating for {'degree': 14, 'lmda': 2.6489692876105297} ...\n",
      "The training loss is 5.289404606272104 with std:12.962897112228235. The val loss is 270664358.6319308 with std:3024536718.773721.\n",
      "270664358.6319308 2.6489692876105297 14\n",
      "The training loss is 5.286512352589493 with std:10.642163909865143. The val loss is 8.621314297303666 with std:15.734188173810487.\n",
      "8.621314297303666 2.6489692876105297 14\n",
      "The training loss is 4.824938239385381 with std:8.19578591123844. The val loss is 17.32537799064276 with std:98.99890449466528.\n",
      "17.32537799064276 2.6489692876105297 14\n",
      "Evaluating for {'degree': 14, 'lmda': 3.4551072945922217} ...\n",
      "The training loss is 5.399373332525022 with std:13.127696788612894. The val loss is 240882469.793736 with std:2691690646.6915545.\n",
      "240882469.793736 3.4551072945922217 14\n",
      "The training loss is 5.413307569691631 with std:10.888905107666897. The val loss is 8.53480035157701 with std:15.34304991647861.\n",
      "8.53480035157701 3.4551072945922217 14\n",
      "The training loss is 5.009762720816209 with std:8.484534922889189. The val loss is 16.757766949553538 with std:92.94424034255158.\n",
      "16.757766949553538 3.4551072945922217 14\n",
      "Evaluating for {'degree': 14, 'lmda': 4.506570337745478} ...\n",
      "The training loss is 5.53662812795384 with std:13.34793917562259. The val loss is 213710092.60640225 with std:2388026278.784001.\n",
      "213710092.60640225 4.506570337745478 14\n",
      "The training loss is 5.566828788383812 with std:11.209948190784399. The val loss is 8.448718369574724 with std:14.94180196044448.\n",
      "8.448718369574724 4.506570337745478 14\n",
      "The training loss is 5.224601107611653 with std:8.825250934883483. The val loss is 16.22238476883648 with std:87.14900126834074.\n",
      "16.22238476883648 4.506570337745478 14\n",
      "Evaluating for {'degree': 14, 'lmda': 5.878016072274912} ...\n",
      "The training loss is 5.71706040718439 with std:13.649620961094291. The val loss is 187437434.9695524 with std:2094431137.5329869.\n",
      "187437434.9695524 5.878016072274912 14\n",
      "The training loss is 5.762951093977923 with std:11.631739149077319. The val loss is 8.371515641625882 with std:14.537519425554578.\n",
      "8.371515641625882 5.878016072274912 14\n",
      "The training loss is 5.482853781755937 with std:9.233599422986199. The val loss is 15.728206641491356 with std:81.5117750842862.\n",
      "15.728206641491356 5.878016072274912 14\n",
      "Evaluating for {'degree': 14, 'lmda': 7.666822074546214} ...\n",
      "The training loss is 5.965904099677949 with std:14.06596646913946. The val loss is 161575421.45026034 with std:1805434884.732739.\n",
      "161575421.45026034 7.666822074546214 14\n",
      "The training loss is 6.028132785506737 with std:12.190980982270991. The val loss is 8.322472350374092 with std:14.155083480809271.\n",
      "8.322472350374092 7.666822074546214 14\n",
      "The training loss is 5.808297333469309 with std:9.736135689306249. The val loss is 15.308105288984807 with std:76.06592116289688.\n",
      "15.308105288984807 7.666822074546214 14\n",
      "Evaluating for {'degree': 14, 'lmda': 10.0} ...\n",
      "The training loss is 6.3231732281951025 with std:14.639915254862952. The val loss is 136354793.06765765 with std:1523611862.7791982.\n",
      "136354793.06765765 10.0 14\n",
      "The training loss is 6.40528661149204 with std:12.93757892305938. The val loss is 8.33750147595226 with std:13.842027761980017.\n",
      "8.33750147595226 10.0 14\n",
      "The training loss is 6.24106049186098 with std:10.375109580636433. The val loss is 15.018907873504453 with std:70.92220536411101.\n",
      "15.018907873504453 10.0 14\n"
     ]
    }
   ],
   "source": [
    "#list of lambda values to try.. use np.logspace\n",
    "search_lambda = np.logspace(-2,1,num=27)\n",
    "#list of degrees\n",
    "search_degree = np.arange(1,15,1)\n",
    "\n",
    "params = {'degree':search_degree,'lmda':search_lambda,}\n",
    "k_fold =3\n",
    "#call to the grid search function\n",
    "grid_val,grid_val_std = grid_search_cv(params,k_fold,fold_ind,do_cross_validation_reg,X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from helper import plot_cv_result\n",
    "plot_cv_result(np.log((grid_val.T)),search_lambda,search_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from helper import plot_cv_result\n",
    "plot_cv_result(np.log((grid_val_std.T)),search_lambda,search_degree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.910368715513828\n",
      "Best score achieved using degree:3 and lambda:3.4551072945922217\n"
     ]
    }
   ],
   "source": [
    "# best val score\n",
    "best_score = np.min(grid_val)\n",
    "print(best_score)\n",
    "\n",
    "# params which give best val score\n",
    "d,l= np.unravel_index(np.argmin(grid_val),grid_val.shape)\n",
    "best_degree = search_degree[d]\n",
    "best_lambda = search_lambda[l]\n",
    "print('Best score achieved using degree:{} and lambda:{}'.format(best_degree,best_lambda))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 7.06392522895881 with std:13.444314802640736. The test loss is 8.228462752602276 with std:11.38108061204404.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.228462752602276"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluate on the test set\n",
    "X_train_poly,mu,std = expand_and_normalize_X(X_train,best_degree)\n",
    "w = get_w_analytical_with_regularization(X_train_poly,y_train,best_lambda)\n",
    "X_test_poly = expand_X(X_test,best_degree)\n",
    "X_test_poly[:,1:] =  (X_test_poly[:,1:]-mu)/std\n",
    "\n",
    "get_loss(w,X_train_poly,y_train,X_test_poly,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How can you interpret the linear regression coefficients?\n",
    "\n",
    "**Answer**: The coefficients value closer to zero means that corresponding features are unimportant for the model. Positive coefficient represent positive correlation with the value to be predicted and similar for negative case.\n",
    "\n",
    "**Question**: Is it good to have coefficients' values close to zero? \n",
    "\n",
    "**Answer**: Having coefficients close to zero means corresponding features are redundant for the model predictions. Hence we can represent our data with smaller feature sets. It might become important, when we might have several thousand features and prune out redundant ones, improving the interpretibility of the model.\n",
    "\n",
    "**Question**: How would you proceed to improve the prediction?\n",
    "\n",
    "**Answer**: Better feature expansion. One way would be to choose feature combinations which have high coefficient values. Choosing better hyperparameter. Till now we have done Grid Search, it would better to RandomSearch on the parameter space. Trying out different methods like Random Forest or Neural Nets. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
