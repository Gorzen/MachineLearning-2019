{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1 Introduction\n",
    "\n",
    "Welcome to the ninth practical session of CS233 - Introduction to Machine Learning.  \n",
    "In this exercise class we will start using Machine Learning methods to solve regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Regression Problem\n",
    "\n",
    "Let $f$ be a function $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}^v$ with a data set $\\left(X \\subseteq \\mathbb{R}^d, y\n",
    "\\subseteq\\mathbb{R}^v \\right )$. The regression problem is the task of estimating an approximation $\\hat{f}$ of $f$.\n",
    "Within this exercise we consider the special case of $v=1$, i.e. the problem is univariate as opposed to multivariate.\n",
    "Specifically, we will analyze the Boston house prices data set and predict costs based on properties such as per capita crime rate by town, pupil-teacher ratio by town etc.\n",
    "\n",
    "We will model the given data by means of a linear regression model, i.e. a model that explains a dependent variable in terms of a linear combination of independent variables.\n",
    "\n",
    "- How does a regression problem differ from a classification problem?\n",
    "- Why is the linear regression model a linear model? Is it linear in the dependent variables? Is it linear in the parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Load and inspect data\n",
    "\n",
    "We load the data and split it such that 80% and 20% are train and test data, respectively. \n",
    "After, we normalize the data such that each feature has zero mean and unit standard deviation. Please fill in the required code and complete the function `normalize`.\n",
    "\n",
    "- Explore the relation between different features and the house prices. Describe what you see. Can you identify any trends?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the data set and print a description\n",
    "from sklearn.datasets import load_boston\n",
    "boston_dataset = load_boston()\n",
    "print(boston_dataset.DESCR)\n",
    "\n",
    "X = boston_dataset[\"data\"]\n",
    "y = boston_dataset[\"target\"]\n",
    "\n",
    "# remove categorical feature\n",
    "X = np.delete(X, 3, axis=1)\n",
    "# removing second mode\n",
    "ind = y<40\n",
    "X = X[ind,:]\n",
    "y = y[ind]\n",
    "\n",
    "# split the data into 80% training and 20% test data\n",
    "indices = np.arange(X.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "splitRatio = 0.8\n",
    "n          = X.shape[0]\n",
    "X_train    = X[indices[0:int(n*splitRatio)],:] \n",
    "y_train    = y[indices[0:int(n*splitRatio)]] \n",
    "X_test     = X[indices[int(n*(splitRatio)):],:] \n",
    "y_test     = y[indices[int(n*(splitRatio)):]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Make mean 1 and std dev 0 of the data.\n",
    "'''\n",
    "def normalize(X):\n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\"\n",
    "    \n",
    "    mu    = np.mean(X)\n",
    "    std   = np.std(X)\n",
    "    X     = (X - mu) / std  # normalize\n",
    "    return X, mu, std\n",
    "\n",
    "#Use train stats for normalizing test set\n",
    "X_train,mu_train,std_train = normalize(X_train)\n",
    "X_test = (X_test-mu_train)/std_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Attribute $X_0$ vs Price $y$')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# exploratory analysis of the data\n",
    "\n",
    "feature = 0\n",
    "plt.scatter(X_train[:,feature], y_train)\n",
    "plt.xlabel(f\"Attribute $X_{feature}$\")\n",
    "plt.ylabel(\"Price $y$\")\n",
    "plt.title(f\"Attribute $X_{feature}$ vs Price $y$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Closed-form solution for linear regression\n",
    "\n",
    "The linear regression model has an [analytical solution](https://en.wikipedia.org/wiki/Linear_least_squares). \n",
    "Please use this solution to complete the function `get_w_analytical` and to obtain the weight parameters $w$. Tip: You may want to use the function np.linalg.solve. \n",
    "\n",
    "- What is the time complexity of this approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_analytical(X_train,y_train):\n",
    "    \"\"\"compute the weight parameters w\"\"\"\n",
    "        \n",
    "    # compute w via the normal equation\n",
    "    # X @ w = y\n",
    "    w = np.linalg.solve(X_train.T @ X_train, X_train.T @ y_train)\n",
    "    return w\n",
    "\n",
    "def get_loss(w, X_train, y_train,X_test,y_test,val=False):\n",
    "    # predict dependent variables and MSE loss for seen training data\n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\"\n",
    "    loss_train = np.mean((y_train - X_train @ w) ** 2)\n",
    "    loss_train_std = np.std((y_train - X_train @ w) ** 2)\n",
    "        \n",
    "    # predict dependent variables and MSE loss for unseen test data\n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\"\n",
    "    loss_test = np.mean((y_test - X_test @ w) ** 2)\n",
    "    loss_test_std = np.std((y_test - X_test @ w) ** 2)\n",
    "    \n",
    "    if not val:\n",
    "        print(\"The training loss is {} with std:{}. The test loss is {} with std:{}.\".format(loss_train, loss_train_std, loss_test,loss_test_std))\n",
    "    else:\n",
    "        print(\"The training loss is {} with std:{}. The val loss is {} with std:{}.\".format(loss_train, loss_train_std, loss_test,loss_test_std))\n",
    "\n",
    "    return loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 10.743863402737372 with std:20.84919976462016. The test loss is 12.448309149096193 with std:24.933808088335738.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12.448309149096193"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute w and calculate its goodness\n",
    "w_ana = get_w_analytical(X_train,y_train)\n",
    "get_loss(w_ana, X_train,y_train, X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2.3 Feature expansion\n",
    "\n",
    "Similar to feature expansion for classification problems, we can also perform feature expansion here. Please complete the function `expand_X` and perform a degree-2 polynomial feature expansion of X, including a bias term but omitting interaction terms.\n",
    "\n",
    "- Is our model still a linear regression model? Why (not)?\n",
    "- How does linear regression on degree-2 polynomially expanded data compare against our previous model? Explain!\n",
    "- Try polynomial feature expansion for different parameters values of $d$. What do you observe? Explain!\n",
    "- Look up the concept of the condition number of a matrix. What does this tell us about the feature-expanded data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_X(X,d):\n",
    "    \"\"\"perform degree-d polynomial feature expansion of X, with bias but omitting interaction terms\"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\"\n",
    "    \n",
    "    expand = np.ones((np.shape(X)[0], 1))\n",
    "    \n",
    "    for i in range(1, d + 1):\n",
    "        expand = np.hstack((expand, X ** i))\n",
    "\n",
    "    return expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_and_normalize_X(X,d):\n",
    "    \"\"\"\n",
    "    perform degree-d polynomial feature expansion of X, with bias but omitting interaction terms\n",
    "    and normalize them and return mean and std\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\"\n",
    "    \n",
    "    expand = expand_X(X, d)\n",
    "    expand_no_bias = expand[:, 1:]\n",
    "    expand_no_bias, mu, std = normalize(expand_no_bias)\n",
    "    \n",
    "    expand[:, 1:] = expand_no_bias\n",
    "\n",
    "    return expand, mu, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original data has 12 features.\n",
      "After degree-2 polynomial feature expansion (with bias, without interaction terms) the data has 25 features.\n",
      "The original data X^TX has condition number 2651084.5926421764. \n",
      "The expanded data X^TX has condition number 1213502807339206.8.\n",
      "The training loss is 8.146544669840928 with std:18.144684610999942. The test loss is 8.57678443846797 with std:13.494133346766404.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.57678443846797"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform polynomial feature expansion\n",
    "d  = 2\n",
    "\n",
    "#normalize the data after expansion\n",
    "X_train_poly,mu_train_poly,std_train_poly = expand_and_normalize_X(X_train,d)\n",
    "X_test_poly  = expand_X(X_test,d)\n",
    "X_test_poly[:,1:]  = (X_test_poly[:,1:]-mu_train_poly)/std_train_poly\n",
    "\n",
    "\n",
    "print(\"The original data has {} features.\".format(X_train.shape[1]))\n",
    "print(\"After degree-{} polynomial feature expansion (with bias, without interaction terms) the data has {} features.\".format(d,X_train_poly.shape[1]))\n",
    "\n",
    "cond_num_before = np.linalg.cond(X_train.T@X_train)\n",
    "cond_num_after = np.linalg.cond(X_train_poly.T@X_train_poly)\n",
    "print(\"The original data X^TX has condition number {}. \\nThe expanded data X^TX has condition number {}.\".format(cond_num_before,cond_num_after))\n",
    "\n",
    "# re-compute w and calculate its goodness\n",
    "w_augm = get_w_analytical(X_train_poly,y_train)\n",
    "get_loss(w_augm, X_train_poly,y_train, X_test_poly,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above exercise, we directly evaluate model on test loss and choose the best degree of polynomial. But test should not be touched until your final model. So to choose best degree we'll use Cross Validation. We're going to K-Fold CV for that. We will use our training set and create K splits of it to choose best degree and finally evaluate on our test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for using kth split as validation set to get loss\n",
    "# and k-1 splits to train our model\n",
    "# k = kth fold\n",
    "# k_fold_ind = all the fold indices\n",
    "# X,Y= train data and labels\n",
    "# degree = degree of polynomial expansion\n",
    "\n",
    "def do_cross_validation(k,k_fold_ind,X,Y,degree=1):\n",
    "    \n",
    "    # use one split as val\n",
    "    val_ind = k_fold_ind[k]\n",
    "    # use k-1 split to train\n",
    "    train_splits = [i for i in range(k_fold_ind.shape[0]) if i is not k]\n",
    "    train_ind = k_fold_ind[train_splits,:].reshape(-1)\n",
    "    \n",
    "    #Get train and val \n",
    "    cv_X_train = X[train_ind,:]\n",
    "    cv_Y_train = Y[train_ind]\n",
    "    cv_X_val = X[val_ind,:]\n",
    "    cv_Y_val = Y[val_ind]\n",
    "\n",
    "    #expand and normalize for degree d\n",
    "    cv_X_train_poly,mu,std = expand_and_normalize_X(cv_X_train, d)\n",
    "    #apply the normalization using statistics (mean, std) computed on train data\n",
    "    cv_X_val_poly = expand_X(cv_X_val, d)\n",
    "    cv_X_val_poly[:, 1:] = (cv_X_val_poly[:, 1:] - mu)/std\n",
    "\n",
    "    \n",
    "    #fit on train set\n",
    "    w = get_w_analytical(cv_X_train_poly, cv_Y_train)\n",
    "    \n",
    "    #get loss for val\n",
    "    loss_test = get_loss(w,cv_X_train_poly,cv_Y_train,cv_X_val_poly,cv_Y_val,val=True)\n",
    "    return loss_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do 3-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import fold_indices\n",
    "k_fold=3\n",
    "\n",
    "# We create the k_fold splits of the train data and fix this\n",
    "num_train_examples = X_train.shape[0]\n",
    "fold_ind = fold_indices(num_train_examples,k_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating for {'degree': 1} ...\n",
      "The training loss is 6.704435190596883 with std:13.04269278065948. The val loss is 15.330671563605605 with std:59.19086488546753.\n",
      "The training loss is 8.59812318878593 with std:19.209738182676706. The val loss is 7.992187275870191 with std:12.953014962963975.\n",
      "The training loss is 8.295108705603784 with std:13.781580827324076. The val loss is 8.952265308550476 with std:18.538654250396608.\n",
      "Evaluating for {'degree': 2} ...\n",
      "The training loss is 6.704435190596883 with std:13.04269278065948. The val loss is 15.330671563605605 with std:59.19086488546753.\n",
      "The training loss is 8.59812318878593 with std:19.209738182676706. The val loss is 7.992187275870191 with std:12.953014962963975.\n",
      "The training loss is 8.295108705603784 with std:13.781580827324076. The val loss is 8.952265308550476 with std:18.538654250396608.\n",
      "Evaluating for {'degree': 3} ...\n",
      "The training loss is 6.704435190596883 with std:13.04269278065948. The val loss is 15.330671563605605 with std:59.19086488546753.\n",
      "The training loss is 8.59812318878593 with std:19.209738182676706. The val loss is 7.992187275870191 with std:12.953014962963975.\n",
      "The training loss is 8.295108705603784 with std:13.781580827324076. The val loss is 8.952265308550476 with std:18.538654250396608.\n",
      "Evaluating for {'degree': 4} ...\n",
      "The training loss is 6.704435190596883 with std:13.04269278065948. The val loss is 15.330671563605605 with std:59.19086488546753.\n",
      "The training loss is 8.59812318878593 with std:19.209738182676706. The val loss is 7.992187275870191 with std:12.953014962963975.\n",
      "The training loss is 8.295108705603784 with std:13.781580827324076. The val loss is 8.952265308550476 with std:18.538654250396608.\n",
      "Evaluating for {'degree': 5} ...\n",
      "The training loss is 6.704435190596883 with std:13.04269278065948. The val loss is 15.330671563605605 with std:59.19086488546753.\n",
      "The training loss is 8.59812318878593 with std:19.209738182676706. The val loss is 7.992187275870191 with std:12.953014962963975.\n",
      "The training loss is 8.295108705603784 with std:13.781580827324076. The val loss is 8.952265308550476 with std:18.538654250396608.\n",
      "Evaluating for {'degree': 6} ...\n",
      "The training loss is 6.704435190596883 with std:13.04269278065948. The val loss is 15.330671563605605 with std:59.19086488546753.\n",
      "The training loss is 8.59812318878593 with std:19.209738182676706. The val loss is 7.992187275870191 with std:12.953014962963975.\n",
      "The training loss is 8.295108705603784 with std:13.781580827324076. The val loss is 8.952265308550476 with std:18.538654250396608.\n",
      "Evaluating for {'degree': 7} ...\n",
      "The training loss is 6.704435190596883 with std:13.04269278065948. The val loss is 15.330671563605605 with std:59.19086488546753.\n",
      "The training loss is 8.59812318878593 with std:19.209738182676706. The val loss is 7.992187275870191 with std:12.953014962963975.\n",
      "The training loss is 8.295108705603784 with std:13.781580827324076. The val loss is 8.952265308550476 with std:18.538654250396608.\n",
      "Evaluating for {'degree': 8} ...\n",
      "The training loss is 6.704435190596883 with std:13.04269278065948. The val loss is 15.330671563605605 with std:59.19086488546753.\n",
      "The training loss is 8.59812318878593 with std:19.209738182676706. The val loss is 7.992187275870191 with std:12.953014962963975.\n",
      "The training loss is 8.295108705603784 with std:13.781580827324076. The val loss is 8.952265308550476 with std:18.538654250396608.\n",
      "Evaluating for {'degree': 9} ...\n",
      "The training loss is 6.704435190596883 with std:13.04269278065948. The val loss is 15.330671563605605 with std:59.19086488546753.\n",
      "The training loss is 8.59812318878593 with std:19.209738182676706. The val loss is 7.992187275870191 with std:12.953014962963975.\n",
      "The training loss is 8.295108705603784 with std:13.781580827324076. The val loss is 8.952265308550476 with std:18.538654250396608.\n",
      "Evaluating for {'degree': 10} ...\n",
      "The training loss is 6.704435190596883 with std:13.04269278065948. The val loss is 15.330671563605605 with std:59.19086488546753.\n",
      "The training loss is 8.59812318878593 with std:19.209738182676706. The val loss is 7.992187275870191 with std:12.953014962963975.\n",
      "The training loss is 8.295108705603784 with std:13.781580827324076. The val loss is 8.952265308550476 with std:18.538654250396608.\n",
      "Evaluating for {'degree': 11} ...\n",
      "The training loss is 6.704435190596883 with std:13.04269278065948. The val loss is 15.330671563605605 with std:59.19086488546753.\n",
      "The training loss is 8.59812318878593 with std:19.209738182676706. The val loss is 7.992187275870191 with std:12.953014962963975.\n",
      "The training loss is 8.295108705603784 with std:13.781580827324076. The val loss is 8.952265308550476 with std:18.538654250396608.\n",
      "Evaluating for {'degree': 12} ...\n",
      "The training loss is 6.704435190596883 with std:13.04269278065948. The val loss is 15.330671563605605 with std:59.19086488546753.\n",
      "The training loss is 8.59812318878593 with std:19.209738182676706. The val loss is 7.992187275870191 with std:12.953014962963975.\n",
      "The training loss is 8.295108705603784 with std:13.781580827324076. The val loss is 8.952265308550476 with std:18.538654250396608.\n",
      "Evaluating for {'degree': 13} ...\n",
      "The training loss is 6.704435190596883 with std:13.04269278065948. The val loss is 15.330671563605605 with std:59.19086488546753.\n",
      "The training loss is 8.59812318878593 with std:19.209738182676706. The val loss is 7.992187275870191 with std:12.953014962963975.\n",
      "The training loss is 8.295108705603784 with std:13.781580827324076. The val loss is 8.952265308550476 with std:18.538654250396608.\n",
      "Evaluating for {'degree': 14} ...\n",
      "The training loss is 6.704435190596883 with std:13.04269278065948. The val loss is 15.330671563605605 with std:59.19086488546753.\n",
      "The training loss is 8.59812318878593 with std:19.209738182676706. The val loss is 7.992187275870191 with std:12.953014962963975.\n",
      "The training loss is 8.295108705603784 with std:13.781580827324076. The val loss is 8.952265308550476 with std:18.538654250396608.\n"
     ]
    }
   ],
   "source": [
    "from helper import grid_search_cv\n",
    "\n",
    "# put the list of degree values to be evaluated\n",
    "search_degree = np.arange(1, 15)\n",
    "params={'degree':search_degree}\n",
    "\n",
    "#call to the grid search function\n",
    "grid_val,grid_val_std = grid_search_cv(params,k_fold,fold_ind,do_cross_validation,X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how the validation score decreases and then increases with degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best val score 10.758374716008758\n",
      "Best val score for degree 1\n",
      "The training loss is 10.399481428585553 with std:18.859858692726075. The test loss is 11.792470346790926 with std:23.838931174861134.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11.792470346790926"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the best validation score\n",
    "best_score = np.min(grid_val)\n",
    "print('Best val score {}'.format(best_score))\n",
    "\n",
    "#get degree which gives best score\n",
    "best_degree = search_degree[np.argmin(grid_val)]\n",
    "print('Best val score for degree {}'.format(best_degree))\n",
    "\n",
    "\n",
    "X_train_poly,mu,std = expand_and_normalize_X(X_train,best_degree)\n",
    "w = get_w_analytical(X_train_poly,y_train)\n",
    "X_test_poly = expand_X(X_test,best_degree)\n",
    "X_test_poly[:,1:] =  (X_test_poly[:,1:]-mu)/std\n",
    "\n",
    "get_loss(w,X_train_poly,y_train,X_test_poly,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Numerical solution for linear regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The linear regression model has an analytical solution, but we can also get the weight parameters $w$ numerically, e.g. via stochastic gradient descent. Please use this approach to complete the function `get_w_numerical` below.\n",
    "\n",
    "- How do these results compare against those of the analytical solution? Explain the differences or similarities!\n",
    "- In which cases may it (not) be preferable to use the numerical approach over the analytical solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_numerical(X_train,y_train,X_test_poly,y_test,epochs,lr):\n",
    "    \"\"\"compute the weight parameters w\"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize the weights\n",
    "    w    = np.random.normal(0, 1e-1, X_train.shape[1])\n",
    "    \n",
    "    # iterate a given number of epochs over the training data\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # iterate over each data point\n",
    "        for idx,x_train in enumerate(X_train):\n",
    "            # update the weights\n",
    "            w += lr * (y_train[idx] - x_train @ w) * x_train\n",
    "            \n",
    "        if epoch % 1000 == 0:\n",
    "            print(f\"Epoch {1000+epoch}/{epochs}\")\n",
    "            get_loss(w, X_train_poly,y_train, X_test_poly,y_test)\n",
    "            \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/15000\n",
      "The training loss is 286.81580832780844 with std:243.17044485526657. The test loss is 337.0476652013005 with std:264.84713293981673.\n",
      "Epoch 2000/15000\n",
      "The training loss is 22.84766279006637 with std:35.478514421422084. The test loss is 26.865950405169773 with std:39.910045212129944.\n",
      "Epoch 3000/15000\n",
      "The training loss is 21.818954259627777 with std:34.545645678758994. The test loss is 25.40541102513715 with std:39.60932177313394.\n",
      "Epoch 4000/15000\n",
      "The training loss is 21.260230591327197 with std:34.084328081747735. The test loss is 24.7531684426726 with std:39.37728416826244.\n",
      "Epoch 5000/15000\n",
      "The training loss is 20.79545370026822 with std:33.565534552771055. The test loss is 24.254568348853645 with std:38.901053834665845.\n",
      "Epoch 6000/15000\n",
      "The training loss is 20.372312091457026 with std:33.025481019120434. The test loss is 23.81837609551769 with std:38.33630476146064.\n",
      "Epoch 7000/15000\n",
      "The training loss is 19.97999867955091 with std:32.4967774162827. The test loss is 23.42310553017692 with std:37.762651916181596.\n",
      "Epoch 8000/15000\n",
      "The training loss is 19.614397889872123 with std:31.991665999793078. The test loss is 23.059876594974856 with std:37.20894162477617.\n",
      "Epoch 9000/15000\n",
      "The training loss is 19.272911629835292 with std:31.513158262210652. The test loss is 22.72345324908971 with std:36.683413100576445.\n",
      "Epoch 10000/15000\n",
      "The training loss is 18.953519367701492 with std:31.060923108308504. The test loss is 22.410215222392637 with std:36.186959609857894.\n",
      "Epoch 11000/15000\n",
      "The training loss is 18.65452034875732 with std:30.633595737812154. The test loss is 22.117452946019792 with std:35.718152987607674.\n",
      "Epoch 12000/15000\n",
      "The training loss is 18.374425202310057 with std:30.22959651665373. The test loss is 21.843035166800625 with std:35.27501988168833.\n",
      "Epoch 13000/15000\n",
      "The training loss is 18.111898659877117 with std:29.84739653418627. The test loss is 21.585225113944645 with std:34.85562069767388.\n",
      "Epoch 14000/15000\n",
      "The training loss is 17.865726130539368 with std:29.48558903003172. The test loss is 21.3425692032422 with std:34.458206515685745.\n",
      "Epoch 15000/15000\n",
      "The training loss is 17.634792891461093 with std:29.14289674543518. The test loss is 21.11382527726897 with std:34.081236111744204.\n"
     ]
    }
   ],
   "source": [
    "# compute w and calculate its goodness\n",
    "X_train_poly,mu,std = expand_and_normalize_X(X_train,best_degree)\n",
    "X_test_poly = expand_X(X_test,best_degree)\n",
    "X_test_poly[:,1:] =  (X_test_poly[:,1:]-mu)/std\n",
    "w_num = get_w_numerical(X_train_poly,y_train,X_test_poly,y_test,15000,8*1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the sklearn implementation of the linear regression model. Please look up the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) to \n",
    "\n",
    "1. instantiate the LinearRegression model\n",
    "2. fit the model to our training data\n",
    "3. evaluate the model on the test data\n",
    "4. and compare the results with our previous outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of sklearn linear regression model on test data:  11.792470347125724\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "\"\"\"\n",
    "Please fill in the required code here\n",
    "\"\"\"\n",
    "    \n",
    "model = LinearRegression()\n",
    "model.fit(X_train_poly, y_train)\n",
    "y_hat = model.predict(X_test_poly)\n",
    "\n",
    "print('MSE of sklearn linear regression model on test data: ' , metrics.mean_squared_error(y_test,y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in previous section, we would like to do feature expansion to fit the non-linearity of the data, but it soon leads to overfitting. There are different ways to tackle this problem, like getting more data, changing the prediction method, regularization, etc. For the task of regression, we'll add a regularization to our training objective to mitigate this problem. Intutively, regularization restricts the domain from which the values of model parameters are taken, which means that we are biasing our model.  \n",
    "\n",
    "In Ridge Regression, we restrict the $l_2$ norm of the coefficients $\\mathbf{w}$. Our loss function looks as following,\n",
    "\\begin{align}\n",
    "L(\\mathbf{w}) &=\\frac{1}{N}(\\mathbf{y} - \\mathbf{X}\\mathbf{w})^2 + \\frac{\\lambda}{N}\\|\\mathbf{w}\\|^2 \\\\\n",
    "\\nabla L(\\mathbf{w}) &= -\\frac{2}{N}\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\mathbf{w}) + 2\\frac{\\lambda}{N}\\mathbf{w}\n",
    "\\end{align}\n",
    "\n",
    "$\\nabla L(\\mathbf{w}) = 0$ for minimum condition, we get\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{w} &= (\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}\n",
    "\\end{align}\n",
    "\n",
    "dimensions are following: $\\mathbf{w}$ is $D\\times1$; $\\mathbf{y}$ is $N\\times1$; $\\mathbf{X}$ is $N\\times D$; $\\mathbf{I}$ is identity matrix of dimension $D \\times D$ .\n",
    "\n",
    "$\\lambda$ is our penality term, also know as weight decay. By varying its value, we can allow biasing in our model.\n",
    "\n",
    "**Question**:\n",
    "When $\\lambda$ is high, our model is more complex or less?\n",
    "\n",
    "\n",
    "**Question**:\n",
    "How will $\\lambda$ affect inverse condition number of $\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I}$ ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_analytical_with_regularization(X_train,y_train,lmda):\n",
    "    \"\"\"compute the weight parameters w with ridge regression\"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\"\n",
    "    #create lambda matrix \n",
    "    lmda_mat = lmda * np.eye(X_train.shape[1])\n",
    "    # compute w via the normal equation\n",
    "    # np.linalg.solve is more stable than np.linalg.inv\n",
    "    w = np.linalg.solve(X_train.T @ X_train + lmda_mat, X_train.T @ y_train)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original data has 12 features.\n",
      "After degree-14 polynomial feature expansion (with bias, without interaction terms) the data has 169 features.\n",
      "The original data X^TX has condition number 2651084.5926421764. \n",
      "The expanded data X^TX has condition number 8.33045532963066e+21.\n",
      "The X^TX+lambda*I with lambda:2 has condition number 2042.117830651229\n"
     ]
    }
   ],
   "source": [
    "# perform polynomial feature expansion\n",
    "d  = 14\n",
    "\n",
    "#normalize the data after expansion\n",
    "X_train_poly,mu_train_poly,std_train_poly = expand_and_normalize_X(X_train,d)\n",
    "X_test_poly  = expand_X(X_test,d)\n",
    "X_test_poly[:,1:]  = (X_test_poly[:,1:]-mu_train_poly)/std_train_poly\n",
    "\n",
    "\n",
    "print(\"The original data has {} features.\".format(X_train.shape[1]))\n",
    "print(\"After degree-{} polynomial feature expansion (with bias, without interaction terms) the data has {} features.\".format(d,X_train_poly.shape[1]))\n",
    "\n",
    "cond_num_before = np.linalg.cond(X_train.T@X_train)\n",
    "cond_num_after = np.linalg.cond(X_train_poly.T@X_train_poly)\n",
    "print(\"The original data X^TX has condition number {}. \\nThe expanded data X^TX has condition number {}.\".format(cond_num_before,cond_num_after))\n",
    "\n",
    "#choose lambda value\n",
    "lmda = 2\n",
    "\n",
    "#write the X^TX+\\lambda*I matrix\n",
    "A = X_train.T @ X_train + lmda * np.eye(np.shape(X_train)[1])\n",
    "cond_num_ridge = np.linalg.cond(A)\n",
    "print(\"The X^TX+lambda*I with lambda:{} has condition number {}\".format(lmda,cond_num_ridge))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the condition number has changed with regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation(CV) is used to choose value of $\\lambda$. As seen in previous exercise, we will use K-fold CV.\n",
    "We will use our training set and create K splits of it to choose best degree and corresponding $\\lambda$ and finally evaluate on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for using kth split as validation set to get accuracy\n",
    "# and k-1 splits to train our model\n",
    "# k = kth fold\n",
    "# k_fold_ind = all the fold indices\n",
    "# X,Y= train data and labels\n",
    "# lmbda= penalty term\n",
    "# degree = degree of polynomial expansion\n",
    "def do_cross_validation_reg(k,k_fold_ind,X,Y,lmda=0,degree=1):\n",
    "    \n",
    "    # use one split as val\n",
    "    val_ind = k_fold_ind[k]\n",
    "    # use k-1 split to train\n",
    "    train_splits = [i for i in range(k_fold_ind.shape[0]) if i is not k]\n",
    "    train_ind = k_fold_ind[train_splits,:].reshape(-1)\n",
    "   \n",
    "    #Get train and val \n",
    "    cv_X_train = X[train_ind,:]\n",
    "    cv_Y_train = Y[train_ind]\n",
    "    cv_X_val = X[val_ind,:]\n",
    "    cv_Y_val = Y[val_ind]\n",
    "    \n",
    "   \n",
    "    #expand and normalize for degree d\n",
    "    cv_X_train_poly,mu,std = expand_and_normalize_X(cv_X_train, degree)\n",
    "\n",
    "    #apply the normalization using statistics (mean, std) computed on train data\n",
    "    cv_X_val_poly = expand_X(cv_X_val,degree)\n",
    "    cv_X_val_poly[:,1:] =  (cv_X_val_poly[:,1:]-mu)/std\n",
    "    \n",
    "    #fit on train set using regularised version\n",
    "    w = get_w_analytical_with_regularization(cv_X_train_poly,cv_Y_train,lmda)\n",
    "    \n",
    "    #get loss for val\n",
    "    loss_test = get_loss(w,cv_X_train_poly,cv_Y_train,cv_X_val_poly,cv_Y_val,val=True)\n",
    "    print(loss_test,lmda,degree)\n",
    "    return loss_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do 3-fold CV. We will use same the training data splits as in non regularised case for fairer comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating for {'degree': 1, 'lmda': 0.01} ...\n",
      "The training loss is 11.61168452788202 with std:21.121989101130804. The val loss is 13.474911823310823 with std:19.998063111156018.\n",
      "13.474911823310823 0.01 1\n",
      "The training loss is 13.005500369267383 with std:22.756968792873707. The val loss is 10.301031695109835 with std:16.131345164502477.\n",
      "10.301031695109835 0.01 1\n",
      "The training loss is 11.129432771600449 with std:16.595628461384234. The val loss is 13.89287350496629 with std:28.515557213878697.\n",
      "13.89287350496629 0.01 1\n",
      "Evaluating for {'degree': 1, 'lmda': 0.013043213867190054} ...\n",
      "The training loss is 11.78665124087988 with std:21.368560255464338. The val loss is 13.695765666362437 with std:20.271568297048407.\n",
      "13.695765666362437 0.013043213867190054 1\n",
      "The training loss is 13.203603529585118 with std:22.99397757941054. The val loss is 10.51880969612448 with std:16.49449487331509.\n",
      "10.51880969612448 0.013043213867190054 1\n",
      "The training loss is 11.330289809850358 with std:16.887216387554474. The val loss is 14.026224595728314 with std:28.59738958956273.\n",
      "14.026224595728314 0.013043213867190054 1\n",
      "Evaluating for {'degree': 1, 'lmda': 0.017012542798525893} ...\n",
      "The training loss is 11.977534336344497 with std:21.64009347580476. The val loss is 13.92724646298209 with std:20.551883140481163.\n",
      "13.92724646298209 0.017012542798525893 1\n",
      "The training loss is 13.41738002304049 with std:23.254897424375528. The val loss is 10.759647206989381 with std:16.889796895479503.\n",
      "10.759647206989381 0.017012542798525893 1\n",
      "The training loss is 11.552812130179788 with std:17.209484713315042. The val loss is 14.169564272992053 with std:28.697460838628242.\n",
      "14.169564272992053 0.017012542798525893 1\n",
      "Evaluating for {'degree': 1, 'lmda': 0.02218982341458972} ...\n",
      "The training loss is 12.185504878592452 with std:21.940049785509395. The val loss is 14.167872938167756 with std:20.840751924980882.\n",
      "14.167872938167756 0.02218982341458972 1\n",
      "The training loss is 13.646383210662783 with std:23.538787160559103. The val loss is 11.023235335370266 with std:17.319581171751814.\n",
      "11.023235335370266 0.02218982341458972 1\n",
      "The training loss is 11.797936732204366 with std:17.56693826232208. The val loss is 14.324356405802662 with std:28.813713420660424.\n",
      "14.324356405802662 0.02218982341458972 1\n",
      "Evaluating for {'degree': 1, 'lmda': 0.028942661247167517} ...\n",
      "The training loss is 12.412223887916479 with std:22.274305304015815. The val loss is 14.418141921124683 with std:21.144849214004473.\n",
      "14.418141921124683 0.028942661247167517 1\n",
      "The training loss is 13.890022854703178 with std:23.845979453650685. The val loss is 11.308746236457305 with std:17.786402497836438.\n",
      "11.308746236457305 0.028942661247167517 1\n",
      "The training loss is 12.065939187635932 with std:17.962995558186716. The val loss is 14.492206292863708 with std:28.947530753865497.\n",
      "14.492206292863708 0.028942661247167517 1\n",
      "Evaluating for {'degree': 1, 'lmda': 0.037750532053243954} ...\n",
      "The training loss is 12.660432915780504 with std:22.651654496138217. The val loss is 14.681926164593799 with std:21.47819679066943.\n",
      "14.681926164593799 0.037750532053243954 1\n",
      "The training loss is 14.148363604522455 with std:24.179512324308895. The val loss is 11.61554439468873 with std:18.29362318340569.\n",
      "11.61554439468873 0.037750532053243954 1\n",
      "The training loss is 12.357339918997205 with std:18.401474863986763. The val loss is 14.675522649671342 with std:29.104794344614515.\n",
      "14.675522649671342 0.037750532053243954 1\n",
      "Evaluating for {'degree': 1, 'lmda': 0.04923882631706739} ...\n",
      "The training loss is 12.934529423353961 with std:23.08397108802874. The val loss is 14.967532127972726 with std:21.863168777275263.\n",
      "14.967532127972726 0.04923882631706739 1\n",
      "The training loss is 14.423171661898477 with std:24.546662153369795. The val loss is 11.94406232919609 with std:18.84600753654606.\n",
      "11.94406232919609 0.04923882631706739 1\n",
      "The training loss is 12.674145322333839 with std:18.8885209026556. The val loss is 14.878532788705556 with std:29.29697940583912.\n",
      "14.878532788705556 0.04923882631706739 1\n",
      "Evaluating for {'degree': 1, 'lmda': 0.0642232542222936} ...\n",
      "The training loss is 13.240872095171818 with std:23.58555207241299. The val loss is 15.287934058585005 with std:22.328820242234567.\n",
      "15.287934058585005 0.0642232542222936 1\n",
      "The training loss is 14.718957725614372 with std:24.959924479011804. The val loss is 12.296626056417093 with std:19.45008116098081.\n",
      "12.296626056417093 0.0642232542222936 1\n",
      "The training loss is 13.020955750714974 with std:19.43395264056277. The val loss is 15.108286884734412 with std:29.54164755658861.\n",
      "15.108286884734412 0.0642232542222936 1\n",
      "Evaluating for {'degree': 1, 'lmda': 0.0837677640068292} ...\n",
      "The training loss is 13.58750727533821 with std:24.171226278835707. The val loss is 15.65972737183278 with std:22.90577424249124.\n",
      "15.65972737183278 0.0837677640068292 1\n",
      "The training loss is 15.043625304062058 with std:25.436639860803087. The val loss is 12.677949872756107 with std:20.113979576798346.\n",
      "12.677949872756107 0.0837677640068292 1\n",
      "The training loss is 13.405387290159448 with std:20.050969885678022. The val loss is 15.375094612311004 with std:29.861460451977045.\n",
      "15.375094612311004 0.0837677640068292 1\n",
      "Evaluating for {'degree': 1, 'lmda': 0.10926008611173785} ...\n",
      "The training loss is 13.983049114355442 with std:24.853143411243185. The val loss is 16.100552557466486 with std:23.61829654289021.\n",
      "16.100552557466486 0.10926008611173785 1\n",
      "The training loss is 15.408237433920238 with std:25.996555049261886. The val loss is 13.094963810961804 with std:20.84648717203626.\n",
      "13.094963810961804 0.10926008611173785 1\n",
      "The training loss is 13.837323072774469 with std:20.753578548421277. The val loss is 15.69179018263752 with std:30.280948994613592.\n",
      "15.69179018263752 0.10926008611173785 1\n",
      "Evaluating for {'degree': 1, 'lmda': 0.14251026703029984} ...\n",
      "The training loss is 14.434655857347034 with std:25.636767309010185. The val loss is 16.625196086319754 with std:24.476098029124696.\n",
      "16.625196086319754 0.14251026703029984 1\n",
      "The training loss is 15.825487316875835 with std:26.657219099938676. The val loss is 13.555690208047961 with std:21.65508268955825.\n",
      "13.555690208047961 0.14251026703029984 1\n",
      "The training loss is 14.326786120992967 with std:21.5520513254507. The val loss is 16.071445738103716 with std:30.82097760534992.\n",
      "16.071445738103716 0.14251026703029984 1\n",
      "Evaluating for {'degree': 1, 'lmda': 0.18587918911465645} ...\n",
      "The training loss is 14.945434603344014 with std:26.51724165300193. The val loss is 17.24120467103373 with std:25.4694409403753.\n",
      "17.24120467103373 0.18587918911465645 1\n",
      "The training loss is 16.306800494956313 with std:27.428235941328246. The val loss is 14.06713877517497 with std:22.543139395338315.\n",
      "14.06713877517497 0.18587918911465645 1\n",
      "The training loss is 14.880719750470798 with std:22.447909252015243. The val loss is 16.5237059878368 with std:31.492086692247533.\n",
      "16.5237059878368 0.18587918911465645 1\n",
      "Evaluating for {'degree': 1, 'lmda': 0.24244620170823283} ...\n",
      "The training loss is 15.512056372224409 with std:27.477569169280898. The val loss is 17.945385111317677 with std:26.56998324617116.\n",
      "17.945385111317677 0.24244620170823283 1\n",
      "The training loss is 16.85860142385259 with std:28.306455859294374. The val loss is 14.632643913131552 with std:23.50691625419232.\n",
      "14.632643913131552 0.24244620170823283 1\n",
      "The training loss is 15.499565179054889 with std:23.43054015122559. The val loss is 17.05067921036817 with std:32.28904989028461.\n",
      "17.05067921036817 0.24244620170823283 1\n",
      "Evaluating for {'degree': 1, 'lmda': 0.31622776601683794} ...\n",
      "The training loss is 16.12360072836837 with std:28.48963868530318. The val loss is 18.7226176611376 with std:27.73696001987228.\n",
      "18.7226176611376 0.31622776601683794 1\n",
      "The training loss is 17.478931916338347 with std:29.274294073611095. The val loss is 15.249556020848079 with std:24.533417570960438.\n",
      "15.249556020848079 0.31622776601683794 1\n",
      "The training loss is 16.174956172555383 with std:24.477060567721065. The val loss is 17.643924249199877 with std:33.18910115038697.\n",
      "17.643924249199877 0.31622776601683794 1\n",
      "Evaluating for {'degree': 1, 'lmda': 0.41246263829013524} ...\n",
      "The training loss is 16.762400061180447 with std:29.518141188912402. The val loss is 19.547757194135713 with std:28.925924827551857.\n",
      "19.547757194135713 0.41246263829013524 1\n",
      "The training loss is 18.155886197743953 with std:30.302154537088708. The val loss is 15.908387153843309 with std:25.600292889223063.\n",
      "15.908387153843309 0.41246263829013524 1\n",
      "The training loss is 16.88973679305921 with std:25.55562851238327. The val loss is 18.284033640109516 with std:34.154969676603045.\n",
      "18.284033640109516 0.41246263829013524 1\n",
      "Evaluating for {'degree': 1, 'lmda': 0.5379838403443686} ...\n",
      "The training loss is 17.406913897866296 with std:30.526344388809814. The val loss is 20.390244821513825 with std:30.096945083700643.\n",
      "20.390244821513825 0.5379838403443686 1\n",
      "The training loss is 18.86885232686381 with std:31.354147444689254. The val loss is 16.594117486798694 with std:26.678453229049243.\n",
      "16.594117486798694 0.5379838403443686 1\n",
      "The training loss is 17.620699919717424 with std:26.63107927724742. The val loss is 18.943417799654696 with std:35.14182972647931.\n",
      "18.943417799654696 0.5379838403443686 1\n",
      "Evaluating for {'degree': 1, 'lmda': 0.701703828670383} ...\n",
      "The training loss is 18.03581446812462 with std:31.482116872010458. The val loss is 21.220010365887173 with std:31.22044800591985.\n",
      "21.220010365887173 0.701703828670383 1\n",
      "The training loss is 19.592417971887244 with std:32.39512148619005. The val loss is 17.289482822840473 with std:27.737071686902087.\n",
      "17.289482822840473 0.701703828670383 1\n",
      "The training loss is 18.343293986673125 with std:27.67120030842103. The val loss is 19.59154890065568 with std:36.10580769385676.\n",
      "19.59154890065568 0.701703828670383 1\n",
      "Evaluating for {'degree': 1, 'lmda': 0.9152473108773893} ...\n",
      "The training loss is 18.63202106453978 with std:32.36274171708834. The val loss is 22.01288955742334 with std:32.280411804136655.\n",
      "22.01288955742334 0.9152473108773893 1\n",
      "The training loss is 20.301662706906107 with std:33.397018232913005. The val loss is 17.97920240900049 with std:28.749636553774472.\n",
      "17.97920240900049 0.9152473108773893 1\n",
      "The training loss is 19.036752602199506 with std:28.652160612131965. The val loss is 20.200952882216697 with std:37.011515918928794.\n",
      "20.200952882216697 0.9152473108773893 1\n",
      "Evaluating for {'degree': 1, 'lmda': 1.1937766417144369} ...\n",
      "The training loss is 19.18561466891604 with std:33.157731930835645. The val loss is 22.754225367350266 with std:33.27541841668552.\n",
      "22.754225367350266 1.1937766417144369 1\n",
      "The training loss is 20.977131151183627 with std:34.343320921164626. The val loss is 18.65381724295712 with std:29.699373786935205.\n",
      "18.65381724295712 1.1937766417144369 1\n",
      "The training loss is 19.688139357689185 with std:29.56219491154269. The val loss is 20.75221749913479 with std:37.83696930973862.\n",
      "20.75221749913479 1.1937766417144369 1\n",
      "Evaluating for {'degree': 1, 'lmda': 1.5570684047537318} ...\n",
      "The training loss is 19.695183989097885 with std:33.86963110527566. The val loss is 23.440193180135967 with std:34.21824985109294.\n",
      "23.440193180135967 1.5570684047537318 1\n",
      "The training loss is 21.608223709712306 with std:35.23127186209054. The val loss is 19.31219588211846 with std:30.58284700264714.\n",
      "19.31219588211846 1.5570684047537318 1\n",
      "The training loss is 20.294521446036697 with std:30.40332050955713. The val loss is 21.237069579578662 with std:38.57546371593531.\n",
      "21.237069579578662 1.5570684047537318 1\n",
      "Evaluating for {'degree': 1, 'lmda': 2.030917620904737} ...\n",
      "The training loss is 20.16777270676218 with std:34.513293873158645. The val loss is 24.07715771510602 with std:35.13454346514843.\n",
      "24.07715771510602 2.030917620904737 1\n",
      "The training loss is 22.1946059684799 with std:36.07216206600771. The val loss is 19.962453699687178 with std:31.411447459900828.\n",
      "19.962453699687178 2.030917620904737 1\n",
      "The training loss is 20.863323548879244 with std:31.191414456575544. The val loss is 21.659522651182385 with std:39.23492986458602.\n",
      "21.659522651182385 2.030917620904737 1\n",
      "Evaluating for {'degree': 1, 'lmda': 2.6489692876105297} ...\n",
      "The training loss is 20.61791837469728 with std:35.11426404360472. The val loss is 24.67973237826036 with std:36.06073655935785.\n",
      "24.67973237826036 2.6489692876105297 1\n",
      "The training loss is 22.745937567069678 with std:36.890216379895. The val loss is 20.62155345830752 with std:32.21116710813412.\n",
      "20.62155345830752 2.6489692876105297 1\n",
      "The training loss is 21.41138411851387 with std:31.95519789406163. The val loss is 22.035634617152695 with std:39.83566607095957.\n",
      "22.035634617152695 2.6489692876105297 1\n",
      "Evaluating for {'degree': 1, 'lmda': 3.4551072945922217} ...\n",
      "The training loss is 21.06627507614871 with std:35.70670557040657. The val loss is 25.268206466597675 with std:37.04126767499219.\n",
      "25.268206466597675 3.4551072945922217 1\n",
      "The training loss is 23.280473847058886 with std:37.72046629356739. The val loss is 21.31399781369309 with std:33.02117652031128.\n",
      "21.31399781369309 3.4551072945922217 1\n",
      "The training loss is 21.963226187132047 with std:32.73450298561462. The val loss is 22.392375147758372 with std:40.407278575419895.\n",
      "22.392375147758372 3.4551072945922217 1\n",
      "Evaluating for {'degree': 1, 'lmda': 4.506570337745478} ...\n",
      "The training loss is 21.538152028190716 with std:36.331112945753304. The val loss is 25.86586604816517 with std:38.12494525774103.\n",
      "25.86586604816517 4.506570337745478 1\n",
      "The training loss is 23.822982878467133 with std:38.60573186454591. The val loss is 22.069879116406334 with std:33.89142761931646.\n",
      "22.069879116406334 4.506570337745478 1\n",
      "The training loss is 22.54875976309036 with std:33.57781170663638. The val loss is 22.765665927709616 with std:40.98536517805252.\n",
      "22.765665927709616 4.506570337745478 1\n",
      "Evaluating for {'degree': 1, 'lmda': 5.878016072274912} ...\n",
      "The training loss is 22.062127571605057 with std:37.03191684712971. The val loss is 26.496651353684566 with std:39.360686994728276.\n",
      "26.496651353684566 5.878016072274912 1\n",
      "The training loss is 24.402199073246734 with std:39.59270111870329. The val loss is 22.922346935147754 with std:34.87914178155004.\n",
      "22.922346935147754 5.878016072274912 1\n",
      "The training loss is 23.20035695317825 with std:34.538788323500846. The val loss is 23.197164479260767 with std:41.60817948992648.\n",
      "23.197164479260767 5.878016072274912 1\n",
      "Evaluating for {'degree': 1, 'lmda': 7.666822074546214} ...\n",
      "The training loss is 22.66873655055406 with std:37.85512559577298. The val loss is 27.18355488009583 with std:40.793347550562025.\n",
      "27.18355488009583 7.666822074546214 1\n",
      "The training loss is 25.047908340170455 with std:40.727281862877895. The val loss is 23.90447505422159 with std:36.04397806328118.\n",
      "23.90447505422159 7.666822074546214 1\n",
      "The training loss is 23.949229412623716 with std:35.67165862870037. The val loss is 23.72917673913097 with std:42.31326959011496.\n",
      "23.72917673913097 7.666822074546214 1\n",
      "Evaluating for {'degree': 1, 'lmda': 10.0} ...\n",
      "The training loss is 23.389023512545638 with std:38.84614708008146. The val loss is 27.94799650307352 with std:42.46063924361806.\n",
      "27.94799650307352 10.0 1\n",
      "The training loss is 25.787787398239345 with std:42.0498364172588. The val loss is 25.045616453425385 with std:37.44202803335456.\n",
      "25.045616453425385 10.0 1\n",
      "The training loss is 24.821370728612568 with std:37.02582321675515. The val loss is 24.397498517392624 with std:43.13383686616617.\n",
      "24.397498517392624 10.0 1\n",
      "Evaluating for {'degree': 2, 'lmda': 0.01} ...\n",
      "The training loss is 11.455571057566269 with std:20.131707840788213. The val loss is 13.141658410468775 with std:18.86930420909922.\n",
      "13.141658410468775 0.01 2\n",
      "The training loss is 12.541419607611765 with std:20.46867849011388. The val loss is 10.394451823118706 with std:16.001149947679846.\n",
      "10.394451823118706 0.01 2\n",
      "The training loss is 10.74939103513874 with std:15.24507231580167. The val loss is 13.861577939578607 with std:26.501007211385225.\n",
      "13.861577939578607 0.01 2\n",
      "Evaluating for {'degree': 2, 'lmda': 0.013043213867190054} ...\n",
      "The training loss is 11.685719506935733 with std:20.46606885573361. The val loss is 13.458199060512689 with std:19.273130649562173.\n",
      "13.458199060512689 0.013043213867190054 2\n",
      "The training loss is 12.7993425425168 with std:20.819537157993203. The val loss is 10.610377204549502 with std:16.404979762694808.\n",
      "10.610377204549502 0.013043213867190054 2\n",
      "The training loss is 10.99904391017912 with std:15.604180651014161. The val loss is 14.05762275619367 with std:26.68755495010178.\n",
      "14.05762275619367 0.013043213867190054 2\n",
      "Evaluating for {'degree': 2, 'lmda': 0.017012542798525893} ...\n",
      "The training loss is 11.938413050296132 with std:20.84389267265143. The val loss is 13.803277557366672 with std:19.718448233729323.\n",
      "13.803277557366672 0.017012542798525893 2\n",
      "The training loss is 13.081776696464665 with std:21.20934456990405. The val loss is 10.847396732084395 with std:16.85083132591824.\n",
      "10.847396732084395 0.017012542798525893 2\n",
      "The training loss is 11.278855127231498 with std:16.01578518131264. The val loss is 14.274813193018671 with std:26.901962879246557.\n",
      "14.274813193018671 0.017012542798525893 2\n",
      "Evaluating for {'degree': 2, 'lmda': 0.02218982341458972} ...\n",
      "The training loss is 12.218568451885007 with std:21.278608902082393. The val loss is 14.183207128544241 with std:20.225305515998418.\n",
      "14.183207128544241 0.02218982341458972 2\n",
      "The training loss is 13.3923336552586 with std:21.649325172887295. The val loss is 11.10901567728102 with std:17.347149937969032.\n",
      "11.10901567728102 0.02218982341458972 2\n",
      "The training loss is 11.59139254371102 with std:16.489097665719953. The val loss is 14.517836802702437 with std:27.15575338159381.\n",
      "14.517836802702437 0.02218982341458972 2\n",
      "Evaluating for {'degree': 2, 'lmda': 0.028942661247167517} ...\n",
      "The training loss is 12.532926797803285 with std:21.786198985945965. The val loss is 14.607274561117896 with std:20.818096947872373.\n",
      "14.607274561117896 0.028942661247167517 2\n",
      "The training loss is 13.736435023706743 with std:22.15452179073781. The val loss is 11.400977421216771 with std:17.904832121500185.\n",
      "11.400977421216771 0.028942661247167517 2\n",
      "The training loss is 11.94066728916934 with std:17.035429575241505. The val loss is 14.793074988837358 with std:27.466065485131118.\n",
      "14.793074988837358 0.028942661247167517 2\n",
      "Evaluating for {'degree': 2, 'lmda': 0.037750532053243954} ...\n",
      "The training loss is 12.889949107476399 with std:22.38357491917037. The val loss is 15.086920084910531 with std:21.520359509541958.\n",
      "15.086920084910531 0.037750532053243954 2\n",
      "The training loss is 14.12183162897499 with std:22.74316241324317. The val loss is 11.731584930549761 with std:18.536897282276826.\n",
      "11.731584930549761 0.037750532053243954 2\n",
      "The training loss is 12.332944819499863 with std:17.66823721699269. The val loss is 15.109156851030367 with std:27.855269576809185.\n",
      "15.109156851030367 0.037750532053243954 2\n",
      "Evaluating for {'degree': 2, 'lmda': 0.04923882631706739} ...\n",
      "The training loss is 13.298808400082708 with std:23.08539894477207. The val loss is 15.633722941077506 with std:22.34744540500344.\n",
      "15.633722941077506 0.04923882631706739 2\n",
      "The training loss is 14.55831720771549 with std:23.434097547594728. The val loss is 12.111122628563736 with std:19.256903474885473.\n",
      "12.111122628563736 0.04923882631706739 2\n",
      "The training loss is 12.77661207085761 with std:18.4010776197013. The val loss is 15.476590488587064 with std:28.348314471817.\n",
      "15.476590488587064 0.04923882631706739 2\n",
      "Evaluating for {'degree': 2, 'lmda': 0.0642232542222936} ...\n",
      "The training loss is 13.767468555207614 with std:23.899895244139394. The val loss is 16.25652636423467 with std:23.30008187985886.\n",
      "16.25652636423467 0.0642232542222936 2\n",
      "The training loss is 15.056341963452228 with std:24.242363078430998. The val loss is 12.55028603013173 with std:20.07604199175023.\n",
      "12.55028603013173 0.0642232542222936 2\n",
      "The training loss is 13.280708429923342 with std:19.243446418610933. The val loss is 15.90606566945318 with std:28.96758180459643.\n",
      "15.90606566945318 0.0642232542222936 2\n",
      "Evaluating for {'degree': 2, 'lmda': 0.0837677640068292} ...\n",
      "The training loss is 14.300200697686297 with std:24.82494312194333. The val loss is 16.95839437557398 with std:24.36215769549538.\n",
      "16.95839437557398 0.0837677640068292 2\n",
      "The training loss is 15.624529469460441 with std:25.174034233036863. The val loss is 13.05780690431819 with std:20.999372115467597.\n",
      "13.05780690431819 0.0837677640068292 2\n",
      "The training loss is 13.852235276319702 with std:20.195889885740716. The val loss is 16.40549113935087 with std:29.72630269013708.\n",
      "16.40549113935087 0.0837677640068292 2\n",
      "Evaluating for {'degree': 2, 'lmda': 0.10926008611173785} ...\n",
      "The training loss is 14.895259837598395 with std:25.846013267762874. The val loss is 17.734284726435106 with std:25.504093488345312.\n",
      "17.734284726435106 0.10926008611173785 2\n",
      "The training loss is 16.266580905172933 with std:26.222431243898964. The val loss is 13.63779589383542 with std:22.022203086588963.\n",
      "13.63779589383542 0.10926008611173785 2\n",
      "The training loss is 14.492992712403684 with std:21.24670202131508. The val loss is 16.976487196258855 with std:30.622848905807672.\n",
      "16.976487196258855 0.10926008611173785 2\n",
      "Evaluating for {'degree': 2, 'lmda': 0.14251026703029984} ...\n",
      "The training loss is 15.54367197971495 with std:26.93702111329729. The val loss is 18.57031933263187 with std:26.690122583156302.\n",
      "18.57031933263187 0.14251026703029984 2\n",
      "The training loss is 16.978569863924406 with std:27.367555872610634. The val loss is 14.287644668940011 with std:23.127952705664597.\n",
      "14.287644668940011 0.14251026703029984 2\n",
      "The training loss is 15.197174116633674 with std:22.372044394145483. The val loss is 17.611619116508745 with std:31.638452141768127.\n",
      "17.611619116508745 0.14251026703029984 2\n",
      "Evaluating for {'degree': 2, 'lmda': 0.18587918911465645} ...\n",
      "The training loss is 16.229933190983342 with std:28.064108472173416. The val loss is 19.44526468174213 with std:27.88608795052245.\n",
      "19.44526468174213 0.18587918911465645 2\n",
      "The training loss is 17.747846077151127 with std:28.57921988579462. The val loss is 14.997415849453692 with std:24.28863396586912.\n",
      "14.997415849453692 0.18587918911465645 2\n",
      "The training loss is 15.9509417955456 with std:23.539667021214733. The val loss is 18.29373597782735 with std:32.739647432676875.\n",
      "18.29373597782735 0.18587918911465645 2\n",
      "Evaluating for {'degree': 2, 'lmda': 0.24244620170823283} ...\n",
      "The training loss is 16.934777809456563 with std:29.191305399677276. The val loss is 20.334208066580775 with std:29.065013028604042.\n",
      "20.334208066580775 0.24244620170823283 2\n",
      "The training loss is 18.554364074029174 with std:29.822625376491384. The val loss is 15.75127098609726 with std:25.468344559739137.\n",
      "15.75127098609726 0.24244620170823283 2\n",
      "The training loss is 16.73454967623813 with std:24.71480066675629. The val loss is 18.99814025304812 with std:33.88467160521834.\n",
      "18.99814025304812 0.24244620170823283 2\n",
      "Evaluating for {'degree': 2, 'lmda': 0.31622776601683794} ...\n",
      "The training loss is 17.63926370949964 with std:30.286476615220714. The val loss is 21.213550954899077 with std:30.20965524077119.\n",
      "21.213550954899077 0.31622776601683794 2\n",
      "The training loss is 19.37429961876302 with std:31.064361289649447. The val loss is 16.53069357971951 with std:26.629034609745034.\n",
      "16.53069357971951 0.31622776601683794 2\n",
      "The training loss is 17.52651717642624 with std:25.86623805463498. The val loss is 19.69717615233489 with std:35.03150783523442.\n",
      "19.69717615233489 0.31622776601683794 2\n",
      "Evaluating for {'degree': 2, 'lmda': 0.41246263829013524} ...\n",
      "The training loss is 18.328771990137135 with std:31.32603319706489. The val loss is 22.06577545961938 with std:31.31274188377097.\n",
      "22.06577545961938 0.41246263829013524 2\n",
      "The training loss is 20.18482181282069 with std:32.277164865934516. The val loss is 17.31846780808521 with std:27.73696253106819.\n",
      "17.31846780808521 0.41246263829013524 2\n",
      "The training loss is 18.308463780207248 with std:26.97106040309434. The val loss is 20.365807589565726 with std:36.14503392197354.\n",
      "20.365807589565726 0.41246263829013524 2\n",
      "Evaluating for {'degree': 2, 'lmda': 0.5379838403443686} ...\n",
      "The training loss is 18.99553326082503 with std:32.29741179266715. The val loss is 22.882416022487348 with std:32.37585971333699.\n",
      "22.882416022487348 0.5379838403443686 2\n",
      "The training loss is 20.968427934706526 with std:33.442709565383396. The val loss is 18.10205341073273 with std:28.768074565592514.\n",
      "18.10205341073273 0.5379838403443686 2\n",
      "The training loss is 19.06897276242137 with std:28.01723807601805. The val loss is 20.986423611153967 with std:37.20157009136076.\n",
      "20.986423611153967 0.5379838403443686 2\n",
      "Evaluating for {'degree': 2, 'lmda': 0.701703828670383} ...\n",
      "The training loss is 19.6389265113602 with std:33.19904637861611. The val loss is 23.664321611790733 with std:33.407508190191514.\n",
      "23.664321611790733 0.701703828670383 2\n",
      "The training loss is 21.7155198630771 with std:34.55241636029164. The val loss is 18.875309825805374 with std:29.711082105796386.\n",
      "18.875309825805374 0.701703828670383 2\n",
      "The training loss is 19.805311256400405 with std:29.003978097078914. The val loss is 21.551542935242836 with std:38.19031365174312.\n",
      "21.551542935242836 0.701703828670383 2\n",
      "Evaluating for {'degree': 2, 'lmda': 0.9152473108773893} ...\n",
      "The training loss is 20.26365252733566 with std:34.038244814745276. The val loss is 24.4192514132994 with std:34.42045014301023.\n",
      "24.4192514132994 0.9152473108773893 2\n",
      "The training loss is 22.424650816105633 with std:35.606638898783416. The val loss is 19.638225882189264 with std:30.567875275878897.\n",
      "19.638225882189264 0.9152473108773893 2\n",
      "The training loss is 20.522647599129638 with std:29.940115456799447. The val loss is 22.063916606882177 with std:39.1121279773413.\n",
      "22.063916606882177 0.9152473108773893 2\n",
      "Evaluating for {'degree': 2, 'lmda': 1.1937766417144369} ...\n",
      "The training loss is 20.876572726317438 with std:34.827833865948655. The val loss is 25.15768466226761 with std:35.428544655896424.\n",
      "25.15768466226761 1.1937766417144369 2\n",
      "The training loss is 23.100674669039144 with std:36.612695661502414. The val loss is 20.39501745072443 with std:31.351640950589044.\n",
      "20.39501745072443 1.1937766417144369 2\n",
      "The training loss is 21.231178129085023 with std:30.841097718382727. The val loss is 22.534306248340524 with std:39.97668798799016.\n",
      "22.534306248340524 1.1937766417144369 2\n",
      "Evaluating for {'degree': 2, 'lmda': 1.5570684047537318} ...\n",
      "The training loss is 21.48334888175109 with std:35.58257241198451. The val loss is 25.888218444514965 with std:36.443564272451546.\n",
      "25.888218444514965 1.5570684047537318 2\n",
      "The training loss is 23.751594844483467 with std:37.58226958900956. The val loss is 21.15140635541545 with std:32.08343347215359.\n",
      "21.15140635541545 1.5570684047537318 2\n",
      "The training loss is 21.942105002924375 with std:31.725237915711638. The val loss is 22.97774572472667 with std:40.79906688171462.\n",
      "22.97774572472667 1.5570684047537318 2\n",
      "Evaluating for {'degree': 2, 'lmda': 2.030917620904737} ...\n",
      "The training loss is 22.08600446918804 with std:36.31622326666155. The val loss is 26.614012382423525 with std:37.472793940111266.\n",
      "26.614012382423525 2.030917620904737 2\n",
      "The training loss is 24.38516256645577 with std:38.52871910860391. The val loss is 21.91203526030417 with std:32.78806351543881.\n",
      "21.91203526030417 2.030917620904737 2\n",
      "The training loss is 22.663673724489602 with std:32.60996331319539. The val loss is 23.409402686577216 with std:41.59661394445278.\n",
      "23.409402686577216 2.030917620904737 2\n",
      "Evaluating for {'degree': 2, 'lmda': 2.6489692876105297} ...\n",
      "The training loss is 22.682194887798357 with std:37.03993984539544. The val loss is 27.331397248240624 with std:38.518264507085995.\n",
      "27.331397248240624 2.6489692876105297 2\n",
      "The training loss is 25.006261090670563 with std:39.46486030734207. The val loss is 22.678861762380976 with std:33.49021907943484.\n",
      "22.678861762380976 2.6489692876105297 2\n",
      "The training loss is 23.398484323545397 with std:33.508830271613675. The val loss is 23.841275607017526 with std:42.3866423849863.\n",
      "23.841275607017526 2.6489692876105297 2\n",
      "Evaluating for {'degree': 2, 'lmda': 3.4551072945922217} ...\n",
      "The training loss is 23.266406300950774 with std:37.76230945955452. The val loss is 28.03104294823411 with std:39.57814161520254.\n",
      "28.03104294823411 3.4551072945922217 2\n",
      "The training loss is 25.615868747514014 with std:40.40176748166561. The val loss is 23.451066684649255 with std:34.211759006977594.\n",
      "23.451066684649255 3.4551072945922217 2\n",
      "The training loss is 24.14297681176116 with std:34.430085258920734. The val loss is 24.280797534092954 with std:43.18519757207311.\n",
      "24.280797534092954 3.4551072945922217 2\n",
      "Evaluating for {'degree': 2, 'lmda': 4.506570337745478} ...\n",
      "The training loss is 23.83267601396958 with std:38.4909766614521. The val loss is 28.701203096378215 with std:40.650085508323876.\n",
      "28.701203096378215 4.506570337745478 2\n",
      "The training loss is 26.21192339205012 with std:41.34898150954971. The val loss is 24.22654351107803 with std:34.9709448420028.\n",
      "24.22654351107803 4.506570337745478 2\n",
      "The training loss is 24.88933434544033 with std:35.37735494251197. The val loss is 24.731855633894977 with std:44.00699557037121.\n",
      "24.731855633894977 4.506570337745478 2\n",
      "Evaluating for {'degree': 2, 'lmda': 5.878016072274912} ...\n",
      "The training loss is 24.377984472962865 with std:39.235300924762285. The val loss is 29.331901385512833 with std:41.735630718547924.\n",
      "29.331901385512833 5.878016072274912 2\n",
      "The training loss is 26.791790996888846 with std:42.31609785469448. The val loss is 25.004501377409987 with std:35.78381798971412.\n",
      "25.004501377409987 5.878016072274912 2\n",
      "The training loss is 25.629203574770646 with std:36.35247852471508. The val loss is 25.197860082172305 with std:44.866347570629756.\n",
      "25.197860082172305 5.878016072274912 2\n",
      "Evaluating for {'degree': 2, 'lmda': 7.666822074546214} ...\n",
      "The training loss is 24.905383844908165 with std:40.009178050127225. The val loss is 29.918807219214028 with std:42.84426953437472.\n",
      "29.918807219214028 7.666822074546214 2\n",
      "The training loss is 27.35549542423873 with std:43.315137269429634. The val loss is 25.7883074901709 with std:36.667115563975116.\n",
      "25.7883074901709 7.666822074546214 2\n",
      "The training loss is 26.357951720170064 with std:37.35969464743955. The val loss is 25.685645116876337 with std:45.77846894634333.\n",
      "25.685645116876337 7.666822074546214 2\n",
      "Evaluating for {'degree': 2, 'lmda': 10.0} ...\n",
      "The training loss is 25.426163662921255 with std:40.83315463195947. The val loss is 30.465947081647517 with std:43.99617187980277.\n",
      "30.465947081647517 10.0 2\n",
      "The training loss is 27.908647115429293 with std:44.362723468764486. The val loss is 26.587606532777983 with std:37.64149224953141.\n",
      "26.587606532777983 10.0 2\n",
      "The training loss is 27.077993955706145 with std:38.409831605774485. The val loss is 26.20853748172992 with std:46.76023407561975.\n",
      "26.20853748172992 10.0 2\n",
      "Evaluating for {'degree': 3, 'lmda': 0.01} ...\n",
      "The training loss is 13.926099655498883 with std:24.014911426570325. The val loss is 16.591113472294893 with std:23.72931310015784.\n",
      "16.591113472294893 0.01 3\n",
      "The training loss is 14.818711942543514 with std:23.974065306357996. The val loss is 12.65010981796846 with std:20.544532827039145.\n",
      "12.65010981796846 0.01 3\n",
      "The training loss is 13.084308402579248 with std:18.989877939058236. The val loss is 15.891260360537588 with std:29.491222330453457.\n",
      "15.891260360537588 0.01 3\n",
      "Evaluating for {'degree': 3, 'lmda': 0.013043213867190054} ...\n",
      "The training loss is 14.543357696177749 with std:25.01484781326375. The val loss is 17.465484757450298 with std:25.06060143610224.\n",
      "17.465484757450298 0.013043213867190054 3\n",
      "The training loss is 15.500635330753802 with std:25.082915166576075. The val loss is 13.215891822721499 with std:21.476147205958878.\n",
      "13.215891822721499 0.013043213867190054 3\n",
      "The training loss is 13.766531260379239 with std:20.09408639744956. The val loss is 16.452326731428737 with std:30.28710951271465.\n",
      "16.452326731428737 0.013043213867190054 3\n",
      "Evaluating for {'degree': 3, 'lmda': 0.017012542798525893} ...\n",
      "The training loss is 15.218704890878898 with std:26.09645948135023. The val loss is 18.407626408529303 with std:26.440210000036835.\n",
      "18.407626408529303 0.017012542798525893 3\n",
      "The training loss is 16.262038780374166 with std:26.304958320317844. The val loss is 13.854297720867159 with std:22.507399423631906.\n",
      "13.854297720867159 0.017012542798525893 3\n",
      "The training loss is 14.526121897217012 with std:21.297123915079062. The val loss is 17.09494671237151 with std:31.20451908586078.\n",
      "17.09494671237151 0.017012542798525893 3\n",
      "Evaluating for {'degree': 3, 'lmda': 0.02218982341458972} ...\n",
      "The training loss is 15.936478740846988 with std:27.224498000483. The val loss is 19.39526972387494 with std:27.82153848405283.\n",
      "19.39526972387494 0.02218982341458972 3\n",
      "The training loss is 17.091628867576603 with std:27.60482429629783. The val loss is 14.554983892921262 with std:23.610263648168228.\n",
      "14.554983892921262 0.02218982341458972 3\n",
      "The training loss is 15.349165071664451 with std:22.561015900554864. The val loss is 17.807361438918825 with std:32.21817743523387.\n",
      "17.807361438918825 0.02218982341458972 3\n",
      "Evaluating for {'degree': 3, 'lmda': 0.028942661247167517} ...\n",
      "The training loss is 16.675441869858016 with std:28.35843485143912. The val loss is 20.40101972407444 with std:29.166690786855334.\n",
      "20.40101972407444 0.028942661247167517 3\n",
      "The training loss is 17.969411507312405 with std:28.93899631698894. The val loss is 15.300423153329756 with std:24.74626899440101.\n",
      "15.300423153329756 0.028942661247167517 3\n",
      "The training loss is 16.213274254236786 with std:23.84111396999273. The val loss is 18.567806767419697 with std:33.29036010572219.\n",
      "18.567806767419697 0.028942661247167517 3\n",
      "Evaluating for {'degree': 3, 'lmda': 0.037750532053243954} ...\n",
      "The training loss is 17.4123928806187 with std:29.458910347109185. The val loss is 21.396647486574576 with std:30.45088574174517.\n",
      "21.396647486574576 0.037750532053243954 3\n",
      "The training loss is 18.86924742958056 with std:30.263186316356908. The val loss is 16.068456667353765 with std:25.871688491650577.\n",
      "16.068456667353765 0.037750532053243954 3\n",
      "The training loss is 17.09100028992188 with std:25.09354679343521. The val loss is 19.347537836164896 with std:34.377800558781544.\n",
      "19.347537836164896 0.037750532053243954 3\n",
      "Evaluating for {'degree': 3, 'lmda': 0.04923882631706739} ...\n",
      "The training loss is 18.12646893942282 with std:30.493675614112018. The val loss is 22.357870812707894 with std:31.66336035571988.\n",
      "22.357870812707894 0.04923882631706739 3\n",
      "The training loss is 19.763294039156726 with std:31.539122580112938. The val loss is 16.8362703766696 with std:26.944155014933187.\n",
      "16.8362703766696 0.04923882631706739 3\n",
      "The training loss is 17.954845733961555 with std:26.281909531583754. The val loss is 20.115868059312195 with std:35.43943095682012.\n",
      "20.115868059312195 0.04923882631706739 3\n",
      "Evaluating for {'degree': 3, 'lmda': 0.0642232542222936} ...\n",
      "The training loss is 18.8027249903671 with std:31.44172787535766. The val loss is 23.268110782210975 with std:32.805882391030885.\n",
      "23.268110782210975 0.0642232542222936 3\n",
      "The training loss is 20.62698620918154 with std:32.739362686168015. The val loss is 17.584613560301772 with std:27.929057053517344.\n",
      "17.584613560301772 0.0642232542222936 3\n",
      "The training loss is 18.78232741861328 with std:27.3819446545786. The val loss is 20.845645992933473 with std:36.44280197167154.\n",
      "20.845645992933473 0.0642232542222936 3\n",
      "Evaluating for {'degree': 3, 'lmda': 0.0837677640068292} ...\n",
      "The training loss is 19.433910533760088 with std:32.29488731389618. The val loss is 24.12002366928829 with std:33.889583790922174.\n",
      "24.12002366928829 0.0837677640068292 3\n",
      "The training loss is 21.443019941918795 with std:33.849677031183155. The val loss is 18.300981789984327 with std:28.804236853066747.\n",
      "18.300981789984327 0.0837677640068292 3\n",
      "The training loss is 19.559601193443687 with std:28.383729978411342. The val loss is 21.517540081385448 with std:37.36791419849976.\n",
      "21.517540081385448 0.0837677640068292 3\n",
      "Evaluating for {'degree': 3, 'lmda': 0.10926008611173785} ...\n",
      "The training loss is 20.02011455717674 with std:33.05672200627982. The val loss is 24.91444390259943 with std:34.930306447386606.\n",
      "24.91444390259943 0.10926008611173785 3\n",
      "The training loss is 22.203252569054943 with std:34.8690288450473. The val loss is 18.98092132142434 with std:29.56215257661336.\n",
      "18.98092132142434 0.10926008611173785 3\n",
      "The training loss is 20.282787328600577 with std:29.291341947209023. The val loss is 22.12211156357991 with std:38.20811232402152.\n",
      "22.12211156357991 0.10926008611173785 3\n",
      "Evaluating for {'degree': 3, 'lmda': 0.14251026703029984} ...\n",
      "The training loss is 20.56668350888015 with std:33.73934366252992. The val loss is 25.657280110885075 with std:35.94280433309311.\n",
      "25.657280110885075 0.14251026703029984 3\n",
      "The training loss is 22.908198978362655 with std:35.80731527435913. The val loss is 19.62728320936539 with std:30.209399777393177.\n",
      "19.62728320936539 0.14251026703029984 3\n",
      "The training loss is 20.95690419172765 with std:30.120190583995935. The val loss is 22.659487712960427 with std:38.96833769396393.\n",
      "22.659487712960427 0.14251026703029984 3\n",
      "Evaluating for {'degree': 3, 'lmda': 0.18587918911465645} ...\n",
      "The training loss is 21.08128201412091 with std:34.35906547235034. The val loss is 26.3555437885588 with std:36.93493593664164.\n",
      "26.3555437885588 0.18587918911465645 3\n",
      "The training loss is 23.56452610217812 with std:36.681217234176685. The val loss is 20.247852456050115 with std:30.764021506483207.\n",
      "20.247852456050115 0.18587918911465645 3\n",
      "The training loss is 21.592915956398333 with std:30.892434861684695. The val loss is 23.137100127813294 with std:39.66143639299201.\n",
      "23.137100127813294 0.18587918911465645 3\n",
      "Evaluating for {'degree': 3, 'lmda': 0.24244620170823283} ...\n",
      "The training loss is 21.571116599184503 with std:34.93217004186037. The val loss is 27.013925246058424 with std:37.90374195391485.\n",
      "27.013925246058424 0.24244620170823283 3\n",
      "The training loss is 24.181448790588306 with std:37.50894369194325. The val loss is 20.852163468951304 with std:31.251329252252877.\n",
      "20.852163468951304 0.24244620170823283 3\n",
      "The training loss is 22.203769532946275 with std:31.631250484332043. The val loss is 23.566309468952728 with std:40.30350971541476.\n",
      "23.566309468952728 0.24244620170823283 3\n",
      "Evaluating for {'degree': 3, 'lmda': 0.31622776601683794} ...\n",
      "The training loss is 22.041187219644375 with std:35.47197668390639. The val loss is 27.633101993888758 with std:38.83523948184816.\n",
      "27.633101993888758 0.31622776601683794 3\n",
      "The training loss is 24.767172790355637 with std:38.305202080247554. The val loss is 21.44847581361689 with std:31.69913199684114.\n",
      "21.44847581361689 0.31622776601683794 3\n",
      "The training loss is 22.800505128659747 with std:32.35522927017576. The val loss is 23.958881501332232 with std:40.909507995881874.\n",
      "23.958881501332232 0.31622776601683794 3\n",
      "Evaluating for {'degree': 3, 'lmda': 0.41246263829013524} ...\n",
      "The training loss is 22.493980031567663 with std:35.98791367943621. The val loss is 28.210267135353963 with std:39.70853954739812.\n",
      "28.210267135353963 0.41246263829013524 3\n",
      "The training loss is 25.32646977701861 with std:39.07795771699617. The val loss is 22.041793968101373 with std:32.13338117285797.\n",
      "22.041793968101373 0.41246263829013524 3\n",
      "The training loss is 23.38954450536381 with std:33.07454279858783. The val loss is 24.324258366933716 with std:41.490271275345705.\n",
      "24.324258366933716 0.41246263829013524 3\n",
      "Evaluating for {'degree': 3, 'lmda': 0.5379838403443686} ...\n",
      "The training loss is 22.93038887761071 with std:36.4864778282845. The val loss is 28.741413519020902 with std:40.503028238057915.\n",
      "28.741413519020902 0.5379838403443686 3\n",
      "The training loss is 25.86003812560066 with std:39.828067701548505. The val loss is 22.63341877218864 with std:32.575154994632825.\n",
      "22.63341877218864 0.5379838403443686 3\n",
      "The training loss is 23.97193581568861 with std:33.79023173296705. The val loss is 24.668311030233763 with std:42.05181510641342.\n",
      "24.668311030233763 0.5379838403443686 3\n",
      "Evaluating for {'degree': 3, 'lmda': 0.701703828670383} ...\n",
      "The training loss is 23.351133475444932 with std:36.97317904467053. The val loss is 29.224155994119364 with std:41.20594293587185.\n",
      "29.224155994119364 0.701703828670383 3\n",
      "The training loss is 26.365580347892873 with std:40.55173261737063. The val loss is 23.22189518918456 with std:33.03947543713061.\n",
      "23.22189518918456 0.701703828670383 3\n",
      "The training loss is 24.544615678509356 with std:34.49688732438708. The val loss is 24.993722412445972 with std:42.596849678498586.\n",
      "24.993722412445972 0.701703828670383 3\n",
      "Evaluating for {'degree': 3, 'lmda': 0.9152473108773893} ...\n",
      "The training loss is 23.75780956069289 with std:37.45426613539952. The val loss is 29.65972737535325 with std:41.81754627144143.\n",
      "29.65972737535325 0.9152473108773893 3\n",
      "The training loss is 26.839838401368045 with std:41.24450643414335. The val loss is 23.804662327560415 with std:33.53577493579131.\n",
      "23.804662327560415 0.9152473108773893 3\n",
      "The training loss is 25.102915485361258 with std:35.187524417569335. The val loss is 25.301512369729128 with std:43.12764437392349.\n",
      "25.301512369729128 0.9152473108773893 3\n",
      "Evaluating for {'degree': 3, 'lmda': 1.1937766417144369} ...\n",
      "The training loss is 24.153024532169926 with std:37.93736937659723. The val loss is 30.05329539164416 with std:42.35221178428321.\n",
      "30.05329539164416 1.1937766417144369 3\n",
      "The training loss is 27.2805542852693 with std:41.90507694134362. The val loss is 24.37954485842983 with std:34.06928711234187.\n",
      "24.37954485842983 1.1937766417144369 3\n",
      "The training loss is 25.643071551487854 with std:35.85848741349674. The val loss is 25.592817926082233 with std:43.648884121642844.\n",
      "25.592817926082233 1.1937766417144369 3\n",
      "Evaluating for {'degree': 3, 'lmda': 1.5570684047537318} ...\n",
      "The training loss is 24.53965137895262 with std:38.430945022337134. The val loss is 30.412608847836253 with std:42.83542042362993.\n",
      "30.412608847836253 1.5570684047537318 3\n",
      "The training loss is 27.68759300953854 with std:42.53744008537657. The val loss is 24.94553319352396 with std:34.642570699254996.\n",
      "24.94553319352396 1.5570684047537318 3\n",
      "The training loss is 26.16369172670937 with std:36.51239053882646. The val loss is 25.870122128331943 with std:44.16935743847129.\n",
      "25.870122128331943 1.5570684047537318 3\n",
      "Evaluating for {'degree': 3, 'lmda': 2.030917620904737} ...\n",
      "The training loss is 24.91976536225682 with std:38.94313926256731. The val loss is 30.745761889229822 with std:43.29820371483563.\n",
      "30.745761889229822 2.030917620904737 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 28.063059070585442 with std:43.151084919123534. The val loss is 25.502850701800707 with std:35.25677422209664.\n",
      "25.502850701800707 2.030917620904737 3\n",
      "The training loss is 26.665846701924792 with std:37.15824616400152. The val loss is 26.137618362631464 with std:44.70202407500225.\n",
      "26.137618362631464 2.030917620904737 3\n",
      "Evaluating for {'degree': 3, 'lmda': 2.6489692876105297} ...\n",
      "The training loss is 25.29404402333716 with std:39.481029755538124. The val loss is 31.059226224172797 with std:43.7714267887904.\n",
      "31.059226224172797 2.6489692876105297 3\n",
      "The training loss is 28.410789842695184 with std:43.75976868159715. The val loss is 26.052747079020385 with std:35.91275974904253.\n",
      "26.052747079020385 2.6489692876105297 3\n",
      "The training loss is 27.15225009253266 with std:37.80939274706247. The val loss is 26.400997753406255 with std:45.262828030359394.\n",
      "26.400997753406255 2.6489692876105297 3\n",
      "Evaluating for {'degree': 3, 'lmda': 3.4551072945922217} ...\n",
      "The training loss is 25.662203696907447 with std:40.05099323028669. The val loss is 31.357102903767128 with std:44.28215184043576.\n",
      "31.357102903767128 3.4551072945922217 3\n",
      "The training loss is 28.735854265500315 with std:44.37993996055663. The val loss is 26.597556439261375 with std:36.61244673047843.\n",
      "26.597556439261375 3.4551072945922217 3\n",
      "The training loss is 27.626428743110957 with std:38.48082689581034. The val loss is 26.66730990556338 with std:45.869126295981246.\n",
      "26.66730990556338 3.4551072945922217 3\n",
      "Evaluating for {'degree': 3, 'lmda': 4.506570337745478} ...\n",
      "The training loss is 26.024540327450108 with std:40.66032789304951. The val loss is 31.641921864018702 with std:44.85320279070177.\n",
      "31.641921864018702 4.506570337745478 3\n",
      "The training loss is 29.044565193002555 with std:45.02973444495414. The val loss is 27.14131411048766 with std:37.360580679588885.\n",
      "27.14131411048766 4.506570337745478 3\n",
      "The training loss is 28.092660339715255 with std:39.18760253455281. The val loss is 26.945465397512084 with std:46.53854770221448.\n",
      "26.945465397512084 4.506570337745478 3\n",
      "Evaluating for {'degree': 3, 'lmda': 5.878016072274912} ...\n",
      "The training loss is 26.3841430887831 with std:41.31960952742768. The val loss is 31.916640720382322 with std:45.50556017818635.\n",
      "31.916640720382322 5.878016072274912 3\n",
      "The training loss is 29.345156178417717 with std:45.72890463464971. The val loss is 27.690824471751743 with std:38.16674453447922.\n",
      "27.690824471751743 5.878016072274912 3\n",
      "The training loss is 28.556883771426104 with std:39.94513515574976. The val loss is 27.247454552228238 with std:47.28853966318564.\n",
      "27.247454552228238 5.878016072274912 3\n",
      "Evaluating for {'degree': 3, 'lmda': 7.666822074546214} ...\n",
      "The training loss is 26.74912612703917 with std:42.04495363692639. The val loss is 32.1871384211782 with std:46.262197775110934.\n",
      "32.1871384211782 7.666822074546214 3\n",
      "The training loss is 29.64889689190928 with std:46.49942900807461. The val loss is 28.25675945192008 with std:39.04711648811483.\n",
      "28.25675945192008 7.666822074546214 3\n",
      "The training loss is 29.02812893177939 with std:40.771108157780134. The val loss is 27.589716131156347 with std:48.13618069951913.\n",
      "27.589716131156347 7.666822074546214 3\n",
      "Evaluating for {'degree': 3, 'lmda': 10.0} ...\n",
      "The training loss is 27.13432244650146 with std:42.8595073048247. The val loss is 32.46457859981773 with std:47.151885583630325.\n",
      "32.46457859981773 10.0 3\n",
      "The training loss is 29.971210792782504 with std:47.366238948129904. The val loss is 28.854301757245945 with std:40.0254482788817.\n",
      "28.854301757245945 10.0 3\n",
      "The training loss is 29.519657991160177 with std:41.68794536116289. The val loss is 27.993666393791898 with std:49.09751909965397.\n",
      "27.993666393791898 10.0 3\n",
      "Evaluating for {'degree': 4, 'lmda': 0.01} ...\n",
      "The training loss is 19.973895636088546 with std:32.9839211900445. The val loss is 24.842186486265792 with std:34.78700467298665.\n",
      "24.842186486265792 0.01 4\n",
      "The training loss is 21.75046353946482 with std:34.17566464753786. The val loss is 18.74728804413497 with std:29.616853093703217.\n",
      "18.74728804413497 0.01 4\n",
      "The training loss is 19.74683561951865 with std:28.667050413144775. The val loss is 21.89420623891359 with std:38.130139250677324.\n",
      "21.89420623891359 0.01 4\n",
      "Evaluating for {'degree': 4, 'lmda': 0.013043213867190054} ...\n",
      "The training loss is 20.581872338278398 with std:33.69189313423018. The val loss is 25.781586084671357 with std:36.08835138040639.\n",
      "25.781586084671357 0.013043213867190054 4\n",
      "The training loss is 22.552952060612412 with std:35.273321439650445. The val loss is 19.45016341416069 with std:30.26065583800786.\n",
      "19.45016341416069 0.013043213867190054 4\n",
      "The training loss is 20.532056688195482 with std:29.607854511028982. The val loss is 22.48292889618919 with std:38.922649571625016.\n",
      "22.48292889618919 0.013043213867190054 4\n",
      "Evaluating for {'degree': 4, 'lmda': 0.017012542798525893} ...\n",
      "The training loss is 21.14822046268821 with std:34.325420771283035. The val loss is 26.670214483793153 with std:37.36430595687964.\n",
      "26.670214483793153 0.017012542798525893 4\n",
      "The training loss is 23.30946944962055 with std:36.293462362502865. The val loss is 20.120626562815698 with std:30.80523191643799.\n",
      "20.120626562815698 0.017012542798525893 4\n",
      "The training loss is 21.287142251554926 with std:30.501049430894756. The val loss is 23.018775254216994 with std:39.63288792683335.\n",
      "23.018775254216994 0.017012542798525893 4\n",
      "Evaluating for {'degree': 4, 'lmda': 0.02218982341458972} ...\n",
      "The training loss is 21.676373601261542 with std:34.89828617607257. The val loss is 27.507318415866344 with std:38.60944063994487.\n",
      "27.507318415866344 0.02218982341458972 4\n",
      "The training loss is 24.021764269017204 with std:37.24399515599791. The val loss is 20.760494267144477 with std:31.26900615485439.\n",
      "20.760494267144477 0.02218982341458972 4\n",
      "The training loss is 22.016399125044934 with std:31.360316326336132. The val loss is 23.51019376243543 with std:40.27727349815975.\n",
      "23.51019376243543 0.02218982341458972 4\n",
      "Evaluating for {'degree': 4, 'lmda': 0.028942661247167517} ...\n",
      "The training loss is 22.168193817626747 with std:35.42126639679624. The val loss is 28.289431153775247 with std:39.809603985715725.\n",
      "28.289431153775247 0.028942661247167517 4\n",
      "The training loss is 24.692206128308293 with std:38.13366276513458. The val loss is 21.371139528865957 with std:31.671862941404974.\n",
      "21.371139528865957 0.028942661247167517 4\n",
      "The training loss is 22.72263618502825 with std:32.19639928241899. The val loss is 23.966189109235778 with std:40.87172598591156.\n",
      "23.966189109235778 0.028942661247167517 4\n",
      "Evaluating for {'degree': 4, 'lmda': 0.037750532053243954} ...\n",
      "The training loss is 22.623372129591875 with std:35.90058413777116. The val loss is 29.01001027020343 with std:40.942954924955394.\n",
      "29.01001027020343 0.037750532053243954 4\n",
      "The training loss is 25.32161233900853 with std:38.9683383729604. The val loss is 21.951989014575876 with std:32.031519374600855.\n",
      "21.951989014575876 0.037750532053243954 4\n",
      "The training loss is 23.40500557582073 with std:33.01358520728289. The val loss is 24.39349418202073 with std:41.4279940719297.\n",
      "24.39349418202073 0.037750532053243954 4\n",
      "Evaluating for {'degree': 4, 'lmda': 0.04923882631706739} ...\n",
      "The training loss is 23.040025931209836 with std:36.33830691481897. The val loss is 29.660759752725735 with std:41.98387557910604.\n",
      "29.660759752725735 0.04923882631706739 4\n",
      "The training loss is 25.908325468564076 with std:39.749010172818096. The val loss is 22.50020754108158 with std:32.36127202196981.\n",
      "22.50020754108158 0.04923882631706739 4\n",
      "The training loss is 24.058385701357228 with std:33.8081772789961. The val loss is 24.79505709762743 with std:41.95180777550629.\n",
      "24.79505709762743 0.04923882631706739 4\n",
      "Evaluating for {'degree': 4, 'lmda': 0.0642232542222936} ...\n",
      "The training loss is 23.415984122552086 with std:36.7341228110847. The val loss is 30.233828117100543 with std:42.90816209731934.\n",
      "30.233828117100543 0.0642232542222936 4\n",
      "The training loss is 26.44868336073063 with std:40.47208021360962. The val loss is 23.01156129575934 with std:32.66949343369548.\n",
      "23.01156129575934 0.0642232542222936 4\n",
      "The training loss is 24.674413869297005 with std:34.569539655656435. The val loss is 25.17006546276064 with std:42.44314451306283.\n",
      "25.17006546276064 0.0642232542222936 4\n",
      "Evaluating for {'degree': 4, 'lmda': 0.0837677640068292} ...\n",
      "The training loss is 23.750090528244787 with std:37.0875214409819. The val loss is 30.723857242518942 with std:43.697501572787104.\n",
      "30.723857242518942 0.0837677640068292 4\n",
      "The training loss is 26.938442627933483 with std:41.13154614279852. The val loss is 23.481985178318315 with std:32.960691852084146.\n",
      "23.481985178318315 0.0837677640068292 4\n",
      "The training loss is 25.243673546408026 with std:35.28329279241. The val loss is 25.515195552082474 with std:42.898182080011836.\n",
      "25.515195552082474 0.0837677640068292 4\n",
      "Evaluating for {'degree': 4, 'lmda': 0.10926008611173785} ...\n",
      "The training loss is 24.04304630774424 with std:37.39952661194354. The val loss is 31.12917933504655 with std:44.34193844576746.\n",
      "31.12917933504655 0.10926008611173785 4\n",
      "The training loss is 27.374394371791897 with std:41.72186895890087. The val loss is 23.909152283211075 with std:33.23743408973532.\n",
      "23.909152283211075 0.10926008611173785 4\n",
      "The training loss is 25.758139984607904 with std:35.93536337720623. The val loss is 25.826394775900546 with std:43.311974714765626.\n",
      "25.826394775900546 0.10926008611173785 4\n",
      "Evaluating for {'degree': 4, 'lmda': 0.14251026703029984} ...\n",
      "The training loss is 24.29766877033023 with std:37.67358990805997. The val loss is 31.45202932764116 with std:44.84019487843501.\n",
      "31.45202932764116 0.14251026703029984 4\n",
      "The training loss is 27.755513738514125 with std:42.240307196458474. The val loss is 24.293479482924234 with std:33.50227322137561.\n",
      "24.293479482924234 0.14251026703029984 4\n",
      "The training loss is 26.21303663495269 with std:36.51543293492257. The val loss is 26.1004830855304 with std:43.680842048494625.\n",
      "26.1004830855304 0.14251026703029984 4\n",
      "Evaluating for {'degree': 4, 'lmda': 0.18587918911465645} ...\n",
      "The training loss is 24.518726421309946 with std:37.91571960314112. The val loss is 31.698087965520127 with std:45.19859331578073.\n",
      "31.698087965520127 0.18587918911465645 4\n",
      "The training loss is 28.08337258640941 with std:42.688080207445815. The val loss is 24.63836648644544 with std:33.759068882215324.\n",
      "24.63836648644544 0.18587918911465645 4\n",
      "The training loss is 26.607680487867835 with std:37.01890847120923. The val loss is 26.336150001620194 with std:44.003866015061156.\n",
      "26.336150001620194 0.18587918911465645 4\n",
      "Evaluating for {'degree': 4, 'lmda': 0.24244620170823283} ...\n",
      "The training loss is 24.71259508185442 with std:38.13415307932416. The val loss is 31.875812700531068 with std:45.429594161139526.\n",
      "31.875812700531068 0.24244620170823283 4\n",
      "The training loss is 28.36192545020215 with std:43.070409072925955. The val loss is 24.949801387187755 with std:34.0135092523423.\n",
      "24.949801387187755 0.24244620170823283 4\n",
      "The training loss is 26.945385025961727 with std:37.44736216886647. The val loss is 26.534296123066987 with std:44.283425646254045.\n",
      "26.534296123066987 0.24244620170823283 4\n",
      "Evaluating for {'degree': 4, 'lmda': 0.31622776601683794} ...\n",
      "The training loss is 24.886894435004887 with std:38.33885999455195. The val loss is 31.995884828914036 with std:45.5506956372795.\n",
      "31.995884828914036 0.31622776601683794 4\n",
      "The training loss is 28.596958411533894 with std:43.39587718633096. The val loss is 25.235611883664177 with std:34.2729679632202.\n",
      "25.235611883664177 0.31622776601683794 4\n",
      "The training loss is 27.232776381627794 with std:37.80794370246448. The val loss is 26.69791080161276 with std:44.52505824699231.\n",
      "26.69791080161276 0.31622776601683794 4\n",
      "Evaluating for {'degree': 4, 'lmda': 0.41246263829013524} ...\n",
      "The training loss is 25.050111295449266 with std:38.54100097463454. The val loss is 32.070842460723554 with std:45.583917979337414.\n",
      "32.070842460723554 0.41246263829013524 4\n",
      "The training loss is 28.79546288615561 with std:43.675580979190535. The val loss is 25.504604242742612 with std:34.54594257848617.\n",
      "25.504604242742612 0.41246263829013524 4\n",
      "The training loss is 27.478899571540556 with std:38.112381268180826. The val loss is 26.831730315388352 with std:44.73701933451016.\n",
      "26.831730315388352 0.41246263829013524 4\n",
      "Evaluating for {'degree': 4, 'lmda': 0.5379838403443686} ...\n",
      "The training loss is 25.21109590915946 with std:38.75230122226334. The val loss is 32.11472083453444 with std:45.55556724612558.\n",
      "32.11472083453444 0.5379838403443686 4\n",
      "The training loss is 28.96506770123731 with std:43.922353464415906. The val loss is 25.765714909458463 with std:34.841279289501465.\n",
      "25.765714909458463 0.5379838403443686 4\n",
      "The training loss is 27.694343377654633 with std:38.375977901089165. The val loss is 26.941841626668328 with std:44.92980015643073.\n",
      "26.941841626668328 0.5379838403443686 4\n",
      "Evaluating for {'degree': 4, 'lmda': 0.701703828670383} ...\n",
      "The training loss is 25.378297074266456 with std:38.98423303623976. The val loss is 32.14239374013633 with std:45.49564299494728.\n",
      "32.14239374013633 0.701703828670383 4\n",
      "The training loss is 29.113539490223964 with std:44.1501210862679. The val loss is 26.027203917195465 with std:35.1673105820441.\n",
      "26.027203917195465 0.701703828670383 4\n",
      "The training loss is 27.890426724146128 with std:38.61669406502311. The val loss is 27.035275632317745 with std:45.11567984184632.\n",
      "27.035275632317745 0.701703828670383 4\n",
      "Evaluating for {'degree': 4, 'lmda': 0.9152473108773893} ...\n",
      "The training loss is 25.558710512964065 with std:39.24698498517968. The val loss is 32.16838164537226 with std:45.43627904540629.\n",
      "32.16838164537226 0.9152473108773893 4\n",
      "The training loss is 29.248300776201486 with std:44.373314483678215. The val loss is 26.295896772905653 with std:35.53100916819114.\n",
      "26.295896772905653 0.9152473108773893 4\n",
      "The training loss is 28.078365599728603 with std:38.85416390417594. The val loss is 27.11954327491098 with std:45.308242868528154.\n",
      "27.11954327491098 0.9152473108773893 4\n",
      "Evaluating for {'degree': 4, 'lmda': 1.1937766417144369} ...\n",
      "The training loss is 25.7567310485774 with std:39.548407371451795. The val loss is 32.20518165950328 with std:45.40906896657163.\n",
      "32.20518165950328 1.1937766417144369 4\n",
      "The training loss is 29.375932640214618 with std:44.60624414542086. The val loss is 26.57653366460282 with std:35.93731554702557.\n",
      "26.57653366460282 1.1937766417144369 4\n",
      "The training loss is 28.26833560665082 with std:39.10843829095954. The val loss is 27.202057300183164 with std:45.52174696761613.\n",
      "27.202057300183164 1.1937766417144369 4\n",
      "Evaluating for {'degree': 4, 'lmda': 1.5570684047537318} ...\n",
      "The training loss is 25.973320153411574 with std:39.89335108117951. The val loss is 32.261570816084244 with std:45.44189170940713.\n",
      "32.261570816084244 1.5570684047537318 4\n",
      "The training loss is 29.501711731987502 with std:44.862466935655675. The val loss is 26.871372844333195 with std:36.38888335276908.\n",
      "26.871372844333195 1.5570684047537318 4\n",
      "The training loss is 28.46847479419058 with std:39.39843482444369. The val loss is 27.28947298639468 with std:45.77030969993946.\n",
      "27.28947298639468 1.5570684047537318 4\n",
      "Evaluating for {'degree': 4, 'lmda': 2.030917620904737} ...\n",
      "The training loss is 26.20596057897267 with std:40.28388296260605. The val loss is 32.34158856780645 with std:45.55648713419971.\n",
      "32.34158856780645 2.030917620904737 4\n",
      "The training loss is 29.629334649468944 with std:45.15434228960676. The val loss is 27.180249239663787 with std:36.88651528806585.\n",
      "27.180249239663787 2.030917620904737 4\n",
      "The training loss is 28.684077702441826 with std:39.74043919556485. The val loss is 27.38713922837798 with std:46.06706043410026.\n",
      "27.38713922837798 2.030917620904737 4\n",
      "Evaluating for {'degree': 4, 'lmda': 2.6489692876105297} ...\n",
      "The training loss is 26.449657941260984 with std:40.72064493250192. The val loss is 32.44478358865393 with std:45.768030316066.\n",
      "32.44478358865393 2.6489692876105297 4\n",
      "The training loss is 29.761032366177652 with std:45.49308273080968. The val loss is 27.501238044265758 with std:37.430449844886134.\n",
      "27.501238044265758 2.6489692876105297 4\n",
      "The training loss is 28.917374300360844 with std:40.1473208541651. The val loss is 27.498970241638347 with std:46.42356184129055.\n",
      "27.498970241638347 2.6489692876105297 4\n",
      "Evaluating for {'degree': 4, 'lmda': 3.4551072945922217} ...\n",
      "The training loss is 26.698847209421093 with std:41.20520184598541. The val loss is 32.56783030816145 with std:46.08719731814893.\n",
      "32.56783030816145 3.4551072945922217 4\n",
      "The training loss is 29.898213735444063 with std:45.88952990379548. The val loss is 27.831900758639488 with std:38.02240769802704.\n",
      "27.831900758639488 3.4551072945922217 4\n",
      "The training loss is 29.168212895059156 with std:40.62911234039467. The val loss is 27.627996518817323 with std:46.84979048630867.\n",
      "27.627996518817323 3.4551072945922217 4\n",
      "Evaluating for {'degree': 4, 'lmda': 4.506570337745478} ...\n",
      "The training loss is 26.94972787645268 with std:41.74285830385433. The val loss is 32.70709707748444 with std:46.524185737680526.\n",
      "32.70709707748444 4.506570337745478 4\n",
      "The training loss is 30.042619012351373 with std:46.3556471648125. The val loss is 28.17088208195526 with std:38.66804232479064.\n",
      "28.17088208195526 4.506570337745478 4\n",
      "The training loss is 29.43565081121318 with std:41.19515702490152. The val loss is 27.777593171000895 with std:47.35472968951679.\n",
      "27.777593171000895 4.506570337745478 4\n",
      "Evaluating for {'degree': 4, 'lmda': 5.878016072274912} ...\n",
      "The training loss is 27.20251268790807 with std:42.345366591661474. The val loss is 32.86154179175507 with std:47.093555017616374.\n",
      "32.86154179175507 5.878016072274912 4\n",
      "The training loss is 30.197819629906604 with std:46.90648034272824. The val loss is 28.519526363938784 with std:39.379341564229726.\n",
      "28.519526363938784 5.878016072274912 4\n",
      "The training loss is 29.720086206630295 with std:41.857413078154636. The val loss is 27.95305018502618 with std:47.94732363228467.\n",
      "27.95305018502618 5.878016072274912 4\n",
      "Evaluating for {'degree': 4, 'lmda': 7.666822074546214} ...\n",
      "The training loss is 27.463344854721527 with std:43.033231267915816. The val loss is 33.035530958669035 with std:47.8188991167488.\n",
      "33.035530958669035 7.666822074546214 4\n",
      "The training loss is 30.37088983521117 with std:47.56229804824564. The val loss is 28.883292792563875 with std:40.17669134971673.\n",
      "28.883292792563875 7.666822074546214 4\n",
      "The training loss is 30.0254195785337 with std:42.63416148907469. The val loss is 28.162986994528154 with std:48.63744853574609.\n",
      "28.162986994528154 7.666822074546214 4\n",
      "Evaluating for {'degree': 4, 'lmda': 10.0} ...\n",
      "The training loss is 27.746054200384723 with std:43.837767062985144. The val loss is 33.241654037085596 with std:48.73700392755237.\n",
      "33.241654037085596 10.0 4\n",
      "The training loss is 30.574245402850995 with std:48.350846008620806. The val loss is 29.27305349963875 with std:41.09067977008256.\n",
      "29.27305349963875 10.0 4\n",
      "The training loss is 30.360972546653937 with std:43.553554908877786. The val loss is 28.420308506428135 with std:49.436846939637626.\n",
      "28.420308506428135 10.0 4\n",
      "Evaluating for {'degree': 5, 'lmda': 0.01} ...\n",
      "The training loss is 24.169917546454382 with std:37.594042911306154. The val loss is 31.391855264828276 with std:44.71328534858903.\n",
      "31.391855264828276 0.01 5\n",
      "The training loss is 27.37906437715206 with std:41.77422872975425. The val loss is 23.954733903333338 with std:33.34759953892988.\n",
      "23.954733903333338 0.01 5\n",
      "The training loss is 25.703063786495832 with std:35.977160163068014. The val loss is 25.819108126046928 with std:43.27711447735868.\n",
      "25.819108126046928 0.01 5\n",
      "Evaluating for {'degree': 5, 'lmda': 0.013043213867190054} ...\n",
      "The training loss is 24.445408504893596 with std:37.857461351389496. The val loss is 31.86058991403887 with std:45.41022151783561.\n",
      "31.86058991403887 0.013043213867190054 5\n",
      "The training loss is 27.77486129259392 with std:42.296478091919106. The val loss is 24.353430699053277 with std:33.61014896323519.\n",
      "24.353430699053277 0.013043213867190054 5\n",
      "The training loss is 26.191565819885973 with std:36.556750496660754. The val loss is 26.104112927002593 with std:43.65251863927587.\n",
      "26.104112927002593 0.013043213867190054 5\n",
      "Evaluating for {'degree': 5, 'lmda': 0.017012542798525893} ...\n",
      "The training loss is 24.682421015649265 with std:38.082084913762166. The val loss is 32.25771573805367 with std:45.98879725122367.\n",
      "32.25771573805367 0.017012542798525893 5\n",
      "The training loss is 28.12217206311847 with std:42.747247282569475. The val loss is 24.705327567085135 with std:33.84847929434156.\n",
      "24.705327567085135 0.017012542798525893 5\n",
      "The training loss is 26.631632015328886 with std:37.070843201954716. The val loss is 26.359167979353288 with std:43.98520381421665.\n",
      "26.359167979353288 0.017012542798525893 5\n",
      "Evaluating for {'degree': 5, 'lmda': 0.02218982341458972} ...\n",
      "The training loss is 24.884031423789008 with std:38.271159406339194. The val loss is 32.58493700288293 with std:46.452233781838075.\n",
      "32.58493700288293 0.02218982341458972 5\n",
      "The training loss is 28.42279009661949 with std:43.12973130132308. The val loss is 25.01131915351554 with std:34.06157445114779.\n",
      "25.01131915351554 0.02218982341458972 5\n",
      "The training loss is 27.021532255679567 with std:37.51920725378283. The val loss is 26.5840101227878 with std:44.27584345591433.\n",
      "26.5840101227878 0.02218982341458972 5\n",
      "Evaluating for {'degree': 5, 'lmda': 0.028942661247167517} ...\n",
      "The training loss is 25.054129824202644 with std:38.42877463008242. The val loss is 32.845964405781515 with std:46.80658064960392.\n",
      "32.845964405781515 0.028942661247167517 5\n",
      "The training loss is 28.679976005917826 with std:43.44953298101479. The val loss is 25.274409395839022 with std:34.250285710716284.\n",
      "25.274409395839022 0.028942661247167517 5\n",
      "The training loss is 27.36150444114169 with std:37.90452340720446. The val loss is 26.779373963395038 with std:44.526343793593256.\n",
      "26.779373963395038 0.028942661247167517 5\n",
      "Evaluating for {'degree': 5, 'lmda': 0.037750532053243954} ...\n",
      "The training loss is 25.19698496656679 with std:38.55970958017898. The val loss is 33.045711375275474 with std:47.05975149313901.\n",
      "33.045711375275474 0.037750532053243954 5\n",
      "The training loss is 28.897871236923002 with std:43.71371250863757. The val loss is 25.49918955585574 with std:34.41725573834664.\n",
      "25.49918955585574 0.037750532053243954 5\n",
      "The training loss is 27.653412453423567 with std:38.23129324741403. The val loss is 26.946729211428462 with std:44.73946673517609.\n",
      "26.946729211428462 0.037750532053243954 5\n",
      "Evaluating for {'degree': 5, 'lmda': 0.04923882631706739} ...\n",
      "The training loss is 25.316936469474868 with std:38.669227783934424. The val loss is 33.18955577003588 with std:47.22052929323339.\n",
      "33.18955577003588 0.04923882631706739 5\n",
      "The training loss is 29.08099393574797 with std:43.92997223826138. The val loss is 25.691281989866717 with std:34.56664853494339.\n",
      "25.691281989866717 0.04923882631706739 5\n",
      "The training loss is 27.900457304659792 with std:38.50511041193487. The val loss is 27.088077672158704 with std:44.918576735053236.\n",
      "27.088077672158704 0.04923882631706739 5\n",
      "Evaluating for {'degree': 5, 'lmda': 0.0642232542222936} ...\n",
      "The training loss is 25.418262292308075 with std:38.76289417692544. The val loss is 33.28282903977539 with std:47.297746352934546.\n",
      "33.28282903977539 0.0642232542222936 5\n",
      "The training loss is 29.233884630135286 with std:44.10604644496546. The val loss is 25.85684194478054 with std:34.70378305829483.\n",
      "25.85684194478054 0.0642232542222936 5\n",
      "The training loss is 28.10689756537857 with std:38.73224665659568. The val loss is 27.20581165051208 with std:45.067471192428265.\n",
      "27.20581165051208 0.0642232542222936 5\n",
      "Evaluating for {'degree': 5, 'lmda': 0.0837677640068292} ...\n",
      "The training loss is 25.505214886459864 with std:38.84649138512902. The val loss is 33.33062911899427 with std:47.29990377040166.\n",
      "33.33062911899427 0.0837677640068292 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 29.36091668206508 with std:44.24931998280986. The val loss is 26.00217989916676 with std:34.83475972840845.\n",
      "26.00217989916676 0.0837677640068292 5\n",
      "The training loss is 28.27775625050773 with std:38.919423803408264. The val loss is 27.302608183517865 with std:45.19024564808597.\n",
      "27.302608183517865 0.0837677640068292 5\n",
      "Evaluating for {'degree': 5, 'lmda': 0.10926008611173785} ...\n",
      "The training loss is 25.582175715857364 with std:38.926063703895004. The val loss is 33.33797690016778 with std:47.235396271738715.\n",
      "33.33797690016778 0.10926008611173785 5\n",
      "The training loss is 29.466244226692435 with std:44.36666974059634. The val loss is 26.133521474281736 with std:34.96614739821966.\n",
      "26.133521474281736 0.10926008611173785 5\n",
      "The training loss is 28.41854079543241 with std:39.07369195933543. The val loss is 27.38134550733604 with std:45.29118394991362.\n",
      "27.38134550733604 0.10926008611173785 5\n",
      "Evaluating for {'degree': 5, 'lmda': 0.14251026703029984} ...\n",
      "The training loss is 25.653853568553107 with std:39.008045883265794. The val loss is 33.31026759881143 with std:47.11332857397192.\n",
      "33.31026759881143 0.14251026703029984 5\n",
      "The training loss is 29.553836970977642 with std:44.464491590066324. The val loss is 26.25687513173149 with std:35.10475009626567.\n",
      "26.25687513173149 0.14251026703029984 5\n",
      "The training loss is 28.535024676698722 with std:39.202400764777074. The val loss is 27.445046082601113 with std:45.37469316333987.\n",
      "27.445046082601113 0.14251026703029984 5\n",
      "Evaluating for {'degree': 5, 'lmda': 0.18587918911465645} ...\n",
      "The training loss is 25.725438710425912 with std:39.09937114632526. The val loss is 33.25390151686367 with std:46.94471245071194.\n",
      "33.25390151686367 0.18587918911465645 5\n",
      "The training loss is 29.62755049205726 with std:44.54885243454503. The val loss is 26.377947945040717 with std:35.25741754535121.\n",
      "26.377947945040717 0.18587918911465645 5\n",
      "The training loss is 28.633124037931296 with std:39.313273408602655. The val loss is 27.496856278879996 with std:45.445305816807995.\n",
      "27.496856278879996 0.18587918911465645 5\n",
      "Evaluating for {'degree': 5, 'lmda': 0.24244620170823283} ...\n",
      "The training loss is 25.80261031931859 with std:39.20741462258247. The val loss is 33.17689861925691 with std:46.74364624500261.\n",
      "33.17689861925691 0.24244620170823283 5\n",
      "The training loss is 29.691191408241433 with std:44.62569687967314. The val loss is 26.50204143289109 with std:35.430826366419616.\n",
      "26.50204143289109 0.24244620170823283 5\n",
      "The training loss is 28.71886465512911 with std:39.41456797631366. The val loss is 27.540062681479746 with std:45.50775427492588.\n",
      "27.540062681479746 0.24244620170823283 5\n",
      "Evaluating for {'degree': 5, 'lmda': 0.31622776601683794} ...\n",
      "The training loss is 25.891284377440037 with std:39.339632616967855. The val loss is 33.08921658266213 with std:46.527940408302236.\n",
      "33.08921658266213 0.31622776601683794 5\n",
      "The training loss is 29.748543547779768 with std:44.7010344242591. The val loss is 26.633867740208995 with std:35.631154261537326.\n",
      "26.633867740208995 0.31622776601683794 5\n",
      "The training loss is 28.798388996610278 with std:39.51525649278825. The val loss is 27.57812244060379 with std:45.56709367105054.\n",
      "27.57812244060379 0.31622776601683794 5\n",
      "Evaluating for {'degree': 5, 'lmda': 0.41246263829013524} ...\n",
      "The training loss is 25.997015621803175 with std:39.50283464261462. The val loss is 33.00245960826246 with std:46.31866395014201.\n",
      "33.00245960826246 0.41246263829013524 5\n",
      "The training loss is 29.80331946449699 with std:44.781032044928544. The val loss is 26.77724896187163 with std:35.863604099453454.\n",
      "26.77724896187163 0.41246263829013524 5\n",
      "The training loss is 28.877911677037602 with std:39.62509399006011. The val loss is 27.614657673859192 with std:45.62881868217761.\n",
      "27.614657673859192 0.41246263829013524 5\n",
      "Evaluating for {'degree': 5, 'lmda': 0.5379838403443686} ...\n",
      "The training loss is 26.124071406080553 with std:39.7021922357532. The val loss is 32.92877736136192 with std:46.13837277415459.\n",
      "32.92877736136192 0.5379838403443686 5\n",
      "The training loss is 29.859002815317346 with std:44.87194993432474. The val loss is 26.934704337089087 with std:36.13181308757289.\n",
      "26.934704337089087 0.5379838403443686 5\n",
      "The training loss is 28.963510406584085 with std:39.754422496080764. The val loss is 27.65334490622902 with std:45.69889581216133.\n",
      "27.65334490622902 0.5379838403443686 5\n",
      "Evaluating for {'degree': 5, 'lmda': 0.701703828670383} ...\n",
      "The training loss is 26.27438071001205 with std:39.94030446471188. The val loss is 32.87906255708552 with std:46.00835107289884.\n",
      "32.87906255708552 0.701703828670383 5\n",
      "The training loss is 29.91857149915831 with std:44.97990772473015. The val loss is 27.10699621801191 with std:36.437297170243966.\n",
      "27.10699621801191 0.701703828670383 5\n",
      "The training loss is 29.060665401635994 with std:39.9136043432843. The val loss is 27.69764048283562 with std:45.78363561662787.\n",
      "27.69764048283562 0.701703828670383 5\n",
      "Evaluating for {'degree': 5, 'lmda': 0.9152473108773893} ...\n",
      "The training loss is 26.446746906591734 with std:40.216782083607015. The val loss is 32.86097040252694 with std:45.945809884114965.\n",
      "32.86097040252694 0.9152473108773893 5\n",
      "The training loss is 29.984151155789327 with std:45.1105666445127. The val loss is 27.2927841774624 with std:36.77919475995357.\n",
      "27.2927841774624 0.9152473108773893 5\n",
      "The training loss is 29.17355953454696 with std:40.11214041396128. The val loss is 27.7503421704502 with std:45.889381835457144.\n",
      "27.7503421704502 0.9152473108773893 5\n",
      "Evaluating for {'degree': 5, 'lmda': 1.1937766417144369} ...\n",
      "The training loss is 26.636750457101815 with std:40.52874420303982. The val loss is 32.87754784879904 with std:45.96223023462512.\n",
      "32.87754784879904 1.1937766417144369 5\n",
      "The training loss is 30.056727307174008 with std:45.26893139119276. The val loss is 27.488587495557212 with std:37.15461380416296.\n",
      "27.488587495557212 1.1937766417144369 5\n",
      "The training loss is 29.304316078366117 with std:40.35777277783914. The val loss is 27.81309373576913 with std:46.02210330591583.\n",
      "27.81309373576913 1.1937766417144369 5\n",
      "Evaluating for {'degree': 5, 'lmda': 1.5570684047537318} ...\n",
      "The training loss is 26.837560533021623 with std:40.872323656107326. The val loss is 32.92712581893231 with std:46.06366518337173.\n",
      "32.92712581893231 1.5570684047537318 5\n",
      "The training loss is 30.136092552791286 with std:45.459540826777406. The val loss is 27.6892262636369 with std:37.5597832408183.\n",
      "27.6892262636369 1.5570684047537318 5\n",
      "The training loss is 29.452504950658764 with std:40.656070651238224. The val loss is 27.886046032711185 with std:46.187101442845425.\n",
      "27.886046032711185 1.5570684047537318 5\n",
      "Evaluating for {'degree': 5, 'lmda': 2.030917620904737} ...\n",
      "The training loss is 27.0415056519941 with std:41.24490771710868. The val loss is 33.00460814704761 with std:46.25302218683992.\n",
      "33.00460814704761 2.030917620904737 5\n",
      "The training loss is 30.221171945493033 with std:45.68725935769479. The val loss is 27.888780626106836 with std:37.991973747253425.\n",
      "27.888780626106836 2.030917620904737 5\n",
      "The training loss is 29.615270675319223 with std:41.01098695578858. The val loss is 27.96791505731461 with std:46.38910833678329.\n",
      "27.96791505731461 2.030917620904737 5\n",
      "Evaluating for {'degree': 5, 'lmda': 2.6489692876105297} ...\n",
      "The training loss is 27.241962357316837 with std:41.64763054716068. The val loss is 33.10372100019256 with std:46.53364363296062.\n",
      "33.10372100019256 2.6489692876105297 5\n",
      "The training loss is 30.310758142126158 with std:45.958711097046034. The val loss is 28.081933522529955 with std:38.45190652407379.\n",
      "28.081933522529955 2.6489692876105297 5\n",
      "The training loss is 29.788260067474017 with std:41.42659228626523. The val loss is 28.056576540093317 with std:46.632980159271085.\n",
      "28.056576540093317 2.6489692876105297 5\n",
      "Evaluating for {'degree': 5, 'lmda': 3.4551072945922217} ...\n",
      "The training loss is 27.43510849409632 with std:42.087705662673216. The val loss is 33.21954858385293 with std:46.91328511739345.\n",
      "33.21954858385293 3.4551072945922217 5\n",
      "The training loss is 30.40457532098184 with std:46.284225178432195. The val loss is 28.26546003535157 with std:38.94627721429828.\n",
      "28.26546003535157 3.4551072945922217 5\n",
      "The training loss is 29.967242159988626 with std:41.90979472030045. The val loss is 28.150158753076195 with std:46.92501448769136.\n",
      "28.150158753076195 3.4551072945922217 5\n",
      "Evaluating for {'degree': 5, 'lmda': 4.506570337745478} ...\n",
      "The training loss is 27.621349712263328 with std:42.580474902081654. The val loss is 33.35088657644076 with std:47.40787088476221.\n",
      "33.35088657644076 4.506570337745478 5\n",
      "The training loss is 30.504574350168298 with std:46.68010806998114. The val loss is 28.439683315897685 with std:39.49015740026244.\n",
      "28.439683315897685 4.506570337745478 5\n",
      "The training loss is 30.150121724324542 with std:42.47360438274817. The val loss is 28.248472498535325 with std:47.274766024362144.\n",
      "28.248472498535325 4.506570337745478 5\n",
      "Evaluating for {'degree': 5, 'lmda': 5.878016072274912} ...\n",
      "The training loss is 27.806594608535804 with std:43.151379202042484. The val loss is 33.50240478801642 with std:48.04490607501308.\n",
      "33.50240478801642 5.878016072274912 5\n",
      "The training loss is 30.61647951774696 with std:47.17116282388991. The val loss is 28.609922246112912 with std:40.10931930185797.\n",
      "28.609922246112912 5.878016072274912 5\n",
      "The training loss is 30.339106718276106 with std:43.14056214148896. The val loss is 28.354658084037204 with std:47.697228305628926.\n",
      "28.354658084037204 5.878016072274912 5\n",
      "Evaluating for {'degree': 5, 'lmda': 7.666822074546214} ...\n",
      "The training loss is 28.003860464631792 with std:43.83827693354279. The val loss is 33.6870649078517 with std:48.86684053764084.\n",
      "33.6870649078517 7.666822074546214 5\n",
      "The training loss is 30.751819858088126 with std:47.793561433575896. The val loss is 28.788222844189075 with std:40.84280141130443.\n",
      "28.788222844189075 7.666822074546214 5\n",
      "The training loss is 30.543082807320726 with std:43.94625183808551. The val loss is 28.477134035568657 with std:48.21540072829245.\n",
      "28.477134035568657 7.666822074546214 5\n",
      "Evaluating for {'degree': 5, 'lmda': 10.0} ...\n",
      "The training loss is 28.23588762081856 with std:44.694590355046486. The val loss is 33.929558687358906 with std:49.93484354332475.\n",
      "33.929558687358906 10.0 5\n",
      "The training loss is 30.930919540122837 with std:48.59832905021286. The val loss is 28.99591375632929 with std:41.7461667173107.\n",
      "28.99591375632929 10.0 5\n",
      "The training loss is 30.78063150360766 with std:44.94313130131966. The val loss is 28.632215739946464 with std:48.86346526666807.\n",
      "28.632215739946464 10.0 5\n",
      "Evaluating for {'degree': 6, 'lmda': 0.01} ...\n",
      "The training loss is 25.606117786485154 with std:39.01294016617332. The val loss is 33.3463535854803 with std:47.202904030257514.\n",
      "33.3463535854803 0.01 6\n",
      "The training loss is 29.34249322141408 with std:44.28413694014529. The val loss is 26.06184432928354 with std:35.02251050477775.\n",
      "26.06184432928354 0.01 6\n",
      "The training loss is 28.24492275355891 with std:39.03927232496367. The val loss is 27.2397223259645 with std:44.98399519571797.\n",
      "27.2397223259645 0.01 6\n",
      "Evaluating for {'degree': 6, 'lmda': 0.013043213867190054} ...\n",
      "The training loss is 25.745543493101195 with std:39.14300908050481. The val loss is 33.54062583936272 with std:47.39547219289383.\n",
      "33.54062583936272 0.013043213867190054 6\n",
      "The training loss is 29.471883247140614 with std:44.38586489983449. The val loss is 26.23069033901965 with std:35.17469333703747.\n",
      "26.23069033901965 0.013043213867190054 6\n",
      "The training loss is 28.41603750228644 with std:39.161618213772485. The val loss is 27.3520438205996 with std:45.11898285445938.\n",
      "27.3520438205996 0.013043213867190054 6\n",
      "Evaluating for {'degree': 6, 'lmda': 0.017012542798525893} ...\n",
      "The training loss is 25.87556790006315 with std:39.27537404835517. The val loss is 33.7166067682395 with std:47.583467250362695.\n",
      "33.7166067682395 0.017012542798525893 6\n",
      "The training loss is 29.595395074324426 with std:44.48317488067975. The val loss is 26.3930408562276 with std:35.32575670851787.\n",
      "26.3930408562276 0.017012542798525893 6\n",
      "The training loss is 28.58475497249869 with std:39.28509978438381. The val loss is 27.461695794952114 with std:45.25629334839047.\n",
      "27.461695794952114 0.017012542798525893 6\n",
      "Evaluating for {'degree': 6, 'lmda': 0.02218982341458972} ...\n",
      "The training loss is 25.99356375719827 with std:39.40376681556907. The val loss is 33.86551724559272 with std:47.74681294758388.\n",
      "33.86551724559272 0.02218982341458972 6\n",
      "The training loss is 29.71083877723735 with std:44.57545857858778. The val loss is 26.546109162542145 with std:35.47371217864653.\n",
      "26.546109162542145 0.02218982341458972 6\n",
      "The training loss is 28.748058554868518 with std:39.40997132282156. The val loss is 27.566656260979254 with std:45.39207830629092.\n",
      "27.566656260979254 0.02218982341458972 6\n",
      "Evaluating for {'degree': 6, 'lmda': 0.028942661247167517} ...\n",
      "The training loss is 26.09820832262935 with std:39.52237573091975. The val loss is 33.980595307218486 with std:47.8657677500569.\n",
      "33.980595307218486 0.028942661247167517 6\n",
      "The training loss is 29.816091157085612 with std:44.661339436016945. The val loss is 26.68763978190892 with std:35.61680993510247.\n",
      "26.68763978190892 0.028942661247167517 6\n",
      "The training loss is 28.90222135751647 with std:39.534317838716824. The val loss is 27.66475063109558 with std:45.52177167013465.\n",
      "27.66475063109558 0.028942661247167517 6\n",
      "Evaluating for {'degree': 6, 'lmda': 0.037750532053243954} ...\n",
      "The training loss is 26.189594980979386 with std:39.62723519027528. The val loss is 34.05748868982064 with std:47.92428020232757.\n",
      "34.05748868982064 0.037750532053243954 6\n",
      "The training loss is 29.90968074459406 with std:44.73947430768919. The val loss is 26.81647334741385 with std:35.754183381225076.\n",
      "26.81647334741385 0.037750532053243954 6\n",
      "The training loss is 29.04384862329931 with std:39.655282908640054. The val loss is 27.754200827620974 with std:45.641180088569094.\n",
      "27.754200827620974 0.037750532053243954 6\n",
      "Evaluating for {'degree': 6, 'lmda': 0.04923882631706739} ...\n",
      "The training loss is 26.26911462334117 with std:39.7170089489328. The val loss is 34.094185966477 with std:47.91201665383568.\n",
      "34.094185966477 0.04923882631706739 6\n",
      "The training loss is 29.991059120678795 with std:44.809081579294634. The val loss is 26.932747219406696 with std:35.886250132311304.\n",
      "26.932747219406696 0.04923882631706739 6\n",
      "The training loss is 29.170617650783004 with std:39.770241016220524. The val loss is 27.833973276738096 with std:45.747208433551286.\n",
      "27.833973276738096 0.04923882631706739 6\n",
      "Evaluating for {'degree': 6, 'lmda': 0.0642232542222936} ...\n",
      "The training loss is 26.339251242643318 with std:39.79326327939797. The val loss is 34.09082950509232 with std:47.8254077665646.\n",
      "34.09082950509232 0.0642232542222936 6\n",
      "The training loss is 30.060603265247824 with std:44.87017610012407. The val loss is 27.03779647795952 with std:36.014829893034765.\n",
      "27.03779647795952 0.0642232542222936 6\n",
      "The training loss is 29.281613911647952 with std:39.87765000214294. The val loss is 27.90389944305845 with std:45.83814441674954.\n",
      "27.90389944305845 0.0642232542222936 6\n",
      "Evaluating for {'degree': 6, 'lmda': 0.0837677640068292} ...\n",
      "The training loss is 26.403388197321068 with std:39.86041266911481. The val loss is 34.04966498591336 with std:47.668225528520885.\n",
      "34.04966498591336 0.0837677640068292 6\n",
      "The training loss is 30.11946086883579 with std:44.92358901008285. The val loss is 27.133879421915026 with std:36.14302998609854.\n",
      "27.133879421915026 0.0837677640068292 6\n",
      "The training loss is 29.377324008409673 with std:39.977499253420376. The val loss is 27.964620067363338 with std:45.913592718347836.\n",
      "27.964620067363338 0.0837677640068292 6\n",
      "Evaluating for {'degree': 6, 'lmda': 0.10926008611173785} ...\n",
      "The training loss is 26.465646863039073 with std:39.92542559383406. The val loss is 33.9752149745076 with std:47.45190134615417.\n",
      "33.9752149745076 0.10926008611173785 6\n",
      "The training loss is 30.16934950400378 with std:44.970878083580004. The val loss is 27.22383945606899 with std:36.274979596871354.\n",
      "27.22383945606899 0.10926008611173785 6\n",
      "The training loss is 29.459430557838072 with std:40.071425006149354. The val loss is 28.01743819075788 with std:45.97422283838942.\n",
      "28.01743819075788 0.10926008611173785 6\n",
      "Evaluating for {'degree': 6, 'lmda': 0.14251026703029984} ...\n",
      "The training loss is 26.530708320517604 with std:39.99722221036145. The val loss is 33.874575369441324 with std:47.19530643863764.\n",
      "33.874575369441324 0.14251026703029984 6\n",
      "The training loss is 30.212380625531274 with std:45.014208197898796. The val loss is 27.310766927195697 with std:36.41547022142774.\n",
      "27.310766927195697 0.14251026703029984 6\n",
      "The training loss is 29.530551227909253 with std:40.16262528322256. The val loss is 28.06415716695856 with std:46.02148326408612.\n",
      "28.06415716695856 0.14251026703029984 6\n",
      "Evaluating for {'degree': 6, 'lmda': 0.18587918911465645} ...\n",
      "The training loss is 26.603523445378812 with std:40.08561591733681. The val loss is 33.75756541752682 with std:46.92337288161059.\n",
      "33.75756541752682 0.18587918911465645 6\n",
      "The training loss is 30.250931322195257 with std:45.056233870474145. The val loss is 27.39767573335749 with std:36.56951673547597.\n",
      "27.39767573335749 0.18587918911465645 6\n",
      "The training loss is 29.59400772913263 with std:40.2556766577886. The val loss is 28.106947175067074 with std:46.057379275120375.\n",
      "28.106947175067074 0.18587918911465645 6\n",
      "Evaluating for {'degree': 6, 'lmda': 0.24244620170823283} ...\n",
      "The training loss is 26.688821519625016 with std:40.19975542017972. The val loss is 33.63637560070777 with std:46.664062764547516.\n",
      "33.63637560070777 0.24244620170823283 6\n",
      "The training loss is 30.287548580396667 with std:45.099974146659235. The val loss is 27.48718166145933 with std:36.74182880164763.\n",
      "27.48718166145933 0.24244620170823283 6\n",
      "The training loss is 29.65363956879528 with std:40.35627961274584. The val loss is 28.14824666754663 with std:46.084350607646655.\n",
      "28.14824666754663 0.24244620170823283 6\n",
      "Evaluating for {'degree': 6, 'lmda': 0.31622776601683794} ...\n",
      "The training loss is 26.790405704773526 with std:40.34632942730251. The val loss is 33.52444025588052 with std:46.44394228164537.\n",
      "33.52444025588052 0.31622776601683794 6\n",
      "The training loss is 30.324850715188607 with std:45.14865455667895. The val loss is 27.581171295392206 with std:36.93620469258388.\n",
      "27.581171295392206 0.31622776601683794 6\n",
      "The training loss is 29.713618196125754 with std:40.47089353674317. The val loss is 28.190675817313327 with std:46.10523790738226.\n",
      "28.190675817313327 0.31622776601683794 6\n",
      "Evaluating for {'degree': 6, 'lmda': 0.41246263829013524} ...\n",
      "The training loss is 26.910370720208576 with std:40.52814551247452. The val loss is 33.43456601504094 with std:46.28366830923336.\n",
      "33.43456601504094 0.41246263829013524 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 30.36539403937727 with std:45.205515164558825. The val loss is 27.680482865236083 with std:37.15493935878463.\n",
      "27.680482865236083 0.41246263829013524 6\n",
      "The training loss is 29.778187307650576 with std:40.606213764490725. The val loss is 28.236924428678776 with std:46.123300533025024.\n",
      "28.236924428678776 0.41246263829013524 6\n",
      "Evaluating for {'degree': 6, 'lmda': 0.5379838403443686} ...\n",
      "The training loss is 27.04852739727284 with std:40.74379024170221. The val loss is 33.37678560588542 with std:46.19523359934753.\n",
      "33.37678560588542 0.5379838403443686 6\n",
      "The training loss is 30.41150299642621 with std:45.27364801040037. The val loss is 27.784674186109022 with std:37.39845125603172.\n",
      "27.784674186109022 0.5379838403443686 6\n",
      "The training loss is 29.85127110048559 with std:40.768519511287124. The val loss is 28.28958306827092 with std:46.14224740598813.\n",
      "28.28958306827092 0.5379838403443686 6\n",
      "Evaluating for {'degree': 6, 'lmda': 0.701703828670383} ...\n",
      "The training loss is 27.20235288856833 with std:40.98872302519127. The val loss is 33.356718379117744 with std:46.182176099780875.\n",
      "33.356718379117744 0.701703828670383 6\n",
      "The training loss is 30.46511085253149 with std:45.35600327740666. The val loss is 27.892001191553533 with std:37.66540680157082.\n",
      "27.892001191553533 0.701703828670383 6\n",
      "The training loss is 29.935964520867017 with std:40.96306537093272. The val loss is 28.35091662509011 with std:46.1662712774598.\n",
      "28.35091662509011 0.701703828670383 6\n",
      "Evaluating for {'degree': 6, 'lmda': 0.9152473108773893} ...\n",
      "The training loss is 27.367623289898255 with std:41.257518156635975. The val loss is 33.37512920170768 with std:46.242461523204334.\n",
      "33.37512920170768 0.9152473108773893 6\n",
      "The training loss is 30.527697267815974 with std:45.455734009505704. The val loss is 27.999731109611197 with std:37.953565882165705.\n",
      "27.999731109611197 0.9152473108773893 6\n",
      "The training loss is 30.034030548299718 with std:41.19381534281219. The val loss is 28.422625622699215 with std:46.200128722850955.\n",
      "28.422625622699215 0.9152473108773893 6\n",
      "Evaluating for {'degree': 6, 'lmda': 1.1937766417144369} ...\n",
      "The training loss is 27.53958545235869 with std:41.546493575493685. The val loss is 33.4288668808734 with std:46.37256918058219.\n",
      "33.4288668808734 1.1937766417144369 6\n",
      "The training loss is 30.600399709512587 with std:45.57698304297836. The val loss is 28.104833512987124 with std:38.26135300650352.\n",
      "28.104833512987124 1.1937766417144369 6\n",
      "The training loss is 30.145617916654246 with std:41.463810812997835. The val loss is 28.50567655408059 with std:46.249355166514974.\n",
      "28.50567655408059 1.1937766417144369 6\n",
      "Evaluating for {'degree': 6, 'lmda': 1.5570684047537318} ...\n",
      "The training loss is 27.714274393267143 with std:41.85595661082017. The val loss is 33.51275262491576 with std:46.57125773153787.\n",
      "33.51275262491576 1.5570684047537318 6\n",
      "The training loss is 30.684303898331542 with std:45.72606257662351. The val loss is 28.204945472393998 with std:38.589869240099844.\n",
      "28.204945472393998 1.5570684047537318 6\n",
      "The training loss is 30.269400660480635 with std:41.77627930952481. The val loss is 28.60027934933281 with std:46.32070470165048.\n",
      "28.60027934933281 1.5570684047537318 6\n",
      "Evaluating for {'degree': 6, 'lmda': 2.030917620904737} ...\n",
      "The training loss is 27.889552937532343 with std:42.19166666575235. The val loss is 33.62168405729779 with std:46.84226492807244.\n",
      "33.62168405729779 2.030917620904737 6\n",
      "The training loss is 30.780828447571633 with std:45.912823024714555. The val loss is 28.29937369973812 with std:38.944883059711245.\n",
      "28.29937369973812 2.030917620904737 6\n",
      "The training loss is 30.403208398725297 with std:42.13633469354753. The val loss is 28.706048617378187 with std:46.422842476392454.\n",
      "28.706048617378187 2.030917620904737 6\n",
      "Evaluating for {'degree': 6, 'lmda': 2.6489692876105297} ...\n",
      "The training loss is 28.06566050208908 with std:42.56556185673834. The val loss is 33.75237420932026 with std:47.19600392050033.\n",
      "33.75237420932026 2.6489692876105297 6\n",
      "The training loss is 30.89209738316566 with std:46.15196428480651. The val loss is 28.389880446740396 with std:39.33841115293751.\n",
      "28.389880446740396 2.6489692876105297 6\n",
      "The training loss is 30.5450502965982 with std:42.552978209767396. The val loss is 28.822352291280524 with std:46.5672397395934.\n",
      "28.822352291280524 2.6489692876105297 6\n",
      "Evaluating for {'degree': 6, 'lmda': 3.4551072945922217} ...\n",
      "The training loss is 28.245388020807276 with std:42.996120448135656. The val loss is 33.90459110664147 with std:47.650739920625284.\n",
      "33.90459110664147 3.4551072945922217 6\n",
      "The training loss is 31.02130436607652 with std:46.464167522481134. The val loss is 28.481154306090822 with std:39.78980984859727.\n",
      "28.481154306090822 3.4551072945922217 6\n",
      "The training loss is 30.694375281770835 with std:43.04118769164251. The val loss is 28.948883967500453 with std:46.769216676033096.\n",
      "28.948883967500453 3.4551072945922217 6\n",
      "Evaluating for {'degree': 6, 'lmda': 4.506570337745478} ...\n",
      "The training loss is 28.434297728234114 with std:43.50890399705199. The val loss is 34.08222308878417 with std:48.233841014044465.\n",
      "34.08222308878417 4.506570337745478 6\n",
      "The training loss is 31.173292930838524 with std:46.87718628494518. The val loss is 28.581151366385697 with std:40.32669977073131.\n",
      "28.581151366385697 4.506570337745478 6\n",
      "The training loss is 30.853524332422737 with std:43.624118555285236. The val loss is 29.086599995174225 with std:47.049194040886434.\n",
      "29.086599995174225 4.506570337745478 6\n",
      "Evaluating for {'degree': 6, 'lmda': 5.878016072274912} ...\n",
      "The training loss is 28.641586653065293 with std:44.13788798607209. The val loss is 34.294789472034516 with std:48.9836753266685.\n",
      "34.294789472034516 5.878016072274912 6\n",
      "The training loss is 31.355806932364125 with std:47.42731886193181. The val loss is 28.70179290471609 with std:40.986356562405355.\n",
      "28.70179290471609 5.878016072274912 6\n",
      "The training loss is 31.02955922343001 with std:44.335678707219124. The val loss is 29.23928354256304 with std:47.43439765758873.\n",
      "29.23928354256304 5.878016072274912 6\n",
      "Evaluating for {'degree': 6, 'lmda': 7.666822074546214} ...\n",
      "The training loss is 28.88223258097676 with std:44.92811956687594. The val loss is 34.56012537494164 with std:49.95262185586881.\n",
      "34.56012537494164 7.666822074546214 6\n",
      "The training loss is 31.581998771459013 with std:48.16183841784829. The val loss is 28.860706022926717 with std:41.81827509017491.\n",
      "28.860706022926717 7.666822074546214 6\n",
      "The training loss is 31.236886249166258 with std:45.223848926018384. The val loss is 29.4160826613249 with std:47.961389344399166.\n",
      "29.4160826613249 7.666822074546214 6\n",
      "Evaluating for {'degree': 6, 'lmda': 10.0} ...\n",
      "The training loss is 29.18101531967677 with std:45.94002542308929. The val loss is 34.908980686919826 with std:51.21143552352567.\n",
      "34.908980686919826 10.0 6\n",
      "The training loss is 31.874800347263673 with std:49.14286621625884. The val loss is 29.084751288186997 with std:42.88839779452265.\n",
      "29.084751288186997 10.0 6\n",
      "The training loss is 31.50128111693583 with std:46.35505084517332. The val loss is 29.635435544415994 with std:48.6797839364876.\n",
      "29.635435544415994 10.0 6\n",
      "Evaluating for {'degree': 7, 'lmda': 0.01} ...\n",
      "The training loss is 26.470778957970953 with std:39.885741733262. The val loss is 33.74907128114591 with std:46.96923109690446.\n",
      "33.74907128114591 0.01 7\n",
      "The training loss is 30.052286359818087 with std:44.84536342695216. The val loss is 27.090460954299054 with std:36.26503077862043.\n",
      "27.090460954299054 0.01 7\n",
      "The training loss is 29.220287603547664 with std:39.98220618659688. The val loss is 27.83332978310577 with std:45.51639966984696.\n",
      "27.83332978310577 0.01 7\n",
      "Evaluating for {'degree': 7, 'lmda': 0.013043213867190054} ...\n",
      "The training loss is 26.63859935480987 with std:40.077737528431065. The val loss is 33.910869304305294 with std:47.11805060481977.\n",
      "33.910869304305294 0.013043213867190054 7\n",
      "The training loss is 30.16419723787724 with std:44.927318924668555. The val loss is 27.27148693419711 with std:36.47355363949508.\n",
      "27.27148693419711 0.013043213867190054 7\n",
      "The training loss is 29.381209679718776 with std:40.10660033552411. The val loss is 27.960153765221374 with std:45.67101446091474.\n",
      "27.960153765221374 0.013043213867190054 7\n",
      "Evaluating for {'degree': 7, 'lmda': 0.017012542798525893} ...\n",
      "The training loss is 26.7926729808891 with std:40.26166880395608. The val loss is 34.056445302412136 with std:47.26910055760495.\n",
      "34.056445302412136 0.017012542798525893 7\n",
      "The training loss is 30.26907173772931 with std:45.00896203815182. The val loss is 27.441552149626602 with std:36.67287863435316.\n",
      "27.441552149626602 0.017012542798525893 7\n",
      "The training loss is 29.540431946117064 with std:40.239471937805334. The val loss is 28.084266881940856 with std:45.822995631057054.\n",
      "28.084266881940856 0.017012542798525893 7\n",
      "Evaluating for {'degree': 7, 'lmda': 0.02218982341458972} ...\n",
      "The training loss is 26.929936242675424 with std:40.4288703749339. The val loss is 34.17660048585737 with std:47.39698316001315.\n",
      "34.17660048585737 0.02218982341458972 7\n",
      "The training loss is 30.364431081097816 with std:45.0866301137207. The val loss is 27.59610783351257 with std:36.85817232547609.\n",
      "27.59610783351257 0.02218982341458972 7\n",
      "The training loss is 29.69201731680474 with std:40.37458257981096. The val loss is 28.20178244661835 with std:45.9655000019531.\n",
      "28.20178244661835 0.02218982341458972 7\n",
      "Evaluating for {'degree': 7, 'lmda': 0.028942661247167517} ...\n",
      "The training loss is 27.04939836349688 with std:40.57351743399166. The val loss is 34.26507427878378 with std:47.48053285169439.\n",
      "34.26507427878378 0.028942661247167517 7\n",
      "The training loss is 30.448799252480836 with std:45.15749851655193. The val loss is 27.73249330313921 with std:37.02677649357126.\n",
      "27.73249330313921 0.028942661247167517 7\n",
      "The training loss is 29.831277824940113 with std:40.50638137326537. The val loss is 28.309753625322088 with std:46.09317819020795.\n",
      "28.309753625322088 0.028942661247167517 7\n",
      "Evaluating for {'degree': 7, 'lmda': 0.037750532053243954} ...\n",
      "The training loss is 27.151838548628547 with std:40.693257258634304. The val loss is 34.31847490574224 with std:47.5055473981907.\n",
      "34.31847490574224 0.037750532053243954 7\n",
      "The training loss is 30.52172754423262 with std:45.21979148079629. The val loss is 27.84984688567261 with std:37.17821881678879.\n",
      "27.84984688567261 0.037750532053243954 7\n",
      "The training loss is 29.955271523484424 with std:40.63093157952514. The val loss is 28.40650428569238 with std:46.20248992413705.\n",
      "28.40650428569238 0.037750532053243954 7\n",
      "Evaluating for {'degree': 7, 'lmda': 0.04923882631706739} ...\n",
      "The training loss is 27.239388978750615 with std:40.78928584753142. The val loss is 34.33597402328868 with std:47.4660793662968.\n",
      "34.33597402328868 0.04923882631706739 7\n",
      "The training loss is 30.583660471787113 with std:45.272758877187876. The val loss is 27.948755516769868 with std:37.313919498113016.\n",
      "27.948755516769868 0.04923882631706739 7\n",
      "The training loss is 30.062852297387796 with std:40.74630057931542. The val loss is 28.49166000072735 with std:46.29160353334171.\n",
      "28.49166000072735 0.04923882631706739 7\n",
      "Evaluating for {'degree': 7, 'lmda': 0.0642232542222936} ...\n",
      "The training loss is 27.315145692420543 with std:40.86608813271066. The val loss is 34.31906132183436 with std:47.36481120123426.\n",
      "34.31906132183436 0.0642232542222936 7\n",
      "The training loss is 30.63573053295437 with std:45.3165092224704. The val loss is 28.03080402448061 with std:37.43675411437654.\n",
      "28.03080402448061 0.0642232542222936 7\n",
      "The training loss is 30.154412226927043 with std:40.85250700084245. The val loss is 28.56597060818886 with std:46.36003908849961.\n",
      "28.56597060818886 0.0642232542222936 7\n",
      "Evaluating for {'degree': 7, 'lmda': 0.0837677640068292} ...\n",
      "The training loss is 27.382867723490584 with std:40.93095515450858. The val loss is 34.2714969870642 with std:47.21281550927794.\n",
      "34.2714969870642 0.0837677640068292 7\n",
      "The training loss is 30.679563234561748 with std:45.35180137243012. The val loss is 28.098151194155342 with std:37.550608350277926.\n",
      "28.098151194155342 0.0837677640068292 7\n",
      "The training loss is 30.231500506840725 with std:40.951221133189804. The val loss is 28.631050104300275 with std:46.40823838587428.\n",
      "28.631050104300275 0.0837677640068292 7\n",
      "Evaluating for {'degree': 7, 'lmda': 0.10926008611173785} ...\n",
      "The training loss is 27.446751771837892 with std:40.99322495643692. The val loss is 34.19941696286103 with std:47.0285317590933.\n",
      "34.19941696286103 0.10926008611173785 7\n",
      "The training loss is 30.7171446169999 with std:45.37987228502931. The val loss is 28.15320316375051 with std:37.66000010671828.\n",
      "28.15320316375051 0.10926008611173785 7\n",
      "The training loss is 30.29646566955811 with std:41.04539961383234. The val loss is 28.689135587268208 with std:46.43720473172968.\n",
      "28.689135587268208 0.10926008611173785 7\n",
      "Evaluating for {'degree': 7, 'lmda': 0.14251026703029984} ...\n",
      "The training loss is 27.51122505596271 with std:41.06311798313464. The val loss is 34.1113873901576 with std:46.835499733733094.\n",
      "34.1113873901576 0.14251026703029984 7\n",
      "The training loss is 30.750770993766544 with std:45.402342702381326. The val loss is 28.198406115935448 with std:37.76978491271622.\n",
      "28.198406115935448 0.14251026703029984 7\n",
      "The training loss is 30.352199921461253 with std:41.138961972456805. The val loss is 28.74292000699465 with std:46.44829667366024.\n",
      "28.74292000699465 0.14251026703029984 7\n",
      "Evaluating for {'degree': 7, 'lmda': 0.18587918911465645} ...\n",
      "The training loss is 27.580693546705792 with std:41.15016477460917. The val loss is 34.01812173024416 with std:46.65864006832353.\n",
      "34.01812173024416 0.18587918911465645 7\n",
      "The training loss is 30.78307405326704 with std:45.421214555705134. The val loss is 28.236151408111095 with std:37.884923511223555.\n",
      "28.236151408111095 0.18587918911465645 7\n",
      "The training loss is 30.40200086414738 with std:41.23653837866285. The val loss is 28.795468590983717 with std:46.44321041908139.\n",
      "28.795468590983717 0.18587918911465645 7\n",
      "Evaluating for {'degree': 7, 'lmda': 0.24244620170823283} ...\n",
      "The training loss is 27.659221691200575 with std:41.2615472155808. The val loss is 33.93163004309995 with std:46.51972618137668.\n",
      "33.93163004309995 0.24244620170823283 7\n",
      "The training loss is 30.817093787555006 with std:45.43895968766361. The val loss is 28.268774965185806 with std:38.01028455671115.\n",
      "28.268774965185806 0.24244620170823283 7\n",
      "The training loss is 30.44951810864532 with std:41.34326334374102. The val loss is 28.85019369862309 with std:46.424150461671005.\n",
      "28.85019369862309 0.24244620170823283 7\n",
      "Evaluating for {'degree': 7, 'lmda': 0.31622776601683794} ...\n",
      "The training loss is 27.750191036772573 with std:41.40099170651415. The val loss is 33.863796601348895 with std:46.4336449920892.\n",
      "33.863796601348895 0.31622776601683794 7\n",
      "The training loss is 30.856353496274956 with std:45.45869709647897. The val loss is 28.298630112818845 with std:38.1504800883193.\n",
      "28.298630112818845 0.31622776601683794 7\n",
      "The training loss is 30.49871880061872 with std:41.46457014669807. The val loss is 28.910835107377217 with std:46.39415744620575.\n",
      "28.910835107377217 0.31622776601683794 7\n",
      "Evaluating for {'degree': 7, 'lmda': 0.41246263829013524} ...\n",
      "The training loss is 27.856045686171647 with std:41.56883626793575. The val loss is 33.824704293843844 with std:46.40717205076858.\n",
      "33.824704293843844 0.41246263829013524 7\n",
      "The training loss is 30.904873880011756 with std:45.484456112063604. The val loss is 28.328208171932484 with std:38.30977437611764.\n",
      "28.328208171932484 0.41246263829013524 7\n",
      "The training loss is 30.55378637116684 with std:41.60595887613704. The val loss is 28.98137030192548 with std:46.35752499458203.\n",
      "28.98137030192548 0.41246263829013524 7\n",
      "Evaluating for {'degree': 7, 'lmda': 0.5379838403443686} ...\n",
      "The training loss is 27.978221929874238 with std:41.763391680269876. The val loss is 33.82124458235771 with std:46.44081106203665.\n",
      "33.82124458235771 0.5379838403443686 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 30.96705080538758 with std:45.521509071679766. The val loss is 28.360264567898383 with std:38.49213720039063.\n",
      "28.360264567898383 0.5379838403443686 7\n",
      "The training loss is 30.618869838666583 with std:41.77276158950108. The val loss is 29.065769156992136 with std:46.320191150310926.\n",
      "29.065769156992136 0.5379838403443686 7\n",
      "Evaluating for {'degree': 7, 'lmda': 0.701703828670383} ...\n",
      "The training loss is 28.11725606571222 with std:41.98301418589563. The val loss is 33.85648047032621 with std:46.532519983470316.\n",
      "33.85648047032621 0.701703828670383 7\n",
      "The training loss is 31.047325166043247 with std:45.57671397332219. The val loss is 28.397879232978735 with std:38.701491084411245.\n",
      "28.397879232978735 0.701703828670383 7\n",
      "The training loss is 30.69764323229827 with std:41.96998705955135. The val loss is 29.167528832233227 with std:46.28995308512891.\n",
      "29.167528832233227 0.701703828670383 7\n",
      "Evaluating for {'degree': 7, 'lmda': 0.9152473108773893} ...\n",
      "The training loss is 28.272936862808695 with std:42.227947244190176. The val loss is 33.929882271270515 with std:46.6812943215048.\n",
      "33.929882271270515 0.9152473108773893 7\n",
      "The training loss is 31.149613358121314 with std:45.65874715563632. The val loss is 28.444358848104546 with std:38.94211974551124.\n",
      "28.444358848104546 0.9152473108773893 7\n",
      "The training loss is 30.792725668578765 with std:42.20235619934564. The val loss is 29.289001360622926 with std:46.27636556216714.\n",
      "29.289001360622926 0.9152473108773893 7\n",
      "Evaluating for {'degree': 7, 'lmda': 1.1937766417144369} ...\n",
      "The training loss is 28.44434434553827 with std:42.50125527044121. The val loss is 34.03818648684731 with std:46.88920529054311.\n",
      "34.03818648684731 1.1937766417144369 7\n",
      "The training loss is 31.27656412408002 with std:45.7780901836302. The val loss is 28.50291870189923 with std:39.219125642919614.\n",
      "28.50291870189923 1.1937766417144369 7\n",
      "The training loss is 30.905132927785846 with std:42.47461759731125. The val loss is 29.430661021531655 with std:46.2902876750716.\n",
      "29.430661021531655 1.1937766417144369 7\n",
      "Evaluating for {'degree': 7, 'lmda': 1.5570684047537318} ...\n",
      "The training loss is 28.629760827770994 with std:42.80883477183114. The val loss is 34.17650861857333 with std:47.16184711929595.\n",
      "34.17650861857333 1.5570684047537318 7\n",
      "The training loss is 31.428857420621778 with std:45.94674752039657. The val loss is 28.576200824224898 with std:39.53885626724226.\n",
      "28.576200824224898 1.5570684047537318 7\n",
      "The training loss is 31.03402341776785 with std:42.792192855722014. The val loss is 29.590601897096604 with std:46.34324099044899.\n",
      "29.590601897096604 1.5570684047537318 7\n",
      "Evaluating for {'degree': 7, 'lmda': 2.030917620904737} ...\n",
      "The training loss is 28.826667727828948 with std:43.15907898990891. The val loss is 34.3395069700905 with std:47.508129868276264.\n",
      "34.3395069700905 2.030917620904737 7\n",
      "The training loss is 31.604900278660605 with std:46.17792331801009. The val loss is 28.665865026926458 with std:39.909399687539256.\n",
      "28.665865026926458 2.030917620904737 7\n",
      "The training loss is 31.176995389458032 with std:43.16220027037188. The val loss is 29.764608841889594 with std:46.44695539253107.\n",
      "29.764608841889594 2.030917620904737 7\n",
      "Evaluating for {'degree': 7, 'lmda': 2.6489692876105297} ...\n",
      "The training loss is 29.032169784192472 with std:43.56297604933171. The val loss is 34.52264498406381 with std:47.940520000602184.\n",
      "34.52264498406381 2.6489692876105297 7\n",
      "The training loss is 31.801293080188415 with std:46.486141709873365. The val loss is 28.772617722083403 with std:40.3414631687635.\n",
      "28.772617722083403 2.6489692876105297 7\n",
      "The training loss is 31.331065540200196 with std:43.59494622902115. The val loss is 29.947028662212777 with std:46.613567473830116.\n",
      "29.947028662212777 2.6489692876105297 7\n",
      "Evaluating for {'degree': 7, 'lmda': 3.4551072945922217} ...\n",
      "The training loss is 29.244108340068724 with std:44.035221308588. The val loss is 34.723728310910374 with std:48.476420319182886.\n",
      "34.723728310910374 3.4551072945922217 7\n",
      "The training loss is 32.014275017868556 with std:46.88834834571819. The val loss is 28.8970059731606 with std:40.85000931637667.\n",
      "28.8970059731606 3.4551072945922217 7\n",
      "The training loss is 31.4942814828135 with std:44.105993156293536. The val loss is 30.1324234127379 with std:46.85681356252434.\n",
      "30.1324234127379 3.4551072945922217 7\n",
      "Evaluating for {'degree': 7, 'lmda': 4.506570337745478} ...\n",
      "The training loss is 29.462921723284335 with std:44.596531046743344. The val loss is 34.94488313709878 with std:49.14083449563149.\n",
      "34.94488313709878 4.506570337745478 7\n",
      "The training loss is 32.24209962277843 with std:47.40630341593195. The val loss is 29.041119346974405 with std:41.45687480669127.\n",
      "29.041119346974405 4.506570337745478 7\n",
      "The training loss is 31.66782627492371 with std:44.71887446222462. The val loss is 30.317786673407205 with std:47.19429733645602.\n",
      "30.317786673407205 4.506570337745478 7\n",
      "Evaluating for {'degree': 7, 'lmda': 5.878016072274912} ...\n",
      "The training loss is 29.69419323832865 with std:45.27703198864246. The val loss is 35.19513848433233 with std:49.97010271039163.\n",
      "35.19513848433233 5.878016072274912 7\n",
      "The training loss is 32.48815156460229 with std:48.07022769819059. The val loss is 29.211180611208807 with std:42.19437803560454.\n",
      "29.211180611208807 5.878016072274912 7\n",
      "The training loss is 31.858564827216473 with std:45.468484324565345. The val loss is 30.505122572765053 with std:47.65070211600843.\n",
      "30.505122572765053 5.878016072274912 7\n",
      "Evaluating for {'degree': 7, 'lmda': 7.666822074546214} ...\n",
      "The training loss is 29.951943392614282 with std:46.12052954460594. The val loss is 35.49390995857911 with std:51.01642733534733.\n",
      "35.49390995857911 7.666822074546214 7\n",
      "The training loss is 32.76475355015107 with std:48.92344673557218. The val loss is 29.42104654367594 with std:43.10981590518075.\n",
      "29.42104654367594 7.666822074546214 7\n",
      "The training loss is 32.08225401508019 with std:46.40518859560355. The val loss is 30.704459055957795 with std:48.26179896676625.\n",
      "30.704459055957795 7.666822074546214 7\n",
      "Evaluating for {'degree': 7, 'lmda': 10.0} ...\n",
      "The training loss is 30.263045861156485 with std:47.18956704312702. The val loss is 35.875961978743035 with std:52.3530125602269.\n",
      "35.875961978743035 10.0 7\n",
      "The training loss is 33.09799910435692 with std:50.02778555229986. The val loss is 29.696913239493608 with std:44.2707750979924.\n",
      "29.696913239493608 10.0 7\n",
      "The training loss is 32.3679948980254 with std:47.599743102974315. The val loss is 30.93777144662438 with std:49.079232105775496.\n",
      "30.93777144662438 10.0 7\n",
      "Evaluating for {'degree': 8, 'lmda': 0.01} ...\n",
      "The training loss is 27.431166164397418 with std:40.90265611471777. The val loss is 33.881589902259435 with std:46.47257760980406.\n",
      "33.881589902259435 0.01 8\n",
      "The training loss is 30.66483158198745 with std:45.327982409993474. The val loss is 28.065807685814168 with std:37.67115569803284.\n",
      "28.065807685814168 0.01 8\n",
      "The training loss is 30.097697877047967 with std:40.95480227937203. The val loss is 28.50561441297611 with std:46.04892656048713.\n",
      "28.50561441297611 0.01 8\n",
      "Evaluating for {'degree': 8, 'lmda': 0.013043213867190054} ...\n",
      "The training loss is 27.595389313665024 with std:41.0764156248504. The val loss is 33.9965429627929 with std:46.540222457354076.\n",
      "33.9965429627929 0.013043213867190054 8\n",
      "The training loss is 30.758982839648215 with std:45.40667136141277. The val loss is 28.223899247568053 with std:37.87430925998227.\n",
      "28.223899247568053 0.013043213867190054 8\n",
      "The training loss is 30.248737711153623 with std:41.10362125315664. The val loss is 28.639562677022433 with std:46.18724147001894.\n",
      "28.639562677022433 0.013043213867190054 8\n",
      "Evaluating for {'degree': 8, 'lmda': 0.017012542798525893} ...\n",
      "The training loss is 27.73722722226889 with std:41.228224331207784. The val loss is 34.09449431368836 with std:46.603391665988326.\n",
      "34.09449431368836 0.017012542798525893 8\n",
      "The training loss is 30.84185975671882 with std:45.47510978101984. The val loss is 28.358315004809103 with std:38.050434942463475.\n",
      "28.358315004809103 0.017012542798525893 8\n",
      "The training loss is 30.38459773312459 with std:41.24233907833908. The val loss is 28.761196879887013 with std:46.30719445633092.\n",
      "28.761196879887013 0.017012542798525893 8\n",
      "Evaluating for {'degree': 8, 'lmda': 0.02218982341458972} ...\n",
      "The training loss is 27.857199880011244 with std:41.3578356773034. The val loss is 34.17264746853733 with std:46.65472078521828.\n",
      "34.17264746853733 0.02218982341458972 8\n",
      "The training loss is 30.9132778754223 with std:45.53162627788065. The val loss is 28.468426200652424 with std:38.199555424505796.\n",
      "28.468426200652424 0.02218982341458972 8\n",
      "The training loss is 30.503326650917632 with std:41.367979999952595. The val loss is 28.86947894925935 with std:46.40626484732327.\n",
      "28.86947894925935 0.02218982341458972 8\n",
      "Evaluating for {'degree': 8, 'lmda': 0.028942661247167517} ...\n",
      "The training loss is 27.957272624877216 with std:41.46688062751828. The val loss is 34.23004506424823 with std:46.68975939415141.\n",
      "34.23004506424823 0.028942661247167517 8\n",
      "The training loss is 30.973946581623945 with std:45.57552828118141. The val loss is 28.554944348680056 with std:38.323477234974426.\n",
      "28.554944348680056 0.028942661247167517 8\n",
      "The training loss is 30.604734027533734 with std:41.47967579009689. The val loss is 28.964895051551053 with std:46.483467838293464.\n",
      "28.964895051551053 0.028942661247167517 8\n",
      "Evaluating for {'degree': 8, 'lmda': 0.037750532053243954} ...\n",
      "The training loss is 28.040337550326303 with std:41.55841084491127. The val loss is 34.267079765677124 with std:46.70668976937234.\n",
      "34.267079765677124 0.037750532053243954 8\n",
      "The training loss is 31.025272715502098 with std:45.60689459756732. The val loss is 28.619447260305346 with std:38.425250734091755.\n",
      "28.619447260305346 0.037750532053243954 8\n",
      "The training loss is 30.689976047699847 with std:41.5783111889919. The val loss is 29.04914012549437 with std:46.53894299721792.\n",
      "29.04914012549437 0.037750532053243954 8\n",
      "Evaluating for {'degree': 8, 'lmda': 0.04923882631706739} ...\n",
      "The training loss is 28.109847517304992 with std:41.63654783625905. The val loss is 34.28515448909126 with std:46.70601412701379.\n",
      "34.28515448909126 0.04923882631706739 8\n",
      "The training loss is 31.069194030595877 with std:45.62638615882041. The val loss is 28.66401380289726 with std:38.5087607469007.\n",
      "28.66401380289726 0.04923882631706739 8\n",
      "The training loss is 30.761123702650938 with std:41.66608009841444. The val loss is 29.124786996014247 with std:46.57356457266524.\n",
      "29.124786996014247 0.04923882631706739 8\n",
      "Evaluating for {'degree': 8, 'lmda': 0.0642232542222936} ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 28.16962655236549 with std:41.70627555763666. The val loss is 34.286565984451954 with std:46.69036463282116.\n",
      "34.286565984451954 0.0642232542222936 8\n",
      "The training loss is 31.108091490799584 with std:45.63515246446994. The val loss is 28.691029880899425 with std:38.57851295067813.\n",
      "28.691029880899425 0.0642232542222936 8\n",
      "The training loss is 30.82082054967734 with std:41.74609788899589. The val loss is 29.19502189349453 with std:46.58868037237882.\n",
      "29.19502189349453 0.0642232542222936 8\n",
      "Evaluating for {'degree': 8, 'lmda': 0.0837677640068292} ...\n",
      "The training loss is 28.2238295799607 with std:41.773328410515354. The val loss is 34.27460254116526 with std:46.66442165215196.\n",
      "34.27460254116526 0.0837677640068292 8\n",
      "The training loss is 31.14479073279433 with std:45.634869980918985. The val loss is 28.703177053869634 with std:38.63961504478654.\n",
      "28.703177053869634 0.0837677640068292 8\n",
      "The training loss is 30.872066642886846 with std:41.82213031312769. The val loss is 29.263465053419534 with std:46.586024505108774.\n",
      "29.263465053419534 0.0837677640068292 8\n",
      "Evaluating for {'degree': 8, 'lmda': 0.10926008611173785} ...\n",
      "The training loss is 28.276990227256267 with std:41.84406693954887. The val loss is 34.253775623312215 with std:46.634796339431524.\n",
      "34.253775623312215 0.10926008611173785 8\n",
      "The training loss is 31.18262889991614 with std:45.62791139112581. The val loss is 28.703576290494855 with std:38.69789790128115.\n",
      "28.703576290494855 0.10926008611173785 8\n",
      "The training loss is 30.91811340374203 with std:41.89843297078037. The val loss is 29.334045731202323 with std:46.56779836966461.\n",
      "29.334045731202323 0.10926008611173785 8\n",
      "Evaluating for {'degree': 8, 'lmda': 0.14251026703029984} ...\n",
      "The training loss is 28.334066704275006 with std:41.92521368373439. The val loss is 34.23003694625296 with std:46.60968578877317.\n",
      "34.23003694625296 0.14251026703029984 8\n",
      "The training loss is 31.225530239245277 with std:45.617606788598394. The val loss is 28.696022151696003 with std:38.7600710030518.\n",
      "28.696022151696003 0.14251026703029984 8\n",
      "The training loss is 30.962421394812566 with std:41.97965333092998. The val loss is 29.410868166474287 with std:46.53687152058948.\n",
      "29.410868166474287 0.14251026703029984 8\n",
      "Evaluating for {'degree': 8, 'lmda': 0.18587918911465645} ...\n",
      "The training loss is 28.40037088680942 with std:42.023363074841605. The val loss is 34.21077586179206 with std:46.59818780988697.\n",
      "34.21077586179206 0.18587918911465645 8\n",
      "The training loss is 31.278001190597884 with std:45.60851446898842. The val loss is 28.685199570039863 with std:38.833756649899506.\n",
      "28.685199570039863 0.18587918911465645 8\n",
      "The training loss is 31.00861484491049 with std:42.0707277953288. The val loss is 29.497990899858074 with std:46.49701491125211.\n",
      "29.497990899858074 0.18587918911465645 8\n",
      "Evaluating for {'degree': 8, 'lmda': 0.24244620170823283} ...\n",
      "The training loss is 28.48125991775191 with std:42.144292479960235. The val loss is 34.20438411718795 with std:46.609368466273125.\n",
      "34.20438411718795 0.24244620170823283 8\n",
      "The training loss is 31.34493355833694 with std:45.606575728588346. The val loss is 28.676733950484707 with std:38.927220376121994.\n",
      "28.676733950484707 0.24244620170823283 8\n",
      "The training loss is 31.060364001713808 with std:42.17671067733782. The val loss is 29.599055683506574 with std:46.45304992647043.\n",
      "29.599055683506574 0.24244620170823283 8\n",
      "Evaluating for {'degree': 8, 'lmda': 0.31622776601683794} ...\n",
      "The training loss is 28.58151331099211 with std:42.2922498326649. The val loss is 34.21926816433537 with std:46.651393647361225.\n",
      "34.21926816433537 0.31622776601683794 8\n",
      "The training loss is 31.431118321164693 with std:45.61900960009275. The val loss is 28.67691547044176 with std:39.048660643029706.\n",
      "28.67691547044176 0.31622776601683794 8\n",
      "The training loss is 31.12114559073024 with std:42.30250655117579. The val loss is 29.716756558294296 with std:46.41079909211057.\n",
      "29.716756558294296 0.31622776601683794 8\n",
      "Evaluating for {'degree': 8, 'lmda': 0.41246263829013524} ...\n",
      "The training loss is 28.704438064372347 with std:42.469484634227214. The val loss is 34.262406236546944 with std:46.731060324643124.\n",
      "34.262406236546944 0.41246263829013524 8\n",
      "The training loss is 31.54045288463961 with std:45.65384755534246. The val loss is 28.692003663241877 with std:39.20509079057461.\n",
      "28.692003663241877 0.41246263829013524 8\n",
      "The training loss is 31.193877402142704 with std:42.45254286002848. The val loss is 29.852232899324516 with std:46.3767827582146.\n",
      "29.852232899324516 0.41246263829013524 8\n",
      "Evaluating for {'degree': 8, 'lmda': 0.5379838403443686} ...\n",
      "The training loss is 28.850923808424135 with std:42.676240555367734. The val loss is 34.33782722546456 with std:46.853801852746344.\n",
      "34.33782722546456 0.5379838403443686 8\n",
      "The training loss is 31.674976001834814 with std:45.71913900174628. The val loss is 28.727181330939214 with std:39.40112668637258.\n",
      "28.727181330939214 0.5379838403443686 8\n",
      "The training loss is 31.280486419533883 with std:42.630496267448606. The val loss is 30.004567183331293 with std:46.35771968432703.\n",
      "30.004567183331293 0.5379838403443686 8\n",
      "Evaluating for {'degree': 8, 'lmda': 0.701703828670383} ...\n",
      "The training loss is 29.018821377946956 with std:42.91126308813389. The val loss is 34.445575671873534 with std:47.02391899482539.\n",
      "34.445575671873534 0.701703828670383 8\n",
      "The training loss is 31.83403312620613 with std:45.822047604277145. The val loss is 28.785442876233024 with std:39.63824192432655.\n",
      "28.785442876233024 0.701703828670383 8\n",
      "The training loss is 31.381526613760165 with std:42.839232618926246. The val loss is 30.17060515593635 with std:46.36001156563852.\n",
      "30.17060515593635 0.701703828670383 8\n",
      "Evaluating for {'degree': 8, 'lmda': 0.9152473108773893} ...\n",
      "The training loss is 29.20301189056444 with std:43.17272404392079. The val loss is 34.58165835072115 with std:47.244784314436316.\n",
      "34.58165835072115 0.9152473108773893 8\n",
      "The training loss is 32.01395135549083 with std:45.9682127056808. The val loss is 28.86684884817471 with std:39.91504508567002.\n",
      "28.86684884817471 0.9152473108773893 8\n",
      "The training loss is 31.49598710616688 with std:43.08109942035504. The val loss is 30.345245732944495 with std:46.389449078821286.\n",
      "30.345245732944495 0.9152473108773893 8\n",
      "Evaluating for {'degree': 8, 'lmda': 1.1937766417144369} ...\n",
      "The training loss is 29.396307629068783 with std:43.45942468431979. The val loss is 34.73911789275809 with std:47.519139637515615.\n",
      "34.73911789275809 1.1937766417144369 8\n",
      "The training loss is 32.20849353462903 with std:46.161760571327854. The val loss is 28.96852703412641 with std:40.228769499782544.\n",
      "28.96852703412641 1.1937766417144369 8\n",
      "The training loss is 31.62141378205161 with std:43.35862705972407. The val loss is 30.522191935480432 with std:46.45132958695708.\n",
      "30.522191935480432 1.1937766417144369 8\n",
      "Evaluating for {'degree': 8, 'lmda': 1.5570684047537318} ...\n",
      "The training loss is 29.59098432645962 with std:43.77217758506346. The val loss is 34.90993163705089 with std:47.84993415726279.\n",
      "34.90993163705089 1.5570684047537318 8\n",
      "The training loss is 32.410084536824534 with std:46.406181751272186. The val loss is 29.085535487294692 with std:40.57764485717893.\n",
      "29.085535487294692 1.5570684047537318 8\n",
      "The training loss is 31.754412607646405 with std:43.67560736253429. The val loss is 30.695000381246793 with std:46.55105678487452.\n",
      "30.695000381246793 1.5570684047537318 8\n",
      "Evaluating for {'degree': 8, 'lmda': 2.030917620904737} ...\n",
      "The training loss is 29.78050891889033 with std:44.115298413550995. The val loss is 35.08716809163942 with std:48.242035419632785.\n",
      "35.08716809163942 2.030917620904737 8\n",
      "The training loss is 32.61151417525914 with std:46.70602754136664. The val loss is 29.21237271017303 with std:40.96352113608731.\n",
      "29.21237271017303 2.030917620904737 8\n",
      "The training loss is 31.891524374798433 with std:44.03848798756725. The val loss is 30.858203762007918 with std:46.69517873482956.\n",
      "30.858203762007918 2.030917620904737 8\n",
      "Evaluating for {'degree': 8, 'lmda': 2.6489692876105297} ...\n",
      "The training loss is 29.961056517448334 with std:44.49815612107057. The val loss is 35.26690378817883 with std:48.70471190275025.\n",
      "35.26690378817883 2.6489692876105297 8\n",
      "The training loss is 32.80770795536912 with std:47.06917211835605. The val loss is 29.34473796478467 with std:41.394240361722396.\n",
      "29.34473796478467 2.6489692876105297 8\n",
      "The training loss is 32.03038927760921 with std:44.45805874788999. The val loss is 31.008336506142328 with std:46.89277459231421.\n",
      "31.008336506142328 2.6489692876105297 8\n",
      "Evaluating for {'degree': 8, 'lmda': 3.4551072945922217} ...\n",
      "The training loss is 30.13267313147423 with std:44.93679359084862. The val loss is 35.44973476873452 with std:49.254495565400674.\n",
      "35.44973476873452 3.4551072945922217 8\n",
      "The training loss is 32.99729342363189 with std:47.509352838034516. The val loss is 29.481221417306916 with std:41.885653731869326.\n",
      "29.481221417306916 3.4551072945922217 8\n",
      "The training loss is 32.17110917395796 with std:44.95146974630981. The val loss is 31.14483659274787 with std:47.15713557208175.\n",
      "31.14483659274787 3.4551072945922217 8\n",
      "Evaluating for {'degree': 8, 'lmda': 4.506570337745478} ...\n",
      "The training loss is 30.300271724534053 with std:45.45575928175939. The val loss is 35.64210630620578 with std:49.918130833872205.\n",
      "35.64210630620578 4.506570337745478 8\n",
      "The training loss is 33.18399388556098 with std:48.04884767320235. The val loss is 29.624879042111285 with std:42.4635653363738.\n",
      "29.624879042111285 4.506570337745478 8\n",
      "The training loss is 32.317810461956185 with std:45.54467801914272. The val loss is 31.270974482545856 with std:47.50778013078456.\n",
      "31.270974482545856 4.506570337745478 8\n",
      "Evaluating for {'degree': 8, 'lmda': 5.878016072274912} ...\n",
      "The training loss is 30.474908002770686 with std:46.09042759683978. The val loss is 35.857978987102655 with std:50.73564983133141.\n",
      "35.857978987102655 5.878016072274912 8\n",
      "The training loss is 33.37820505674302 with std:48.72138291700821. The val loss is 29.784972110023848 with std:43.166076905678004.\n",
      "29.784972110023848 5.878016072274912 8\n",
      "The training loss is 32.480606254320065 with std:46.275474188951044. The val loss is 31.39513525134958 with std:47.97295914794072.\n",
      "31.39513525134958 5.878016072274912 8\n",
      "Evaluating for {'degree': 8, 'lmda': 7.666822074546214} ...\n",
      "The training loss is 30.675939817553257 with std:46.89015630051946. The val loss is 36.12152721948667 with std:51.76388790268535.\n",
      "36.12152721948667 7.666822074546214 8\n",
      "The training loss is 33.59936035226309 with std:49.57555538625423. The val loss is 29.979417436021876 with std:44.046807845354486.\n",
      "29.979417436021876 7.666822074546214 8\n",
      "The training loss is 32.67840316251583 with std:47.19729227420761. The val loss is 31.53294012971735 with std:48.592900227778124.\n",
      "31.53294012971735 7.666822074546214 8\n",
      "Evaluating for {'degree': 8, 'lmda': 10.0} ...\n",
      "The training loss is 30.934781464590227 with std:47.92258016719626. The val loss is 36.47169028380513 with std:53.080786883281604.\n",
      "36.47169028380513 10.0 8\n",
      "The training loss is 33.87986529865841 with std:50.67909481791225. The val loss is 30.238685389942646 with std:45.17931493603651.\n",
      "30.238685389942646 10.0 8\n",
      "The training loss is 32.94324493940222 with std:48.38399459535787. The val loss is 31.710847810921905 with std:49.424061397155896.\n",
      "31.710847810921905 10.0 8\n",
      "Evaluating for {'degree': 9, 'lmda': 0.01} ...\n",
      "The training loss is 28.25136327902619 with std:41.801660392907834. The val loss is 34.05787536796576 with std:46.455229517789014.\n",
      "34.05787536796576 0.01 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 31.169830798501714 with std:45.71997125174399. The val loss is 28.780054990036266 with std:38.806513414483405.\n",
      "28.780054990036266 0.01 9\n",
      "The training loss is 30.82135573041385 with std:41.84756925591169. The val loss is 29.155634562273455 with std:46.418470523390155.\n",
      "29.155634562273455 0.01 9\n",
      "Evaluating for {'degree': 9, 'lmda': 0.013043213867190054} ...\n",
      "The training loss is 28.36026824068083 with std:41.914253091586204. The val loss is 34.127211932673966 with std:46.49396761700766.\n",
      "34.127211932673966 0.013043213867190054 9\n",
      "The training loss is 31.232667891781016 with std:45.75836131782431. The val loss is 28.86195248279765 with std:38.926467828395275.\n",
      "28.86195248279765 0.013043213867190054 9\n",
      "The training loss is 30.919729498743582 with std:41.954010459691574. The val loss is 29.259264617602224 with std:46.48893455176392.\n",
      "29.259264617602224 0.013043213867190054 9\n",
      "Evaluating for {'degree': 9, 'lmda': 0.017012542798525893} ...\n",
      "The training loss is 28.45074169027153 with std:42.008373734952634. The val loss is 34.18527784675228 with std:46.52841292700885.\n",
      "34.18527784675228 0.017012542798525893 9\n",
      "The training loss is 31.286721069876936 with std:45.78220937312568. The val loss is 28.920203701675916 with std:39.01988188039213.\n",
      "28.920203701675916 0.017012542798525893 9\n",
      "The training loss is 31.00261972141382 with std:42.045495145888786. The val loss is 29.352649886100576 with std:46.538933070643296.\n",
      "29.352649886100576 0.017012542798525893 9\n",
      "Evaluating for {'degree': 9, 'lmda': 0.02218982341458972} ...\n",
      "The training loss is 28.526464336914426 with std:42.087844549335486. The val loss is 34.23352606513307 with std:46.558490007802234.\n",
      "34.23352606513307 0.02218982341458972 9\n",
      "The training loss is 31.334302289599034 with std:45.79231354429175. The val loss is 28.95708011535372 with std:39.09072075297977.\n",
      "28.95708011535372 0.02218982341458972 9\n",
      "The training loss is 31.07241478143497 with std:42.12436623591491. The val loss is 29.439039663691908 with std:46.569802601610284.\n",
      "29.439039663691908 0.02218982341458972 9\n",
      "Evaluating for {'degree': 9, 'lmda': 0.028942661247167517} ...\n",
      "The training loss is 28.59148678615566 with std:42.15701785630332. The val loss is 34.27390261092347 with std:46.58499302702059.\n",
      "34.27390261092347 0.028942661247167517 9\n",
      "The training loss is 31.37829428389938 with std:45.79014511643646. The val loss is 28.97530523141687 with std:39.143657935282356.\n",
      "28.97530523141687 0.028942661247167517 9\n",
      "The training loss is 31.132037932216694 with std:42.193772090711605. The val loss is 29.522292375061863 with std:46.5835138986669.\n",
      "29.522292375061863 0.028942661247167517 9\n",
      "Evaluating for {'degree': 9, 'lmda': 0.037750532053243954} ...\n",
      "The training loss is 28.650131008884944 with std:42.22070182046188. The val loss is 34.30866939941118 with std:46.60937930326078.\n",
      "34.30866939941118 0.037750532053243954 9\n",
      "The training loss is 31.42212870463776 with std:45.777948276626006. The val loss is 28.978097548595038 with std:39.18410911319384.\n",
      "28.978097548595038 0.037750532053243954 9\n",
      "The training loss is 31.18472370717672 with std:42.25740869680266. The val loss is 29.606628162912678 with std:46.58262628896825.\n",
      "29.606628162912678 0.037750532053243954 9\n",
      "Evaluating for {'degree': 9, 'lmda': 0.04923882631706739} ...\n",
      "The training loss is 28.706961687932317 with std:42.28415616703851. The val loss is 34.340323841453575 with std:46.6336546510901.\n",
      "34.340323841453575 0.04923882631706739 9\n",
      "The training loss is 31.469769842806194 with std:45.75892429439899. The val loss is 28.969327132709108 with std:39.218369535625705.\n",
      "28.969327132709108 0.04923882631706739 9\n",
      "The training loss is 31.233856575796793 with std:42.31933532076406. The val loss is 29.6963723976948 with std:46.57032802587879.\n",
      "29.6963723976948 0.04923882631706739 9\n",
      "Evaluating for {'degree': 9, 'lmda': 0.0642232542222936} ...\n",
      "The training loss is 28.766731958761817 with std:42.353046724645516. The val loss is 34.37156831591263 with std:46.66031485186128.\n",
      "34.37156831591263 0.0642232542222936 9\n",
      "The training loss is 31.52561129692323 with std:45.73739859984694. The val loss is 28.95367317818541 with std:39.25369932315528.\n",
      "28.95367317818541 0.0642232542222936 9\n",
      "The training loss is 31.28280622168834 with std:42.38379378022028. The val loss is 29.79561187083705 with std:46.5504596157057.\n",
      "29.79561187083705 0.0642232542222936 9\n",
      "Evaluating for {'degree': 9, 'lmda': 0.0837677640068292} ...\n",
      "The training loss is 28.83420422277842 with std:42.4332298104008. The val loss is 34.40528243359386 with std:46.69230605588586.\n",
      "34.40528243359386 0.0837677640068292 9\n",
      "The training loss is 31.5941808966115 with std:45.7188230930254. The val loss is 28.93663385835965 with std:39.29816354385982.\n",
      "28.93663385835965 0.0837677640068292 9\n",
      "The training loss is 31.33469965781345 with std:42.454961549792486. The val loss is 29.90770938667037 with std:46.52739328892053.\n",
      "29.90770938667037 0.0837677640068292 9\n",
      "Evaluating for {'degree': 9, 'lmda': 0.10926008611173785} ...\n",
      "The training loss is 28.913776870257127 with std:42.530261434891926. The val loss is 34.44446276265259 with std:46.73298272048466.\n",
      "34.44446276265259 0.10926008611173785 9\n",
      "The training loss is 31.679587512729533 with std:45.70946615163844. The val loss is 28.92424620182542 with std:39.36006098274865.\n",
      "28.92424620182542 0.10926008611173785 9\n",
      "The training loss is 31.39211239670896 with std:42.536611985356416. The val loss is 30.034698514231547 with std:46.50567249987678.\n",
      "30.034698514231547 0.10926008611173785 9\n",
      "Evaluating for {'degree': 9, 'lmda': 0.14251026703029984} ...\n",
      "The training loss is 29.008924962302157 with std:42.6486208005127. The val loss is 34.49211240250206 with std:46.78606130062842.\n",
      "34.49211240250206 0.14251026703029984 9\n",
      "The training loss is 31.78475248028738 with std:45.71573083586954. The val loss is 28.922457813299218 with std:39.446910952452434.\n",
      "28.922457813299218 0.14251026703029984 9\n",
      "The training loss is 31.456733881445633 with std:42.63173698913744. The val loss is 30.17669497288024 with std:46.489421076035875.\n",
      "30.17669497288024 0.14251026703029984 9\n",
      "Evaluating for {'degree': 9, 'lmda': 0.18587918911465645} ...\n",
      "The training loss is 29.121572499973873 with std:42.79080511153139. The val loss is 34.551072165800136 with std:46.85556909908281.\n",
      "34.551072165800136 0.18587918911465645 9\n",
      "The training loss is 31.91062521144442 with std:45.74322341967615. The val loss is 28.93625479994501 with std:39.56420221013017.\n",
      "28.93625479994501 0.18587918911465645 9\n",
      "The training loss is 31.52913329369886 with std:42.742273184607825. The val loss is 30.33155660624039 with std:46.481688258281395.\n",
      "30.33155660624039 0.18587918911465645 9\n",
      "Evaluating for {'degree': 9, 'lmda': 0.24244620170823283} ...\n",
      "The training loss is 29.25160059566888 with std:42.95663310232231. The val loss is 34.62377516792244 with std:46.94576173311447.\n",
      "34.62377516792244 0.24244620170823283 9\n",
      "The training loss is 32.05569703141656 with std:45.795891191554006. The val loss is 28.96881912788777 with std:39.71433338250341.\n",
      "28.96881912788777 0.24244620170823283 9\n",
      "The training loss is 31.608764736385314 with std:42.869098208967735. The val loss is 30.495014558032867 with std:46.48401497443043.\n",
      "30.495014558032867 0.24244620170823283 9\n",
      "Evaluating for {'degree': 9, 'lmda': 0.31622776601683794} ...\n",
      "The training loss is 29.396693494494578 with std:43.14316402614411. The val loss is 34.71189689879062 with std:47.06095991739561.\n",
      "34.71189689879062 0.31622776601683794 9\n",
      "The training loss is 32.2160847715432 with std:45.875613795270084. The val loss is 29.02104037044316 with std:39.896221653273265.\n",
      "29.02104037044316 0.31622776601683794 9\n",
      "The training loss is 31.694268710191515 with std:43.012382317581995. The val loss is 30.661341181055857 with std:46.49648266523144.\n",
      "30.661341181055857 0.31622776601683794 9\n",
      "Evaluating for {'degree': 9, 'lmda': 0.41246263829013524} ...\n",
      "The training loss is 29.55261746853463 with std:43.34548123235324. The val loss is 34.81591571866414 with std:47.20528257239296.\n",
      "34.81591571866414 0.41246263829013524 9\n",
      "The training loss is 32.38622686716103 with std:45.98246866509422. The val loss is 29.091569398481525 with std:40.105840302863754.\n",
      "29.091569398481525 0.41246263829013524 9\n",
      "The training loss is 31.78398055176854 with std:43.172221390742905. The val loss is 30.82438267779544 with std:46.518315628234824.\n",
      "30.82438267779544 0.41246263829013524 9\n",
      "Evaluating for {'degree': 9, 'lmda': 0.5379838403443686} ...\n",
      "The training loss is 29.713877324169363 with std:43.55822088940387. The val loss is 34.9347160915534 with std:47.3823292891317.\n",
      "34.9347160915534 0.5379838403443686 9\n",
      "The training loss is 32.55994629800179 with std:46.11557615736738. The val loss is 29.177348997477043 with std:40.33757704568493.\n",
      "29.177348997477043 0.5379838403443686 9\n",
      "The training loss is 31.87643956958193 with std:43.349345583616405. The val loss is 30.978616756272697 with std:46.54885941425311.\n",
      "30.978616756272697 0.5379838403443686 9\n",
      "Evaluating for {'degree': 9, 'lmda': 0.701703828670383} ...\n",
      "The training loss is 29.87458680948605 with std:43.77735829088281. The val loss is 35.06547756076231 with std:47.59495029558268.\n",
      "35.06547756076231 0.701703828670383 9\n",
      "The training loss is 32.73148823024867 with std:46.27418684784037. The val loss is 29.274367651797103 with std:40.58601704470083.\n",
      "29.274367651797103 0.701703828670383 9\n",
      "The training loss is 31.97069035616885 with std:43.54569132913601. The val loss is 31.1199087468735 with std:46.58861235078752.\n",
      "31.1199087468735 0.701703828670383 9\n",
      "Evaluating for {'degree': 9, 'lmda': 0.9152473108773893} ...\n",
      "The training loss is 30.029360048173867 with std:44.00167504189323. The val loss is 35.204059602009764 with std:47.845314253900185.\n",
      "35.204059602009764 0.9152473108773893 9\n",
      "The training loss is 32.89623316572133 with std:46.45867342125142. The val loss is 29.378380724403275 with std:40.84769741161919.\n",
      "29.378380724403275 0.9152473108773893 9\n",
      "The training loss is 32.06628870250101 with std:43.76474873957688. The val loss is 31.245812742201228 with std:46.64002502221406.\n",
      "31.245812742201228 0.9152473108773893 9\n",
      "Evaluating for {'degree': 9, 'lmda': 1.1937766417144369} ...\n",
      "The training loss is 30.174065445434508 with std:44.23360160367333. The val loss is 35.34588599268628 with std:48.13551326568753.\n",
      "35.34588599268628 1.1937766417144369 9\n",
      "The training loss is 33.05102754389423 with std:46.67129852445949. The val loss is 29.485488189177428 with std:41.1225228735367.\n",
      "29.485488189177428 1.1937766417144369 9\n",
      "The training loss is 32.16309887888608 with std:44.01176730412092. The val loss is 31.355477411141525 with std:46.707955786519015.\n",
      "31.355477411141525 1.1937766417144369 9\n",
      "Evaluating for {'degree': 9, 'lmda': 1.5570684047537318} ...\n",
      "The training loss is 30.306356327255386 with std:44.47954270568021. The val loss is 35.487100782053936 with std:48.468849041162294.\n",
      "35.487100782053936 1.5570684047537318 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 33.19428204721 with std:46.916866054844995. The val loss is 29.592609297216406 with std:41.41475412829198.\n",
      "29.592609297216406 1.5570684047537318 9\n",
      "The training loss is 32.26110130553252 with std:44.29402536024958. The val loss is 31.449356893992938 with std:46.79986910614339.\n",
      "31.449356893992938 1.5570684047537318 9\n",
      "Evaluating for {'degree': 9, 'lmda': 2.030917620904737} ...\n",
      "The training loss is 30.425989087972408 with std:44.75005275782234. The val loss is 35.62569920313546 with std:48.85171718253334.\n",
      "35.62569920313546 2.030917620904737 9\n",
      "The training loss is 33.32605575006761 with std:47.20347659908276. The val loss is 29.69795525731427 with std:41.73368916567673.\n",
      "29.69795525731427 2.030917620904737 9\n",
      "The training loss is 32.36045561424724 with std:44.62140440782699. The val loss is 31.528959407954318 with std:46.926000140110425.\n",
      "31.528959407954318 2.030917620904737 9\n",
      "Evaluating for {'degree': 9, 'lmda': 2.6489692876105297} ...\n",
      "The training loss is 30.535042133865737 with std:45.060242268000216. The val loss is 35.76247656204626 with std:49.29581935762782.\n",
      "35.76247656204626 2.6489692876105297 9\n",
      "The training loss is 33.4483016361414 with std:47.543584098587125. The val loss is 29.80158792574701 with std:42.09430738911658.\n",
      "29.80158792574701 2.6489692876105297 9\n",
      "The training loss is 32.461987031546556 with std:45.00747474160724. The val loss is 31.5968239086756 with std:47.09975350230332.\n",
      "31.5968239086756 2.6489692876105297 9\n",
      "Evaluating for {'degree': 9, 'lmda': 3.4551072945922217} ...\n",
      "The training loss is 30.638236013689546 with std:45.4306860245683. The val loss is 35.901879696449654 with std:49.820467093032285.\n",
      "35.901879696449654 3.4551072945922217 9\n",
      "The training loss is 33.56539853369255 with std:47.955477376201294. The val loss is 29.906145764281373 with std:42.51821308333555.\n",
      "29.906145764281373 3.4551072945922217 9\n",
      "The training loss is 32.56815755327425 with std:45.47123082700588. The val loss is 31.656861524115012 with std:47.33857669316079.\n",
      "31.656861524115012 3.4551072945922217 9\n",
      "Evaluating for {'degree': 9, 'lmda': 4.506570337745478} ...\n",
      "The training loss is 30.74362059694701 with std:45.88901662869248. The val loss is 36.053062852811195 with std:50.45497493470216.\n",
      "36.053062852811195 4.506570337745478 9\n",
      "The training loss is 33.68511329942144 with std:48.465278423070245. The val loss is 30.017874842153297 with std:43.03520568579441.\n",
      "30.017874842153297 4.506570337745478 9\n",
      "The training loss is 32.68454776488681 with std:46.03955320009959. The val loss is 31.715188805651106 with std:47.66549755728452.\n",
      "31.715188805651106 4.506570337745478 9\n",
      "Evaluating for {'degree': 9, 'lmda': 5.878016072274912} ...\n",
      "The training loss is 30.863957111100095 with std:46.47237055338016. The val loss is 36.231599338589824 with std:51.241395343762974.\n",
      "36.231599338589824 5.878016072274912 9\n",
      "The training loss is 33.820238108896824 with std:49.10958629759839. The val loss is 30.14822386960734 with std:43.68575775035191.\n",
      "30.14822386960734 5.878016072274912 9\n",
      "The training loss is 32.82196948973445 with std:46.75045617248329. The val loss is 31.781642011546236 with std:48.11148460504053.\n",
      "31.781642011546236 5.878016072274912 9\n",
      "Evaluating for {'degree': 9, 'lmda': 7.666822074546214} ...\n",
      "The training loss is 31.01921113488466 with std:47.23086857817849. The val loss is 36.462416844412076 with std:52.23796688325807.\n",
      "36.462416844412076 7.666822074546214 9\n",
      "The training loss is 33.9913068929818 with std:49.93895593029765. The val loss is 30.31642244238601 with std:44.524624175526256.\n",
      "30.31642244238601 7.666822074546214 9\n",
      "The training loss is 32.999540134154294 with std:47.65721111652702. The val loss is 31.8722954062841 with std:48.718789139094355.\n",
      "31.8722954062841 7.666822074546214 9\n",
      "Evaluating for {'degree': 9, 'lmda': 10.0} ...\n",
      "The training loss is 31.24070751779362 with std:48.23228207749754. The val loss is 36.784660176884984 with std:53.52357084220314.\n",
      "36.784660176884984 10.0 9\n",
      "The training loss is 34.23098792360788 with std:51.02239946252262. The val loss is 30.553641721282045 with std:45.62570796257894.\n",
      "30.553641721282045 10.0 9\n",
      "The training loss is 33.24931821088622 with std:48.83344432455166. The val loss is 32.013494402761495 with std:49.545419405274664.\n",
      "32.013494402761495 10.0 9\n",
      "Evaluating for {'degree': 10, 'lmda': 0.01} ...\n",
      "The training loss is 28.785041735509513 with std:42.37662233169999. The val loss is 34.295639623272905 with std:46.6828582260108.\n",
      "34.295639623272905 0.01 10\n",
      "The training loss is 31.54683617760968 with std:45.88636241252504. The val loss is 29.11166427353567 with std:39.4482584181503.\n",
      "29.11166427353567 0.01 10\n",
      "The training loss is 31.292963334583572 with std:42.44163328377525. The val loss is 29.692029892568165 with std:46.52247469243978.\n",
      "29.692029892568165 0.01 10\n",
      "Evaluating for {'degree': 10, 'lmda': 0.013043213867190054} ...\n",
      "The training loss is 28.852911824589107 with std:42.44420088727089. The val loss is 34.34122041353864 with std:46.70232047235237.\n",
      "34.34122041353864 0.013043213867190054 10\n",
      "The training loss is 31.596206632449263 with std:45.87568009385774. The val loss is 29.12250751460595 with std:39.49189621998681.\n",
      "29.12250751460595 0.013043213867190054 10\n",
      "The training loss is 31.350896372213377 with std:42.50327611653101. The val loss is 29.786176313669483 with std:46.53256864750057.\n",
      "29.786176313669483 0.013043213867190054 10\n",
      "Evaluating for {'degree': 10, 'lmda': 0.017012542798525893} ...\n",
      "The training loss is 28.91752306402754 with std:42.50839273444534. The val loss is 34.38572300075208 with std:46.721343789425894.\n",
      "34.38572300075208 0.017012542798525893 10\n",
      "The training loss is 31.64871745800415 with std:45.85683603948502. The val loss is 29.12076007257515 with std:39.52544965245148.\n",
      "29.12076007257515 0.017012542798525893 10\n",
      "The training loss is 31.404600340030147 with std:42.56053454677314. The val loss is 29.885230404321383 with std:46.53263785797339.\n",
      "29.885230404321383 0.017012542798525893 10\n",
      "Evaluating for {'degree': 10, 'lmda': 0.02218982341458972} ...\n",
      "The training loss is 28.9837585089323 with std:42.5747232049823. The val loss is 34.43211004459838 with std:46.74283797385897.\n",
      "34.43211004459838 0.02218982341458972 10\n",
      "The training loss is 31.708849090607703 with std:45.83422943487657. The val loss is 29.11125200180678 with std:39.55599060319149.\n",
      "29.11125200180678 0.02218982341458972 10\n",
      "The training loss is 31.457499063317943 with std:42.61739350988959. The val loss is 29.9933841360678 with std:46.526862859513955.\n",
      "29.9933841360678 0.02218982341458972 10\n",
      "Evaluating for {'degree': 10, 'lmda': 0.028942661247167517} ...\n",
      "The training loss is 29.056284077598406 with std:42.648702414139606. The val loss is 34.48319536148299 with std:46.770047933920274.\n",
      "34.48319536148299 0.028942661247167517 10\n",
      "The training loss is 31.78110214785999 with std:45.81329373108206. The val loss is 29.09948092931177 with std:39.59117851658627.\n",
      "29.09948092931177 0.028942661247167517 10\n",
      "The training loss is 31.51269129727758 with std:42.677668648803646. The val loss is 30.11397611531185 with std:46.51985068983064.\n",
      "30.11397611531185 0.028942661247167517 10\n",
      "Evaluating for {'degree': 10, 'lmda': 0.037750532053243954} ...\n",
      "The training loss is 29.139063974493855 with std:42.735287103911205. The val loss is 34.54126909532826 with std:46.80612051989556.\n",
      "34.54126909532826 0.037750532053243954 10\n",
      "The training loss is 31.869372779687545 with std:45.80007335125678. The val loss is 29.091243364953 with std:39.638624587345646.\n",
      "29.091243364953 0.037750532053243954 10\n",
      "The training loss is 31.57258170360177 with std:42.74459686979956. The val loss is 30.248847439203388 with std:46.516153550933645.\n",
      "30.248847439203388 0.037750532053243954 10\n",
      "Evaluating for {'degree': 10, 'lmda': 0.04923882631706739} ...\n",
      "The training loss is 29.23473753404389 with std:42.83814144196138. The val loss is 34.607670668994636 with std:46.85354788148079.\n",
      "34.607670668994636 0.04923882631706739 10\n",
      "The training loss is 31.976132086014633 with std:45.8004103051841. The val loss is 29.091943825888322 with std:39.70487182158922.\n",
      "29.091943825888322 0.04923882631706739 10\n",
      "The training loss is 31.638499169647094 with std:42.820393496650055. The val loss is 30.397752485679636 with std:46.519564523140744.\n",
      "30.397752485679636 0.04923882631706739 10\n",
      "Evaluating for {'degree': 10, 'lmda': 0.0642232542222936} ...\n",
      "The training loss is 29.34404275991939 with std:42.958891755609535. The val loss is 34.68245731212273 with std:46.913664982058854.\n",
      "34.68245731212273 0.0642232542222936 10\n",
      "The training loss is 32.1016351816561 with std:45.8188882376462. The val loss is 29.105722100890997 with std:39.79423370902892.\n",
      "29.105722100890997 0.0642232542222936 10\n",
      "The training loss is 31.710438573707254 with std:42.905926872457826. The val loss is 30.558063498267856 with std:46.53237596280675.\n",
      "30.558063498267856 0.0642232542222936 10\n",
      "Evaluating for {'degree': 10, 'lmda': 0.0837677640068292} ...\n",
      "The training loss is 29.465551969270514 with std:43.096681812739604. The val loss is 34.76436383431837 with std:46.986470949847906.\n",
      "34.76436383431837 0.0837677640068292 10\n",
      "The training loss is 32.24349528607857 with std:45.85788929300084. The val loss is 29.13470388911664 with std:39.907933720806334.\n",
      "29.13470388911664 0.0837677640068292 10\n",
      "The training loss is 31.787071913815762 with std:43.000686532457806. The val loss is 30.724980404533976 with std:46.55489007257781.\n",
      "30.724980404533976 0.0837677640068292 10\n",
      "Evaluating for {'degree': 10, 'lmda': 0.10926008611173785} ...\n",
      "The training loss is 29.59592099885685 with std:43.248290365341845. The val loss is 34.85118122613237 with std:47.07098400561635.\n",
      "34.85118122613237 0.10926008611173785 10\n",
      "The training loss is 32.3968982585466 with std:45.917182259885124. The val loss is 29.17870720581658 with std:40.04397415101675.\n",
      "29.17870720581658 0.10926008611173785 10\n",
      "The training loss is 31.866089980293047 with std:43.10313665318558. The val loss is 30.892283567428596 with std:46.58542264841841.\n",
      "30.892283567428596 0.10926008611173785 10\n",
      "Evaluating for {'degree': 10, 'lmda': 0.14251026703029984} ...\n",
      "The training loss is 29.730640711264396 with std:43.40885259532066. The val loss is 34.94049714068033 with std:47.16610317933164.\n",
      "34.94049714068033 0.14251026703029984 10\n",
      "The training loss is 32.555484119381134 with std:45.99428143128465. The val loss is 29.23556450417761 with std:40.197876739387596.\n",
      "29.23556450417761 0.14251026703029984 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 31.944794449156948 with std:43.21138534524997. The val loss is 31.053429974829704 with std:46.62084010105275.\n",
      "31.053429974829704 0.14251026703029984 10\n",
      "Evaluating for {'degree': 10, 'lmda': 0.18587918911465645} ...\n",
      "The training loss is 29.865037627225746 with std:43.57296033632927. The val loss is 35.03053619147333 with std:47.271660574458274.\n",
      "35.03053619147333 0.18587918911465645 10\n",
      "The training loss is 32.7126194841289 with std:46.085473897486644. The val loss is 29.30193744712865 with std:40.364035728384025.\n",
      "29.30193744712865 0.18587918911465645 10\n",
      "The training loss is 32.02075297419392 with std:43.32396442489633. The val loss is 31.202645054259616 with std:46.65744207254403.\n",
      "31.202645054259616 0.18587918911465645 10\n",
      "Evaluating for {'degree': 10, 'lmda': 0.24244620170823283} ...\n",
      "The training loss is 29.995159647138685 with std:43.73578669654599. The val loss is 35.120744563838464 with std:47.389206024479854.\n",
      "35.120744563838464 0.24244620170823283 10\n",
      "The training loss is 32.86262468667119 with std:46.18712768276603. The val loss is 29.374291611204264 with std:40.537174059323235.\n",
      "29.374291611204264 0.24244620170823283 10\n",
      "The training loss is 32.09232331599048 with std:43.44049025822548. The val loss is 31.335696596759533 with std:46.691897022790656.\n",
      "31.335696596759533 0.24244620170823283 10\n",
      "Evaluating for {'degree': 10, 'lmda': 0.31622776601683794} ...\n",
      "The training loss is 30.11826869637027 with std:43.89396075836749. The val loss is 35.21185073598907 with std:47.52218156451512.\n",
      "35.21185073598907 0.31622776601683794 10\n",
      "The training loss is 33.001589751649234 with std:46.29683632524574. The val loss is 29.449682890699922 with std:40.71344259724679.\n",
      "29.449682890699922 0.31622776601683794 10\n",
      "The training loss is 32.158934032626554 with std:43.56206287928657. The val loss is 31.450211695570175 with std:46.72199990853536.\n",
      "31.450211695570175 0.31622776601683794 10\n",
      "Evaluating for {'degree': 10, 'lmda': 0.41246263829013524} ...\n",
      "The training loss is 30.232872292069583 with std:44.04610716759011. The val loss is 35.30536527724827 with std:47.67544093417866.\n",
      "35.30536527724827 0.41246263829013524 10\n",
      "The training loss is 33.127632959817134 with std:46.414116488676996. The val loss is 29.526154922343398 with std:40.89097039315119.\n",
      "29.526154922343398 0.41246263829013524 10\n",
      "The training loss is 32.22110991270681 with std:43.691384528402864. The val loss is 31.545586369027635 with std:46.7471693148462.\n",
      "31.545586369027635 0.41246263829013524 10\n",
      "Evaluating for {'degree': 10, 'lmda': 0.5379838403443686} ...\n",
      "The training loss is 30.338427692460307 with std:44.193105249667546. The val loss is 35.402747251769306 with std:47.85436366961697.\n",
      "35.402747251769306 0.5379838403443686 10\n",
      "The training loss is 33.24067279501111 with std:46.540611184657365. The val loss is 29.602736328861457 with std:41.069961868614946.\n",
      "29.602736328861457 0.5379838403443686 10\n",
      "The training loss is 32.28029211064692 with std:43.83266881232817. The val loss is 31.622642644812203 with std:46.76872449243261.\n",
      "31.622642644812203 0.5379838403443686 10\n",
      "Evaluating for {'degree': 10, 'lmda': 0.701703828670383} ...\n",
      "The training loss is 30.434950701341002 with std:44.33815127025716. The val loss is 35.50463297455731 with std:48.0639691876599.\n",
      "35.50463297455731 0.701703828670383 10\n",
      "The training loss is 33.341905597917616 with std:46.679932267482386. The val loss is 29.679162130270374 with std:41.252595500615996.\n",
      "29.679162130270374 0.701703828670383 10\n",
      "The training loss is 32.338518025103845 with std:43.99144749511498. The val loss is 31.68319442834519 with std:46.79002661867564.\n",
      "31.68319442834519 0.701703828670383 10\n",
      "Evaluating for {'degree': 10, 'lmda': 0.9152473108773893} ...\n",
      "The training loss is 30.5227376579314 with std:44.48669276044104. The val loss is 35.61050090066015 with std:48.308445687172785.\n",
      "35.61050090066015 0.9152473108773893 10\n",
      "The training loss is 33.43320768256971 with std:46.83736220520447. The val loss is 29.75550212459979 with std:41.44297847804138.\n",
      "29.75550212459979 0.9152473108773893 10\n",
      "The training loss is 32.398025648916935 with std:44.174385262991265. The val loss is 31.729635746389054 with std:46.816551134429766.\n",
      "31.729635746389054 0.9152473108773893 10\n",
      "Evaluating for {'degree': 10, 'lmda': 1.1937766417144369} ...\n",
      "The training loss is 30.602307036395075 with std:44.646330901322045. The val loss is 35.71893918640364 with std:48.591414921064015.\n",
      "35.71893918640364 1.1937766417144369 10\n",
      "The training loss is 33.51665463409708 with std:47.01965035203442. The val loss is 29.83187924921466 with std:41.64731159794423.\n",
      "29.83187924921466 1.1937766417144369 10\n",
      "The training loss is 32.460865381934404 with std:44.38921290354186. The val loss is 31.76461563046395 with std:46.855931020476156.\n",
      "31.76461563046395 1.1937766417144369 10\n",
      "Evaluating for {'degree': 10, 'lmda': 1.5570684047537318} ...\n",
      "The training loss is 30.674559115277464 with std:44.826849337679356. The val loss is 35.82841674112839 with std:48.91706484408774.\n",
      "35.82841674112839 1.5570684047537318 10\n",
      "The training loss is 33.59430221403877 with std:47.23511346640454. The val loss is 29.90842187419778 with std:41.87430949429769.\n",
      "29.90842187419778 1.5570684047537318 10\n",
      "The training loss is 32.52864427779426 with std:44.64490084647738. The val loss is 31.79084196659451 with std:46.91801690805054.\n",
      "31.79084196659451 1.5570684047537318 10\n",
      "Evaluating for {'degree': 10, 'lmda': 2.030917620904737} ...\n",
      "The training loss is 30.741100836096745 with std:45.04055307651711. The val loss is 35.93830456263833 with std:49.29202807962345.\n",
      "35.93830456263833 2.030917620904737 10\n",
      "The training loss is 33.668312902927475 with std:47.49419783940515. The val loss is 29.985528885415945 with std:42.13587750250473.\n",
      "29.985528885415945 2.030917620904737 10\n",
      "The training loss is 32.60256281034035 with std:44.9522164073806. The val loss is 31.811062561661245 with std:47.01504725875387.\n",
      "31.811062561661245 2.030917620904737 10\n",
      "Evaluating for {'degree': 10, 'lmda': 2.6489692876105297} ...\n",
      "The training loss is 30.804708960178687 with std:45.30306136368402. The val loss is 36.049935983944735 with std:49.72769909692907.\n",
      "36.049935983944735 2.6489692876105297 10\n",
      "The training loss is 33.74145501939321 with std:47.810591298106026. The val loss is 30.064461720436615 with std:42.44808578043301.\n",
      "30.064461720436615 2.6489692876105297 10\n",
      "The training loss is 32.683893882066464 with std:45.324825698449494. The val loss is 31.82829084211393 with std:47.16208414163951.\n",
      "31.82829084211393 2.6489692876105297 10\n",
      "Evaluating for {'degree': 10, 'lmda': 3.4551072945922217} ...\n",
      "The training loss is 30.869986853881493 with std:45.634644977962495. The val loss is 36.167682690744314 with std:50.24270963394181.\n",
      "36.167682690744314 3.4551072945922217 10\n",
      "The training loss is 33.81798163504112 with std:48.20291305273548. The val loss is 30.148256530869148 with std:42.83256146379917.\n",
      "30.148256530869148 3.4551072945922217 10\n",
      "The training loss is 32.774998381244714 with std:45.781094061605685. The val loss is 31.846366572938223 with std:47.37791059008759.\n",
      "31.846366572938223 3.4551072945922217 10\n",
      "Evaluating for {'degree': 10, 'lmda': 4.506570337745478} ...\n",
      "The training loss is 30.94437585604286 with std:46.062191921916316. The val loss is 36.30024856279007 with std:50.865509229657555.\n",
      "36.30024856279007 4.506570337745478 10\n",
      "The training loss is 33.90494784070039 with std:48.696994760008316. The val loss is 30.243006964185806 with std:43.31848549551674.\n",
      "30.243006964185806 4.506570337745478 10\n",
      "The training loss is 32.880930222711264 with std:46.346698170664304. The val loss is 31.870977747541403 with std:47.686589116658745.\n",
      "31.870977747541403 4.506570337745478 10\n",
      "Evaluating for {'degree': 10, 'lmda': 5.878016072274912} ...\n",
      "The training loss is 31.03979598303226 with std:46.62193153159855. The val loss is 36.46257542210417 with std:51.637267634885944.\n",
      "36.46257542210417 5.878016072274912 10\n",
      "The training loss is 34.01415459520695 with std:49.32881445213201. The val loss is 30.359704646275713 with std:43.945410094970214.\n",
      "30.359704646275713 5.878016072274912 10\n",
      "The training loss is 33.011734797727584 with std:47.05811435762155. The val loss is 31.911341201677498 with std:48.11986378859922.\n",
      "31.911341201677498 5.878016072274912 10\n",
      "Evaluating for {'degree': 10, 'lmda': 7.666822074546214} ...\n",
      "The training loss is 31.17532000453737 with std:47.36309850635415. The val loss is 36.678914286401636 with std:52.61546680354769.\n",
      "36.678914286401636 7.666822074546214 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 34.16509575259076 with std:50.148223430266. The val loss is 30.517013506021765 with std:44.76710464672832.\n",
      "30.517013506021765 7.666822074546214 10\n",
      "The training loss is 33.185722224256445 with std:47.96702850474741. The val loss is 31.982865829259026 with std:48.72057103348736.\n",
      "31.982865829259026 7.666822074546214 10\n",
      "Evaluating for {'degree': 10, 'lmda': 10.0} ...\n",
      "The training loss is 31.381442906829346 with std:48.35270391575667. The val loss is 36.98777815704232 with std:53.87850040403725.\n",
      "36.98777815704232 10.0 10\n",
      "The training loss is 34.38950464685644 with std:51.223632141647634. The val loss is 30.745565662244875 with std:45.85656357440637.\n",
      "30.745565662244875 10.0 10\n",
      "The training loss is 33.43426796492658 with std:49.14570062456474. The val loss is 32.1113045015386 with std:49.54719270780473.\n",
      "32.1113045015386 10.0 10\n",
      "Evaluating for {'degree': 11, 'lmda': 0.01} ...\n",
      "The training loss is 29.194503799789082 with std:42.77290220644514. The val loss is 34.56735324150704 with std:46.88547420060448.\n",
      "34.56735324150704 0.01 11\n",
      "The training loss is 31.93065044300642 with std:45.880679313393706. The val loss is 29.192580844300434 with std:39.78461573754007.\n",
      "29.192580844300434 0.01 11\n",
      "The training loss is 31.634630867565278 with std:42.838281799688154. The val loss is 30.270979734139377 with std:46.49036252060055.\n",
      "30.270979734139377 0.01 11\n",
      "Evaluating for {'degree': 11, 'lmda': 0.013043213867190054} ...\n",
      "The training loss is 29.282106324035247 with std:42.86211810174896. The val loss is 34.62686909286233 with std:46.91182387522818.\n",
      "34.62686909286233 0.013043213867190054 11\n",
      "The training loss is 32.021820304880386 with std:45.870060402925745. The val loss is 29.191818202401738 with std:39.83331929641872.\n",
      "29.191818202401738 0.013043213867190054 11\n",
      "The training loss is 31.696830578583388 with std:42.90102224499715. The val loss is 30.410338714943414 with std:46.49928885157051.\n",
      "30.410338714943414 0.013043213867190054 11\n",
      "Evaluating for {'degree': 11, 'lmda': 0.017012542798525893} ...\n",
      "The training loss is 29.381745864674755 with std:42.96587429899344. The val loss is 34.69522952647657 with std:46.94872443832252.\n",
      "34.69522952647657 0.017012542798525893 11\n",
      "The training loss is 32.13032032856954 with std:45.87228974675302. The val loss is 29.199516211322326 with std:39.898345967876764.\n",
      "29.199516211322326 0.017012542798525893 11\n",
      "The training loss is 31.764292622594418 with std:42.97056826304531. The val loss is 30.562383375338896 with std:46.51615797338848.\n",
      "30.562383375338896 0.017012542798525893 11\n",
      "Evaluating for {'degree': 11, 'lmda': 0.02218982341458972} ...\n",
      "The training loss is 29.49373721548593 with std:43.085075558636845. The val loss is 34.77251054990797 with std:46.99766248484037.\n",
      "34.77251054990797 0.02218982341458972 11\n",
      "The training loss is 32.25596529345954 with std:45.891306161701166. The val loss is 29.21922029587096 with std:39.983046473363714.\n",
      "29.21922029587096 0.02218982341458972 11\n",
      "The training loss is 31.83680001823682 with std:43.04727678245823. The val loss is 30.72425296156982 with std:46.542844488841446.\n",
      "30.72425296156982 0.02218982341458972 11\n",
      "Evaluating for {'degree': 11, 'lmda': 0.028942661247167517} ...\n",
      "The training loss is 29.616103514531687 with std:43.21801784805412. The val loss is 34.857164387247344 with std:47.05843187271869.\n",
      "34.857164387247344 0.028942661247167517 11\n",
      "The training loss is 32.39595104340797 with std:45.92864914994948. The val loss is 29.252325845497698 with std:40.087537746849904.\n",
      "29.252325845497698 0.028942661247167517 11\n",
      "The training loss is 31.91282491158787 with std:43.13004029710404. The val loss is 30.891078271319255 with std:46.579214588765495.\n",
      "30.891078271319255 0.028942661247167517 11\n",
      "Evaluating for {'degree': 11, 'lmda': 0.037750532053243954} ...\n",
      "The training loss is 29.744904360744428 with std:43.36066714789865. The val loss is 34.94626588759499 with std:47.129189723512894.\n",
      "34.94626588759499 0.037750532053243954 11\n",
      "The training loss is 32.54518260787262 with std:45.98315825452216. The val loss is 29.297921165528802 with std:40.20876440447374.\n",
      "29.297921165528802 0.037750532053243954 11\n",
      "The training loss is 31.989901277860202 with std:43.21665813115723. The val loss is 31.056799944776166 with std:46.623242263189475.\n",
      "31.056799944776166 0.037750532053243954 11\n",
      "Evaluating for {'degree': 11, 'lmda': 0.04923882631706739} ...\n",
      "The training loss is 29.87506293132905 with std:43.507542769189456. The val loss is 35.03613922701546 with std:47.20701854621175.\n",
      "35.03613922701546 0.04923882631706739 11\n",
      "The training loss is 32.69721149322861 with std:46.0514148448233. The val loss is 29.353228823831355 with std:40.341401251023704.\n",
      "29.353228823831355 0.04923882631706739 11\n",
      "The training loss is 32.06520513128964 with std:43.30448245876615. The val loss is 31.215248737878632 with std:46.67159136991472.\n",
      "31.215248737878632 0.04923882631706739 11\n",
      "Evaluating for {'degree': 11, 'lmda': 0.0642232542222936} ...\n",
      "The training loss is 30.001426245523085 with std:43.6529141787181. The val loss is 35.12317532897498 with std:47.288826141249615.\n",
      "35.12317532897498 0.0642232542222936 11\n",
      "The training loss is 32.84547284687116 with std:46.128761744214074. The val loss is 29.414474876844295 with std:40.479275168800356.\n",
      "29.414474876844295 0.0642232542222936 11\n",
      "The training loss is 32.13615018159127 with std:43.39112671663607. The val loss is 31.361142207222432 with std:46.72043078522485.\n",
      "31.361142207222432 0.0642232542222936 11\n",
      "Evaluating for {'degree': 11, 'lmda': 0.0837677640068292} ...\n",
      "The training loss is 30.119711563012366 with std:43.79190413024641. The val loss is 35.204588889818034 with std:47.37229354329011.\n",
      "35.204588889818034 0.0837677640068292 11\n",
      "The training loss is 32.98440644629475 with std:46.21050539261855. The val loss is 29.477851153889336 with std:40.61679118661482.\n",
      "29.477851153889336 0.0837677640068292 11\n",
      "The training loss is 32.20082574183939 with std:43.47502791957054. The val loss is 31.49072235962816 with std:46.76618976361247.\n",
      "31.49072235962816 0.0837677640068292 11\n",
      "Evaluating for {'degree': 11, 'lmda': 0.10926008611173785} ...\n",
      "The training loss is 30.22709054763943 with std:43.92119272306631. The val loss is 35.27892957943531 with std:47.45661546835719.\n",
      "35.27892957943531 0.10926008611173785 11\n",
      "The training loss is 33.1101577048772 with std:46.29290941005603. The val loss is 29.540265121547993 with std:40.74993993966813.\n",
      "29.540265121547993 0.10926008611173785 11\n",
      "The training loss is 32.25820053805361 with std:43.55575659216555. The val loss is 31.601943117841994 with std:46.80605666614964.\n",
      "31.601943117841994 0.10926008611173785 11\n",
      "Evaluating for {'degree': 11, 'lmda': 0.14251026703029984} ...\n",
      "The training loss is 30.32234844107068 with std:44.03923831668985. The val loss is 35.346285672273 with std:47.542907894014476.\n",
      "35.346285672273 0.14251026703029984 11\n",
      "The training loss is 33.220780004657215 with std:46.37377513300759. The val loss is 29.599733547017752 with std:40.87672661782874.\n",
      "29.599733547017752 0.14251026703029984 11\n",
      "The training loss is 32.308119725699946 with std:43.6340889302481. The val loss is 31.694291271500138 with std:46.838183550965425.\n",
      "31.694291271500138 0.14251026703029984 11\n",
      "Evaluating for {'degree': 11, 'lmda': 0.18587918911465645} ...\n",
      "The training loss is 30.405705540651937 with std:44.146126359906724. The val loss is 35.40821076486041 with std:47.63427880803858.\n",
      "35.40821076486041 0.18587918911465645 11\n",
      "The training loss is 33.31604847232405 with std:46.452619166574664. The val loss is 29.655443387781283 with std:40.997106087160674.\n",
      "29.655443387781283 0.18587918911465645 11\n",
      "The training loss is 32.35117972686124 with std:43.71192913610502. The val loss is 31.768409541172772 with std:46.8616810223913.\n",
      "31.768409541172772 0.18587918911465645 11\n",
      "Evaluating for {'degree': 11, 'lmda': 0.24244620170823283} ...\n",
      "The training loss is 30.478444507959054 with std:44.24324697170239. The val loss is 35.46742800625041 with std:47.73561307475652.\n",
      "35.46742800625041 0.24244620170823283 11\n",
      "The training loss is 33.39707038843947 with std:46.53058373227061. The val loss is 29.707592516685082 with std:41.112627491394115.\n",
      "29.707592516685082 0.24244620170823283 11\n",
      "The training loss is 32.38856817798718 with std:43.79218054733795. The val loss is 31.825683438685914 with std:46.876532343791006.\n",
      "31.825683438685914 0.24244620170823283 11\n",
      "Evaluating for {'degree': 11, 'lmda': 0.31622776601683794} ...\n",
      "The training loss is 30.542468079349103 with std:44.33299211778602. The val loss is 35.52734835089967 with std:47.853123677247304.\n",
      "35.52734835089967 0.31622776601683794 11\n",
      "The training loss is 33.465852274842106 with std:46.61023268777875. The val loss is 29.757127963704775 with std:41.22599178354073.\n",
      "29.757127963704775 0.31622776601683794 11\n",
      "The training loss is 32.4219252510134 with std:43.87863465161909. The val loss is 31.867897719475035 with std:46.883539610991704.\n",
      "31.867897719475035 0.31622776601683794 11\n",
      "Evaluating for {'degree': 11, 'lmda': 0.41246263829013524} ...\n",
      "The training loss is 30.599868969155278 with std:44.418594822788435. The val loss is 35.59143609584348 with std:47.99371548052329.\n",
      "35.59143609584348 0.41246263829013524 11\n",
      "The training loss is 33.52491361295629 with std:46.695338205799175. The val loss is 29.8054589495691 with std:41.340670640019795.\n",
      "29.8054589495691 0.41246263829013524 11\n",
      "The training loss is 32.4532420944285 with std:43.97590708966657. The val loss is 31.89700605252689 with std:46.884371482006124.\n",
      "31.89700605252689 0.41246263829013524 11\n",
      "Evaluating for {'degree': 11, 'lmda': 0.5379838403443686} ...\n",
      "The training loss is 30.65256642106824 with std:44.50414265857772. The val loss is 35.66250492969848 with std:48.164230996479795.\n",
      "35.66250492969848 0.5379838403443686 11\n",
      "The training loss is 33.57697125586281 with std:46.790703702249374. The val loss is 29.854177875195585 with std:41.460679792837304.\n",
      "29.854177875195585 0.5379838403443686 11\n",
      "The training loss is 32.48477739650838 with std:44.08941914508262. The val loss is 31.91501095177683 with std:46.8817350216336.\n",
      "31.91501095177683 0.5379838403443686 11\n",
      "Evaluating for {'degree': 11, 'lmda': 0.701703828670383} ...\n",
      "The training loss is 30.702059613424236 with std:44.59471762443599. The val loss is 35.74211531717926 with std:48.37071402204526.\n",
      "35.74211531717926 0.701703828670383 11\n",
      "The training loss is 33.6246833905259 with std:46.90203314752135. The val loss is 29.904800487939323 with std:41.59056374289949.\n",
      "29.904800487939323 0.701703828670383 11\n",
      "The training loss is 32.51895285438118 with std:44.22541085635778. The val loss is 31.92392435793561 with std:46.87965313791368.\n",
      "31.92392435793561 0.701703828670383 11\n",
      "Evaluating for {'degree': 11, 'lmda': 0.9152473108773893} ...\n",
      "The training loss is 30.749348102007396 with std:44.6965842571835. The val loss is 35.830297595997045 with std:48.61792613618607.\n",
      "35.830297595997045 0.9152473108773893 11\n",
      "The training loss is 33.6704446074611 with std:47.03585956346423. The val loss is 29.958544969713675 with std:41.73562346975637.\n",
      "29.958544969713675 0.9152473108773893 11\n",
      "The training loss is 32.558191669316535 with std:44.39098041201388. The val loss is 31.925770649668117 with std:46.883799018074725.\n",
      "31.925770649668117 0.9152473108773893 11\n",
      "Evaluating for {'degree': 11, 'lmda': 1.1937766417144369} ...\n",
      "The training loss is 30.795051010343307 with std:44.8173960754302. The val loss is 35.92576995472467 with std:48.90943016414392.\n",
      "35.92576995472467 1.1937766417144369 11\n",
      "The training loss is 33.71625668150497 with std:47.19958297532586. The val loss is 30.016200854751016 with std:41.90239019493457.\n",
      "30.016200854751016 1.1937766417144369 11\n",
      "The training loss is 32.60470108752789 with std:44.594173600121415. The val loss is 31.922602985383364 with std:46.901830132249934.\n",
      "31.922602985383364 1.1937766417144369 11\n",
      "Evaluating for {'degree': 11, 'lmda': 1.5570684047537318} ...\n",
      "The training loss is 30.839719110765067 with std:44.96649228367303. The val loss is 36.02665903454577 with std:49.248515600923916.\n",
      "36.02665903454577 1.5570684047537318 11\n",
      "The training loss is 33.76374175322216 with std:47.401721884533345. The val loss is 30.078170286136643 with std:42.09932158528252.\n",
      "30.078170286136643 1.5570684047537318 11\n",
      "The training loss is 32.66027111146157 with std:44.84419482957315. The val loss is 31.91653207918505 with std:46.9436916868836.\n",
      "31.91653207918505 1.5570684047537318 11\n",
      "Evaluating for {'degree': 11, 'lmda': 2.030917620904737} ...\n",
      "The training loss is 30.884304761796127 with std:45.15543648825648. The val loss is 36.13156606727291 with std:49.640028515055036.\n",
      "36.13156606727291 2.030917620904737 11\n",
      "The training loss is 33.814388183291655 with std:47.65252333454587. The val loss is 30.1447703367366 with std:42.33770042099357.\n",
      "30.1447703367366 2.030917620904737 11\n",
      "The training loss is 32.72624360110888 with std:45.151874061849774. The val loss is 31.90981105094511 with std:47.02192972023273.\n",
      "31.90981105094511 2.030917620904737 11\n",
      "Evaluating for {'degree': 11, 'lmda': 2.6489692876105297} ...\n",
      "The training loss is 30.930766935444677 with std:45.3989545164786. The val loss is 36.24077686856008 with std:50.092888814192065.\n",
      "36.24077686856008 2.6489692876105297 11\n",
      "The training loss is 33.87011141666827 with std:47.96507824278017. The val loss is 30.21685928906655 with std:42.632772252025184.\n",
      "30.21685928906655 2.6489692876105297 11\n",
      "The training loss is 32.80385430558629 with std:45.53059090008224. The val loss is 31.905070006338693 with std:47.15215050307007.\n",
      "31.905070006338693 2.6489692876105297 11\n",
      "Evaluating for {'degree': 11, 'lmda': 3.4551072945922217} ...\n",
      "The training loss is 30.98285319024766 with std:45.7163863327972. The val loss is 36.35752940279761 with std:50.622949140601015.\n",
      "36.35752940279761 3.4551072945922217 11\n",
      "The training loss is 33.934193656905634 with std:48.357048037151465. The val loss is 30.29682107281647 with std:43.00524979845477.\n",
      "30.29682107281647 3.4551072945922217 11\n",
      "The training loss is 32.89513346767284 with std:45.99789911170533. The val loss is 31.905836959125537 with std:47.35384630812589.\n",
      "31.905836959125537 3.4551072945922217 11\n",
      "Evaluating for {'degree': 11, 'lmda': 4.506570337745478} ...\n",
      "The training loss is 31.047203507389455 with std:46.133746240268486. The val loss is 36.48946735004819 with std:51.25598704635025.\n",
      "36.48946735004819 4.506570337745478 11\n",
      "The training loss is 34.012684324294256 with std:48.853059284430195. The val loss is 30.38996176015101 with std:43.4833886260656.\n",
      "30.38996176015101 4.506570337745478 11\n",
      "The training loss is 33.00449491744365 with std:46.57807385199404. The val loss is 31.917518556484058 with std:47.65184274031525.\n",
      "31.917518556484058 4.506570337745478 11\n",
      "Evaluating for {'degree': 11, 'lmda': 5.878016072274912} ...\n",
      "The training loss is 31.135044725569873 with std:46.68651751279237. The val loss is 36.65063601481021 with std:52.03092154027041.\n",
      "36.65063601481021 5.878016072274912 11\n",
      "The training loss is 34.11643760871253 with std:49.48781349735491. The val loss is 30.5064755667091 with std:44.105867703763955.\n",
      "30.5064755667091 5.878016072274912 11\n",
      "The training loss is 33.14113306604405 with std:47.305714932109254. The val loss is 31.949067183541974 with std:48.0786065288203.\n",
      "31.949067183541974 5.878016072274912 11\n",
      "Evaluating for {'degree': 11, 'lmda': 7.666822074546214} ...\n",
      "The training loss is 31.264885091972427 with std:47.42335784207142. The val loss is 36.864578349404674 with std:53.00359349846773.\n",
      "36.864578349404674 7.666822074546214 11\n",
      "The training loss is 34.264138463857826 with std:50.31000134358502. The val loss is 30.66432083825264 with std:44.92568753805347.\n",
      "30.66432083825264 7.666822074546214 11\n",
      "The training loss is 33.32246175979487 with std:48.230432798552094. The val loss is 32.015661896930084 with std:48.67760782556851.\n",
      "32.015661896930084 7.666822074546214 11\n",
      "Evaluating for {'degree': 11, 'lmda': 10.0} ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 31.466782435593526 with std:48.4108834263036. The val loss is 37.16927499912615 with std:54.251483057716655.\n",
      "37.16927499912615 10.0 11\n",
      "The training loss is 34.48689728091614 with std:51.387141306315065. The val loss is 30.89357514300202 with std:46.01521261766064.\n",
      "30.89357514300202 10.0 11\n",
      "The training loss is 33.57907808113516 with std:49.42255875809318. The val loss is 32.142895331188136 with std:49.507864096712865.\n",
      "32.142895331188136 10.0 11\n",
      "Evaluating for {'degree': 12, 'lmda': 0.01} ...\n",
      "The training loss is 29.726873683834036 with std:43.3083923043864. The val loss is 34.94027601029381 with std:47.145299448515615.\n",
      "34.94027601029381 0.01 12\n",
      "The training loss is 32.52703932797432 with std:45.99330973331116. The val loss is 29.334332055637017 with std:40.229546835196125.\n",
      "29.334332055637017 0.01 12\n",
      "The training loss is 32.00642010610858 with std:43.236921719301286. The val loss is 31.02638530119875 with std:46.59128162752462.\n",
      "31.02638530119875 0.01 12\n",
      "Evaluating for {'degree': 12, 'lmda': 0.013043213867190054} ...\n",
      "The training loss is 29.85479006623898 with std:43.45004142803229. The val loss is 35.026290244833426 with std:47.20672514795836.\n",
      "35.026290244833426 0.013043213867190054 12\n",
      "The training loss is 32.671883867302974 with std:46.04814065296679. The val loss is 29.383580123403974 with std:40.34717906984904.\n",
      "29.383580123403974 0.013043213867190054 12\n",
      "The training loss is 32.08148523971392 with std:43.31650884346456. The val loss is 31.187241498793888 with std:46.64337510191249.\n",
      "31.187241498793888 0.013043213867190054 12\n",
      "Evaluating for {'degree': 12, 'lmda': 0.017012542798525893} ...\n",
      "The training loss is 29.982341143996535 with std:43.59307234716017. The val loss is 35.11266726137993 with std:47.27368700510327.\n",
      "35.11266726137993 0.017012542798525893 12\n",
      "The training loss is 32.81739984636417 with std:46.113954619495296. The val loss is 29.440273555679315 with std:40.47220926439238.\n",
      "29.440273555679315 0.017012542798525893 12\n",
      "The training loss is 32.15380037508775 with std:43.39504567139984. The val loss is 31.3395701469893 with std:46.69861376999721.\n",
      "31.3395701469893 0.017012542798525893 12\n",
      "Evaluating for {'degree': 12, 'lmda': 0.02218982341458972} ...\n",
      "The training loss is 30.10453579660965 with std:43.73171973052122. The val loss is 35.19584899646047 with std:47.342786148027855.\n",
      "35.19584899646047 0.02218982341458972 12\n",
      "The training loss is 32.95749049898602 with std:46.18592149810617. The val loss is 29.500583056271203 with std:40.59837181939114.\n",
      "29.500583056271203 0.02218982341458972 12\n",
      "The training loss is 32.22101858844627 with std:43.470023148335144. The val loss is 31.47883008929029 with std:46.753485683585644.\n",
      "31.47883008929029 0.02218982341458972 12\n",
      "Evaluating for {'degree': 12, 'lmda': 0.028942661247167517} ...\n",
      "The training loss is 30.217315679704797 with std:43.86122938623108. The val loss is 35.27292468920509 with std:47.411053526167755.\n",
      "35.27292468920509 0.028942661247167517 12\n",
      "The training loss is 33.08719090213428 with std:46.25934110086771. The val loss is 29.560824635688334 with std:40.72015875913341.\n",
      "29.560824635688334 0.028942661247167517 12\n",
      "The training loss is 32.28146730822743 with std:43.539675550904335. The val loss is 31.602024132092236 with std:46.80491782447214.\n",
      "31.602024132092236 0.028942661247167517 12\n",
      "Evaluating for {'degree': 12, 'lmda': 0.037750532053243954} ...\n",
      "The training loss is 30.318023658210556 with std:43.9784497255275. The val loss is 35.34199623036782 with std:47.47648109483766.\n",
      "35.34199623036782 0.037750532053243954 12\n",
      "The training loss is 33.20320280529928 with std:46.330463117830575. The val loss is 29.618088967002873 with std:40.83365108779724.\n",
      "29.618088967002873 0.037750532053243954 12\n",
      "The training loss is 32.33427181226017 with std:43.60318377787388. The val loss is 31.7077669727762 with std:46.850638250148755.\n",
      "31.7077669727762 0.037750532053243954 12\n",
      "Evaluating for {'degree': 12, 'lmda': 0.04923882631706739} ...\n",
      "The training loss is 30.405472947431342 with std:44.08195020111278. The val loss is 35.402267558158 with std:47.53826701135182.\n",
      "35.402267558158 0.04923882631706739 12\n",
      "The training loss is 33.30396278270604 with std:46.39688313366824. The val loss is 29.67051876230122 with std:40.936798935460565.\n",
      "29.67051876230122 0.04923882631706739 12\n",
      "The training loss is 32.37928981682117 with std:43.66065863234448. The val loss is 31.796036339080462 with std:46.88924685420003.\n",
      "31.796036339080462 0.04923882631706739 12\n",
      "Evaluating for {'degree': 12, 'lmda': 0.0642232542222936} ...\n",
      "The training loss is 30.479734897094914 with std:44.17179490025139. The val loss is 35.4539424521982 with std:47.5968386501341.\n",
      "35.4539424521982 0.0642232542222936 12\n",
      "The training loss is 33.38937996308582 with std:46.45756941459339. The val loss is 29.71729304695605 with std:41.029277730664774.\n",
      "29.71729304695605 0.0642232542222936 12\n",
      "The training loss is 32.41694332896704 with std:43.7129980424099. The val loss is 31.867771471251125 with std:46.92008754451276.\n",
      "31.867771471251125 0.0642232542222936 12\n",
      "Evaluating for {'degree': 12, 'lmda': 0.0837677640068292} ...\n",
      "The training loss is 30.54180214376513 with std:44.249153410358936. The val loss is 35.49804447225355 with std:47.653767946692824.\n",
      "35.49804447225355 0.0837677640068292 12\n",
      "The training loss is 33.46042646740824 with std:46.512670769295575. The val loss is 29.758442997874774 with std:41.11212464258355.\n",
      "29.758442997874774 0.0837677640068292 12\n",
      "The training loss is 32.44802892956621 with std:43.761712542231834. The val loss is 31.92445975287383 with std:46.94303594684842.\n",
      "31.92445975287383 0.0837677640068292 12\n",
      "Evaluating for {'degree': 12, 'lmda': 0.10926008611173785} ...\n",
      "The training loss is 30.593251942514573 with std:44.31590020348423. The val loss is 35.53624834440881 with std:47.71167912509.\n",
      "35.53624834440881 0.10926008611173785 12\n",
      "The training loss is 33.51872836934379 with std:46.563256451611494. The val loss is 29.79461650202639 with std:41.18733094394138.\n",
      "29.79461650202639 0.10926008611173785 12\n",
      "The training loss is 32.47355965529231 with std:43.80878360168787. The val loss is 31.967795764619687 with std:46.95829202601669.\n",
      "31.967795764619687 0.10926008611173785 12\n",
      "Evaluating for {'degree': 12, 'lmda': 0.14251026703029984} ...\n",
      "The training loss is 30.635976751135427 with std:44.37429237339961. The val loss is 35.57076417107274 with std:47.77420133674999.\n",
      "35.57076417107274 0.14251026703029984 12\n",
      "The training loss is 33.5662403488784 with std:46.61109129409542. The val loss is 29.826866825954717 with std:41.25749483660516.\n",
      "29.826866825954717 0.14251026703029984 12\n",
      "The training loss is 32.49466248822782 with std:43.85658555311428. The val loss is 31.99944423028078 with std:46.96623042733526.\n",
      "31.99944423028078 0.14251026703029984 12\n",
      "Evaluating for {'degree': 12, 'lmda': 0.18587918911465645} ...\n",
      "The training loss is 30.6719973335457 with std:44.42676355003167. The val loss is 35.60426618424467 with std:47.845965171748986.\n",
      "35.60426618424467 0.18587918911465645 12\n",
      "The training loss is 33.60503054342932 with std:46.65849540663068. The val loss is 29.856497200621778 with std:41.32557359648351.\n",
      "29.856497200621778 0.18587918911465645 12\n",
      "The training loss is 32.51253523683302 with std:43.90787711838018. The val loss is 32.02090487292077 with std:46.96733328441372.\n",
      "32.02090487292077 0.18587918911465645 12\n",
      "Evaluating for {'degree': 12, 'lmda': 0.24244620170823283} ...\n",
      "The training loss is 30.703339714416 with std:44.47584365510439. The val loss is 35.63981658672464 with std:47.932596474341544.\n",
      "35.63981658672464 0.24244620170823283 12\n",
      "The training loss is 33.63716626456664 with std:46.708297826859436. The val loss is 29.884961885239775 with std:41.39473330866986.\n",
      "29.884961885239775 0.24244620170823283 12\n",
      "The training loss is 32.52845560178352 with std:43.96585467700198. The val loss is 32.03346269730586 with std:46.962215853688306.\n",
      "32.03346269730586 0.24244620170823283 12\n",
      "Evaluating for {'degree': 12, 'lmda': 0.31622776601683794} ...\n",
      "The training loss is 30.73194185881389 with std:44.52420515994532. The val loss is 35.68071032595151 with std:48.040630442290635.\n",
      "35.68071032595151 0.31622776601683794 12\n",
      "The training loss is 33.664673405431245 with std:46.7638695784401. The val loss is 29.913806196044025 with std:41.4682797659501.\n",
      "29.913806196044025 0.31622776601683794 12\n",
      "The training loss is 32.54382916987791 with std:44.03425288272572. The val loss is 32.03820462556656 with std:46.95175085691729.\n",
      "32.03820462556656 0.31622776601683794 12\n",
      "Evaluating for {'degree': 12, 'lmda': 0.41246263829013524} ...\n",
      "The training loss is 30.75955767281245 with std:44.5748318846353. The val loss is 35.73017283441822 with std:48.17726089711411.\n",
      "35.73017283441822 0.41246263829013524 12\n",
      "The training loss is 33.68953505898701 with std:46.829208717525546. The val loss is 29.944619218614598 with std:41.54965920851316.\n",
      "29.944619218614598 0.41246263829013524 12\n",
      "The training loss is 32.56025709925353 with std:44.11747503620082. The val loss is 32.03608526969854 with std:46.93729590104264.\n",
      "32.03608526969854 0.41246263829013524 12\n",
      "Evaluating for {'degree': 12, 'lmda': 0.5379838403443686} ...\n",
      "The training loss is 30.787644769909317 with std:44.63129054024909. The val loss is 35.790897446366515 with std:48.34986529045425.\n",
      "35.790897446366515 0.5379838403443686 12\n",
      "The training loss is 33.71369324351764 with std:46.90904438399498. The val loss is 29.97897121592677 with std:41.642535324350916.\n",
      "29.97897121592677 0.5379838403443686 12\n",
      "The training loss is 32.57959552368299 with std:44.220733913622716. The val loss is 32.02802558756039 with std:46.92102288309914.\n",
      "32.02802558756039 0.5379838403443686 12\n",
      "Evaluating for {'degree': 12, 'lmda': 0.701703828670383} ...\n",
      "The training loss is 30.817255425682095 with std:44.69805573247094. The val loss is 35.86451281722461 with std:48.56532267570337.\n",
      "35.86451281722461 0.701703828670383 12\n",
      "The training loss is 33.73902262298738 with std:47.008930102027065. The val loss is 30.01831523102394 with std:41.75097062051986.\n",
      "30.01831523102394 0.701703828670383 12\n",
      "The training loss is 32.603968948318965 with std:44.35018479541406. The val loss is 32.01502600617659 with std:46.90633434177039.\n",
      "32.01502600617659 0.701703828670383 12\n",
      "Evaluating for {'degree': 12, 'lmda': 0.9152473108773893} ...\n",
      "The training loss is 30.848984700975553 with std:44.78082049737675. The val loss is 35.95118160470822 with std:48.82928468898493.\n",
      "35.95118160470822 0.9152473108773893 12\n",
      "The training loss is 33.76726286301519 with std:47.13531516038593. The val loss is 30.063854482011603 with std:41.87975376407813.\n",
      "30.063854482011603 0.9152473108773893 12\n",
      "The training loss is 32.635698039299584 with std:44.51304029223413. The val loss is 31.998272193946327 with std:46.89833057148232.\n",
      "31.998272193946327 0.9152473108773893 12\n",
      "Evaluating for {'degree': 12, 'lmda': 1.1937766417144369} ...\n",
      "The training loss is 30.88304502542011 with std:44.88675815965028. The val loss is 36.04957134753487 with std:49.14574474602019.\n",
      "36.04957134753487 1.1937766417144369 12\n",
      "The training loss is 33.79993262967078 with std:47.2956240714272. The val loss is 30.11641666651275 with std:42.0349081347654.\n",
      "30.11641666651275 1.1937766417144369 12\n",
      "The training loss is 32.677124146847596 with std:44.71767682811851. The val loss is 31.97921383996183 with std:46.90427411474057.\n",
      "31.97921383996183 1.1937766417144369 12\n",
      "Evaluating for {'degree': 12, 'lmda': 1.5570684047537318} ...\n",
      "The training loss is 30.919519121389 with std:45.02479975577033. The val loss is 36.157350323117576 with std:49.51736809550673.\n",
      "36.157350323117576 1.5570684047537318 12\n",
      "The training loss is 33.838299100228795 with std:47.498437764472776. The val loss is 30.176424588955342 with std:42.2243932358284.\n",
      "30.176424588955342 1.5570684047537318 12\n",
      "The training loss is 32.73037325833892 with std:44.97378160788765. The val loss is 31.95961463744544 with std:46.93400608647582.\n",
      "31.95961463744544 1.5570684047537318 12\n",
      "Evaluating for {'degree': 12, 'lmda': 2.030917620904737} ...\n",
      "The training loss is 30.958805606368234 with std:45.20610044562184. The val loss is 36.27218222462187 with std:49.946941509954264.\n",
      "36.27218222462187 2.030917620904737 12\n",
      "The training loss is 33.883522418701375 with std:47.753940843787255. The val loss is 30.244086720760848 with std:42.458998239774516.\n",
      "30.244086720760848 2.030917620904737 12\n",
      "The training loss is 32.797196330439185 with std:45.29265357329069. The val loss is 31.941612150354395 with std:47.000322341947665.\n",
      "31.941612150354395 2.030917620904737 12\n",
      "Evaluating for {'degree': 12, 'lmda': 2.6489692876105297} ...\n",
      "The training loss is 31.002250895062666 with std:45.444908038464405. The val loss is 36.393050455989474 with std:50.43995591803696.\n",
      "36.393050455989474 2.6489692876105297 12\n",
      "The training loss is 33.937110264332816 with std:48.07484526132593. The val loss is 30.31992895086467 with std:42.75346415619104.\n",
      "30.31992895086467 2.6489692876105297 12\n",
      "The training loss is 32.879113774298084 with std:45.68786420220475. The val loss is 31.92788490457538 with std:47.11941693523482.\n",
      "31.92788490457538 2.6489692876105297 12\n",
      "Evaluating for {'degree': 12, 'lmda': 3.4551072945922217} ...\n",
      "The training loss is 31.052991577406694 with std:45.76000062162782. The val loss is 36.52175240821072 with std:51.007972354171024.\n",
      "36.52175240821072 3.4551072945922217 12\n",
      "The training loss is 34.00180144054892 with std:48.47799033566039. The val loss is 30.40575562238881 with std:43.12796207466558.\n",
      "30.40575562238881 3.4551072945922217 12\n",
      "The training loss is 32.97812577078351 with std:46.176581116182874. The val loss is 31.92208452614241 with std:47.311612061842.\n",
      "31.92208452614241 3.4551072945922217 12\n",
      "Evaluating for {'degree': 12, 'lmda': 4.506570337745478} ...\n",
      "The training loss is 31.117120668313433 with std:46.17679701684709. The val loss is 36.66457923596987 with std:51.67233420828348.\n",
      "36.66457923596987 4.506570337745478 12\n",
      "The training loss is 34.08298653670726 with std:48.98675212298717. The val loss is 30.50611012509362 with std:43.610149452126024.\n",
      "30.50611012509362 4.506570337745478 12\n",
      "The training loss is 33.098209777324236 with std:46.78189740869682. The val loss is 31.92973973601436 with std:47.602665852187855.\n",
      "31.92973973601436 4.506570337745478 12\n",
      "Evaluating for {'degree': 12, 'lmda': 5.878016072274912} ...\n",
      "The training loss is 31.205423050853316 with std:46.73023556783892. The val loss is 36.834458225779144 with std:52.46804219400831.\n",
      "36.834458225779144 5.878016072274912 12\n",
      "The training loss is 34.19082571570534 with std:49.63431936974804. The val loss is 30.63036257102264 with std:44.2380636108447.\n",
      "30.63036257102264 5.878016072274912 12\n",
      "The training loss is 33.24777221108548 with std:47.536430196101854. The val loss is 31.959886267484343 with std:48.02595052114659.\n",
      "31.959886267484343 5.878016072274912 12\n",
      "Evaluating for {'degree': 12, 'lmda': 7.666822074546214} ...\n",
      "The training loss is 31.336083113481255 with std:47.4685584208262. The val loss is 37.05409811493108 with std:53.44798490191746.\n",
      "37.05409811493108 7.666822074546214 12\n",
      "The training loss is 34.34337278478936 with std:50.46786440811064. The val loss is 30.795712217717135 with std:45.06407258097743.\n",
      "30.795712217717135 7.666822074546214 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 33.443260170195444 with std:48.48726656861331. The val loss is 32.02775621305866 with std:48.62573280530682.\n",
      "32.02775621305866 7.666822074546214 12\n",
      "Evaluating for {'degree': 12, 'lmda': 10.0} ...\n",
      "The training loss is 31.53894447346709 with std:48.45814618347557. The val loss is 37.360910069644085 with std:54.68792071633103.\n",
      "37.360910069644085 10.0 12\n",
      "The training loss is 34.571247207074364 with std:51.55364251900906. The val loss is 31.03163174375271 with std:46.15999009914813.\n",
      "31.03163174375271 10.0 12\n",
      "The training loss is 33.71433641968869 with std:49.7021277224901. The val loss is 32.159003381855456 with std:49.46168624391241.\n",
      "32.159003381855456 10.0 12\n",
      "Evaluating for {'degree': 13, 'lmda': 0.01} ...\n",
      "The training loss is 30.29349965048963 with std:43.92405590319424. The val loss is 35.33053209120517 with std:47.46426818517868.\n",
      "35.33053209120517 0.01 13\n",
      "The training loss is 33.17568430277696 with std:46.31085005865997. The val loss is 29.624589156195984 with std:40.814603076614695.\n",
      "29.624589156195984 0.01 13\n",
      "The training loss is 32.34226285587613 with std:43.60201027946048. The val loss is 31.689864541219247 with std:46.83065074541846.\n",
      "31.689864541219247 0.01 13\n",
      "Evaluating for {'degree': 13, 'lmda': 0.013043213867190054} ...\n",
      "The training loss is 30.388484858586946 with std:44.034338866232524. The val loss is 35.39430153367202 with std:47.51938441795311.\n",
      "35.39430153367202 0.013043213867190054 13\n",
      "The training loss is 33.28223640205211 with std:46.37553792890776. The val loss is 29.67867244687713 with std:40.91726079640706.\n",
      "29.67867244687713 0.013043213867190054 13\n",
      "The training loss is 32.390882418382546 with std:43.656597461385886. The val loss is 31.787740297058257 with std:46.87755723399236.\n",
      "31.787740297058257 0.013043213867190054 13\n",
      "Evaluating for {'degree': 13, 'lmda': 0.017012542798525893} ...\n",
      "The training loss is 30.470180913375632 with std:44.12996870813352. The val loss is 35.44943995834511 with std:47.5694242246304.\n",
      "35.44943995834511 0.017012542798525893 13\n",
      "The training loss is 33.373926080793225 with std:46.43421464995553. The val loss is 29.7270237763118 with std:41.00796680627241.\n",
      "29.7270237763118 0.017012542798525893 13\n",
      "The training loss is 32.43193373215181 with std:43.704063190173954. The val loss is 31.869383969671485 with std:46.91761959886166.\n",
      "31.869383969671485 0.017012542798525893 13\n",
      "Evaluating for {'degree': 13, 'lmda': 0.02218982341458972} ...\n",
      "The training loss is 30.53885289415397 with std:44.21118934890311. The val loss is 35.496052445275744 with std:47.614237613954714.\n",
      "35.496052445275744 0.02218982341458972 13\n",
      "The training loss is 33.451000621834716 with std:46.485988653821984. The val loss is 29.76904933812307 with std:41.08650342828735.\n",
      "29.76904933812307 0.02218982341458972 13\n",
      "The training loss is 32.465925328752355 with std:43.74504034543882. The val loss is 31.936041835801525 with std:46.95063526834699.\n",
      "31.936041835801525 0.02218982341458972 13\n",
      "Evaluating for {'degree': 13, 'lmda': 0.028942661247167517} ...\n",
      "The training loss is 30.595516151504853 with std:44.279176805255084. The val loss is 35.534808967171486 with std:47.65440373964016.\n",
      "35.534808967171486 0.028942661247167517 13\n",
      "The training loss is 33.51456489176459 with std:46.530908128528786. The val loss is 29.804859598172296 with std:41.15374578227938.\n",
      "29.804859598172296 0.028942661247167517 13\n",
      "The training loss is 32.493635767764054 with std:43.78056352122785. The val loss is 31.9893760826396 with std:46.9768409432692.\n",
      "31.9893760826396 0.028942661247167517 13\n",
      "Evaluating for {'degree': 13, 'lmda': 0.037750532053243954} ...\n",
      "The training loss is 30.641606632822143 with std:44.33565153546958. The val loss is 35.56673043195895 with std:47.69105226451097.\n",
      "35.56673043195895 0.037750532053243954 13\n",
      "The training loss is 33.56619864964041 with std:46.569675425450306. The val loss is 29.83503256217902 with std:41.2112694016653.\n",
      "29.83503256217902 0.037750532053243954 13\n",
      "The training loss is 32.51596091688098 with std:43.811912733071765. The val loss is 32.031166655352564 with std:46.99670220571574.\n",
      "32.031166655352564 0.037750532053243954 13\n",
      "Evaluating for {'degree': 13, 'lmda': 0.04923882631706739} ...\n",
      "The training loss is 30.678726883268194 with std:44.38257526312483. The val loss is 35.59303260535745 with std:47.725736473326364.\n",
      "35.59303260535745 0.04923882631706739 13\n",
      "The training loss is 33.60766100556893 with std:46.603403375702676. The val loss is 29.860413323838547 with std:41.26103735051955.\n",
      "29.860413323838547 0.04923882631706739 13\n",
      "The training loss is 32.533808932957335 with std:43.840512713958795. The val loss is 32.06310779254613 with std:47.010750047672765.\n",
      "32.06310779254613 0.04923882631706739 13\n",
      "Evaluating for {'degree': 13, 'lmda': 0.0642232542222936} ...\n",
      "The training loss is 30.708481103991435 with std:44.421949411475666. The val loss is 35.61504201696661 with std:47.76038230259488.\n",
      "35.61504201696661 0.0642232542222936 13\n",
      "The training loss is 33.64069537741177 with std:46.633446804543595. The val loss is 29.88197239001418 with std:41.30519141629589.\n",
      "29.88197239001418 0.0642232542222936 13\n",
      "The training loss is 32.54804115400491 with std:43.86788875544048. The val loss is 32.0866882798463 with std:47.01947125746435.\n",
      "32.0866882798463 0.0642232542222936 13\n",
      "Evaluating for {'degree': 13, 'lmda': 0.0837677640068292} ...\n",
      "The training loss is 30.732389292208747 with std:44.45570401744343. The val loss is 35.63418096150886 with std:47.797317155533705.\n",
      "35.63418096150886 0.0837677640068292 13\n",
      "The training loss is 33.66692265262157 with std:46.66131280112885. The val loss is 29.900722253949453 with std:41.345938643932065.\n",
      "29.900722253949453 0.0837677640068292 13\n",
      "The training loss is 32.559450689262846 with std:43.89567150859498. The val loss is 32.10313485115845 with std:47.02324745451365.\n",
      "32.10313485115845 0.0837677640068292 13\n",
      "Evaluating for {'degree': 13, 'lmda': 0.10926008611173785} ...\n",
      "The training loss is 30.751861667145775 with std:44.48565637257132. The val loss is 35.652010491773865 with std:47.83937144200788.\n",
      "35.652010491773865 0.10926008611173785 13\n",
      "The training loss is 33.68780159907961 with std:46.68864006853214. The val loss is 29.91768154461536 with std:41.38551100814642.\n",
      "29.91768154461536 0.10926008611173785 13\n",
      "The training loss is 32.568770130409476 with std:43.925641722871994. The val loss is 32.11339874398093 with std:47.022335246102415.\n",
      "32.11339874398093 0.10926008611173785 13\n",
      "Evaluating for {'degree': 13, 'lmda': 0.14251026703029984} ...\n",
      "The training loss is 30.76821344496023 with std:44.513520730742606. The val loss is 35.670314529370245 with std:47.89003811383636.\n",
      "35.670314529370245 0.14251026703029984 13\n",
      "The training loss is 33.70463648009717 with std:46.71723500164718. The val loss is 29.933873823953363 with std:41.42617435491076.\n",
      "29.933873823953363 0.14251026703029984 13\n",
      "The training loss is 32.57670227714449 with std:43.95980780895645. The val loss is 32.11817058279209 with std:47.01688340912957.\n",
      "32.11817058279209 0.14251026703029984 13\n",
      "Evaluating for {'degree': 13, 'lmda': 0.18587918911465645} ...\n",
      "The training loss is 30.78270088072715 with std:44.540958856754706. The val loss is 35.69119901941932 with std:47.95366509764558.\n",
      "35.69119901941932 0.18587918911465645 13\n",
      "The training loss is 33.71861625654874 with std:46.74915410913628. The val loss is 29.950349088717587 with std:41.470266459111464.\n",
      "29.950349088717587 0.18587918911465645 13\n",
      "The training loss is 32.58397078895335 with std:44.00051170542008. The val loss is 32.11791506509475 with std:47.00698919262336.\n",
      "32.11791506509475 0.18587918911465645 13\n",
      "Evaluating for {'degree': 13, 'lmda': 0.24244620170823283} ...\n",
      "The training loss is 30.796559138106378 with std:44.56967269141092. The val loss is 35.71716431296341 with std:48.03563873289867.\n",
      "35.71716431296341 0.24244620170823283 13\n",
      "The training loss is 33.73087380952494 with std:46.78682473699633. The val loss is 29.968217067445902 with std:41.52024985362343.\n",
      "29.968217067445902 0.24244620170823283 13\n",
      "The training loss is 32.59138963325933 with std:44.05056018320991. The val loss is 32.11292257035784 with std:46.99280286392228.\n",
      "32.11292257035784 0.24244620170823283 13\n",
      "Evaluating for {'degree': 13, 'lmda': 0.31622776601683794} ...\n",
      "The training loss is 30.811020757611796 with std:44.601551789575716. The val loss is 35.75108959543537 with std:48.1424930921746.\n",
      "35.75108959543537 0.31622776601683794 13\n",
      "The training loss is 33.742555045603346 with std:46.833196067810924. The val loss is 29.988680662041553 with std:41.57877252465846.\n",
      "29.988680662041553 0.31622776601683794 13\n",
      "The training loss is 32.599949627948696 with std:44.11337847991343. The val loss is 32.103378801880204 with std:46.974695806921694.\n",
      "32.103378801880204 0.31622776601683794 13\n",
      "Evaluating for {'degree': 13, 'lmda': 0.41246263829013524} ...\n",
      "The training loss is 30.82729358784221 with std:44.63889336646531. The val loss is 35.79605821843414 with std:48.281855562843724.\n",
      "35.79605821843414 0.41246263829013524 13\n",
      "The training loss is 33.754885546447845 with std:46.89190877532732. The val loss is 30.013054848838234 with std:41.6487400544759.\n",
      "30.013054848838234 0.41246263829013524 13\n",
      "The training loss is 32.610915656223106 with std:44.19318058773256. The val loss is 32.08945497287888 with std:46.95351051116163.\n",
      "32.08945497287888 0.41246263829013524 13\n",
      "Evaluating for {'degree': 13, 'lmda': 0.5379838403443686} ...\n",
      "The training loss is 30.846485536240703 with std:44.68470210328529. The val loss is 35.85497563562052 with std:48.46212619109179.\n",
      "35.85497563562052 0.5379838403443686 13\n",
      "The training loss is 33.769217278855464 with std:46.96746518745662. The val loss is 30.042752404507066 with std:41.7334179508679.\n",
      "30.042752404507066 0.5379838403443686 13\n",
      "The training loss is 32.6259180047899 with std:44.29514602285714. The val loss is 32.071418760213064 with std:46.930907290834355.\n",
      "32.071418760213064 0.5379838403443686 13\n",
      "Evaluating for {'degree': 13, 'lmda': 0.701703828670383} ...\n",
      "The training loss is 30.869486617011162 with std:44.74304447227477. The val loss is 35.9300089278471 with std:48.69181357185511.\n",
      "35.9300089278471 0.701703828670383 13\n",
      "The training loss is 33.78703358512652 with std:47.06537629449598. The val loss is 30.079217420653393 with std:41.83660165358455.\n",
      "30.079217420653393 0.701703828670383 13\n",
      "The training loss is 32.64700676155054 with std:44.4255883417796. The val loss is 32.049759596157664 with std:46.90980896663647.\n",
      "32.049759596157664 0.701703828670383 13\n",
      "Evaluating for {'degree': 13, 'lmda': 0.9152473108773893} ...\n",
      "The training loss is 30.896854995055794 with std:44.81939323231109. The val loss is 36.02200100592803 with std:48.97855770434016.\n",
      "36.02200100592803 0.9152473108773893 13\n",
      "The training loss is 33.80989538078325 with std:47.19226572172523. The val loss is 30.1237987672896 with std:41.962906904833524.\n",
      "30.1237987672896 0.9152473108773893 13\n",
      "The training loss is 32.676626051645876 with std:44.59209985633912. The val loss is 32.02531232521436 with std:46.89492039961284.\n",
      "32.02531232521436 0.9152473108773893 13\n",
      "Evaluating for {'degree': 13, 'lmda': 1.1937766417144369} ...\n",
      "The training loss is 30.92878757644567 with std:44.92089706324904. The val loss is 36.1301234958848 with std:49.32809538274164.\n",
      "36.1301234958848 1.1937766417144369 13\n",
      "The training loss is 33.8393358771485 with std:47.35593551234263. The val loss is 30.177587179138328 with std:42.11823232783729.\n",
      "30.177587179138328 1.1937766417144369 13\n",
      "The training loss is 32.71747052589648 with std:44.803665639349724. The val loss is 31.999356854482144 with std:46.89327236073347.\n",
      "31.999356854482144 1.1937766417144369 13\n",
      "Evaluating for {'degree': 13, 'lmda': 1.5570684047537318} ...\n",
      "The training loss is 30.96526381701261 with std:45.05659301376434. The val loss is 36.252039552719374 with std:49.74371005000965.\n",
      "36.252039552719374 1.5570684047537318 13\n",
      "The training loss is 33.87676054072521 with std:47.56545431608892. The val loss is 30.24129246164589 with std:42.3104228919532.\n",
      "30.24129246164589 1.5570684047537318 13\n",
      "The training loss is 32.77223095396138 with std:45.070765943620664. The val loss is 31.97367961564009 with std:46.91472882021993.\n",
      "31.97367961564009 1.5570684047537318 13\n",
      "Evaluating for {'degree': 13, 'lmda': 2.030917620904737} ...\n",
      "The training loss is 31.006424168357242 with std:45.23772621223661. The val loss is 36.38470978300601 with std:50.22684757153627.\n",
      "36.38470978300601 2.030917620904737 13\n",
      "The training loss is 33.923472188094195 with std:47.83141433026923. The val loss is 30.31529506645471 with std:42.55013672468739.\n",
      "30.31529506645471 2.030917620904737 13\n",
      "The training loss is 32.84333024583953 with std:45.405542018432875. The val loss is 31.95061786994922 with std:46.97243088353052.\n",
      "31.95061786994922 2.030917620904737 13\n",
      "Evaluating for {'degree': 13, 'lmda': 2.6489692876105297} ...\n",
      "The training loss is 31.053204852081155 with std:45.47845241193792. The val loss is 36.52576713870248 with std:50.77934677012492.\n",
      "36.52576713870248 2.6489692876105297 13\n",
      "The training loss is 33.98098898939121 with std:48.166595471115876. The val loss is 30.40003786532967 with std:42.85193477650662.\n",
      "30.40003786532967 2.6489692876105297 13\n",
      "The training loss is 32.93287418649277 with std:45.822201706032736. The val loss is 31.93317303468573 with std:47.083244734850126.\n",
      "31.93317303468573 2.6489692876105297 13\n",
      "Evaluating for {'degree': 13, 'lmda': 3.4551072945922217} ...\n",
      "The training loss is 31.108244276067236 with std:45.79718647780738. The val loss is 36.67526716438877 with std:51.40718177027466.\n",
      "36.67526716438877 3.4551072945922217 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 34.051830513549554 with std:48.58732564159978. The val loss is 30.49690911878964 with std:43.23570414535909.\n",
      "30.49690911878964 3.4551072945922217 13\n",
      "The training loss is 33.043133186935535 with std:46.33798566337033. The val loss is 31.925359320052543 with std:47.26841402451926.\n",
      "31.925359320052543 3.4551072945922217 13\n",
      "Evaluating for {'degree': 13, 'lmda': 4.506570337745478} ...\n",
      "The training loss is 31.17713793522918 with std:46.21875268056579. The val loss is 36.837698871610876 with std:52.12514153371837.\n",
      "36.837698871610876 4.506570337745478 13\n",
      "The training loss is 34.14092648170419 with std:49.115788933706604. The val loss is 30.60972975474707 with std:43.728652929797356.\n",
      "30.60972975474707 4.506570337745478 13\n",
      "The training loss is 33.17787603701106 with std:46.975141837461415. The val loss is 31.933024427739902 with std:47.55472998648684.\n",
      "31.933024427739902 4.506570337745478 13\n",
      "Evaluating for {'degree': 13, 'lmda': 5.878016072274912} ...\n",
      "The training loss is 31.27024863571502 with std:46.77740536269483. The val loss is 37.024399074526734 with std:52.961858255272155.\n",
      "37.024399074526734 5.878016072274912 13\n",
      "The training loss is 34.257812984290226 with std:49.78341604950822. The val loss is 30.746964370462624 with std:44.36818432213972.\n",
      "30.746964370462624 5.878016072274912 13\n",
      "The training loss is 33.34480894669291 with std:47.76434876119079. The val loss is 31.96543176227435 with std:47.976565181337875.\n",
      "31.96543176227435 5.878016072274912 13\n",
      "Evaluating for {'degree': 13, 'lmda': 7.666822074546214} ...\n",
      "The training loss is 31.405454413507638 with std:47.52078554059135. The val loss is 37.25684284871629 with std:53.96500763021293.\n",
      "37.25684284871629 7.666822074546214 13\n",
      "The training loss is 34.419886079741644 with std:50.63536458548865. The val loss is 30.9248897926442 with std:45.205908719770896.\n",
      "30.9248897926442 7.666822074546214 13\n",
      "The training loss is 33.55933850643363 with std:48.74981623339004. The val loss is 32.03795544938016 with std:48.57905295143008.\n",
      "32.03795544938016 7.666822074546214 13\n",
      "Evaluating for {'degree': 13, 'lmda': 10.0} ...\n",
      "The training loss is 31.61241416140071 with std:48.514897283816076. The val loss is 37.57159135291932 with std:55.20694556840355.\n",
      "37.57159135291932 10.0 13\n",
      "The training loss is 34.657199919471616 with std:51.73601886380073. The val loss is 31.17218496808295 with std:46.31289104994897.\n",
      "31.17218496808295 10.0 13\n",
      "The training loss is 33.84999331597039 with std:49.995943986050456. The val loss is 32.176354554172065 with std:49.42255755219049.\n",
      "32.176354554172065 10.0 13\n",
      "Evaluating for {'degree': 14, 'lmda': 0.01} ...\n",
      "The training loss is 30.63696410516687 with std:44.311136212031016. The val loss is 35.56574726084315 with std:47.676538056889804.\n",
      "35.56574726084315 0.01 14\n",
      "The training loss is 33.559335788151756 with std:46.55670268473492. The val loss is 29.84160390424132 with std:41.20021786723309.\n",
      "29.84160390424132 0.01 14\n",
      "The training loss is 32.52697708902087 with std:43.80860368697419. The val loss is 32.0356541498883 with std:46.99627232786026.\n",
      "32.0356541498883 0.01 14\n",
      "Evaluating for {'degree': 14, 'lmda': 0.013043213867190054} ...\n",
      "The training loss is 30.678976466428526 with std:44.36163556666814. The val loss is 35.59393599391895 with std:47.70401737486808.\n",
      "35.59393599391895 0.013043213867190054 14\n",
      "The training loss is 33.605060360061955 with std:46.58916597194543. The val loss is 29.868141683959138 with std:41.248384518129235.\n",
      "29.868141683959138 0.013043213867190054 14\n",
      "The training loss is 32.54677598169608 with std:43.83272426047778. The val loss is 32.07383794986068 with std:47.0167905799181.\n",
      "32.07383794986068 0.013043213867190054 14\n",
      "Evaluating for {'degree': 14, 'lmda': 0.017012542798525893} ...\n",
      "The training loss is 30.712555527242994 with std:44.402561179216406. The val loss is 35.61666942074043 with std:47.7279256207886.\n",
      "35.61666942074043 0.017012542798525893 14\n",
      "The training loss is 33.641578139377536 with std:46.61619378741113. The val loss is 29.889849893966492 with std:41.28823081791736.\n",
      "29.889849893966492 0.017012542798525893 14\n",
      "The training loss is 32.56248079019712 with std:43.85317077577734. The val loss is 32.10357295045248 with std:47.03247095984625.\n",
      "32.10357295045248 0.017012542798525893 14\n",
      "Evaluating for {'degree': 14, 'lmda': 0.02218982341458972} ...\n",
      "The training loss is 30.739167866779564 with std:44.435704722988525. The val loss is 35.634948284002284 with std:47.74934184724306.\n",
      "35.634948284002284 0.02218982341458972 14\n",
      "The training loss is 33.67048777300209 with std:46.63883157661087. The val loss is 29.90757722110611 with std:41.32147601973181.\n",
      "29.90757722110611 0.02218982341458972 14\n",
      "The training loss is 32.57485495162873 with std:43.87098387213863. The val loss is 32.12632250361749 with std:47.04395366730448.\n",
      "32.12632250361749 0.02218982341458972 14\n",
      "Evaluating for {'degree': 14, 'lmda': 0.028942661247167517} ...\n",
      "The training loss is 30.760138001648546 with std:44.46271993157752. The val loss is 35.649714050601304 with std:47.76942707577048.\n",
      "35.649714050601304 0.028942661247167517 14\n",
      "The training loss is 33.69322563738206 with std:46.65814159948226. The val loss is 29.922149037112575 with std:41.34978091241007.\n",
      "29.922149037112575 0.028942661247167517 14\n",
      "The training loss is 32.58456420215143 with std:43.88717436262686. The val loss is 32.14330698304469 with std:47.051769729263505.\n",
      "32.14330698304469 0.028942661247167517 14\n",
      "Evaluating for {'degree': 14, 'lmda': 0.037750532053243954} ...\n",
      "The training loss is 30.776619785550846 with std:44.48508804497037. The val loss is 35.66183972965564 with std:47.78943188431086.\n",
      "35.66183972965564 0.037750532053243954 14\n",
      "The training loss is 33.711033102203636 with std:46.67516428238637. The val loss is 29.934334187744568 with std:41.374714012114914.\n",
      "29.934334187744568 0.037750532053243954 14\n",
      "The training loss is 32.5921753272884 with std:43.902737268368284. The val loss is 32.15549899474656 with std:47.056316092922856.\n",
      "32.15549899474656 0.037750532053243954 14\n",
      "Evaluating for {'degree': 14, 'lmda': 0.04923882631706739} ...\n",
      "The training loss is 30.789600133821473 with std:44.50412144411093. The val loss is 35.67215066845133 with std:47.81074423940604.\n",
      "35.67215066845133 0.04923882631706739 14\n",
      "The training loss is 33.724957519608196 with std:46.69091726835764. The val loss is 29.944838774628728 with std:41.3977600225325.\n",
      "29.944838774628728 0.04923882631706739 14\n",
      "The training loss is 32.598166942130746 with std:43.91868515476734. The val loss is 32.16363564171283 with std:47.05784718656247.\n",
      "32.16363564171283 0.04923882631706739 14\n",
      "Evaluating for {'degree': 14, 'lmda': 0.0642232542222936} ...\n",
      "The training loss is 30.7999229040493 with std:44.52098994684573. The val loss is 35.681470011863595 with std:47.8349744389581.\n",
      "35.681470011863595 0.0642232542222936 14\n",
      "The training loss is 33.73587315085513 with std:46.70642163120444. The val loss is 29.95431758422524 with std:41.420354916044765.\n",
      "29.95431758422524 0.0642232542222936 14\n",
      "The training loss is 32.602947918176326 with std:43.936096497979264. The val loss is 32.168238356685585 with std:47.05647679818378.\n",
      "32.168238356685585 0.0642232542222936 14\n",
      "Evaluating for {'degree': 14, 'lmda': 0.0837677640068292} ...\n",
      "The training loss is 30.80832529265983 with std:44.53675969907804. The val loss is 35.69068704774504 with std:47.86407769161547.\n",
      "35.69068704774504 0.0837677640068292 14\n",
      "The training loss is 33.74451340424667 with std:46.722749003285514. The val loss is 29.963396931097183 with std:41.44393684721305.\n",
      "29.963396931097183 0.0837677640068292 14\n",
      "The training loss is 32.606881615083594 with std:43.95617759664578. The val loss is 32.16963506188759 with std:47.05218778152464.\n",
      "32.16963506188759 0.0837677640068292 14\n",
      "Evaluating for {'degree': 14, 'lmda': 0.10926008611173785} ...\n",
      "The training loss is 30.8154824921888 with std:44.55243953379243. The val loss is 35.70084847080559 with std:47.90051694239084.\n",
      "35.70084847080559 0.10926008611173785 14\n",
      "The training loss is 33.75151001549758 with std:46.741087351535754. The val loss is 29.97270532534455 with std:41.470005879799515.\n",
      "29.97270532534455 0.10926008611173785 14\n",
      "The training loss is 32.610316263746434 with std:43.98033878397033. The val loss is 32.16798279717145 with std:47.04485083459582.\n",
      "32.16798279717145 0.10926008611173785 14\n",
      "Evaluating for {'degree': 14, 'lmda': 0.14251026703029984} ...\n",
      "The training loss is 30.82205797844704 with std:44.569034354788364. The val loss is 35.713270891688964 with std:47.94746723543006.\n",
      "35.713270891688964 0.14251026703029984 14\n",
      "The training loss is 33.7574379163004 with std:46.76282631328658. The val loss is 29.982910152615936 with std:41.500187925056466.\n",
      "29.982910152615936 0.14251026703029984 14\n",
      "The training loss is 32.61362369499885 with std:44.010287315152844. The val loss is 32.16329170597778 with std:47.03425761513216.\n",
      "32.16329170597778 0.14251026703029984 14\n",
      "Evaluating for {'degree': 14, 'lmda': 0.18587918911465645} ...\n",
      "The training loss is 30.82875614211788 with std:44.58761045261244. The val loss is 35.7296653375898 with std:48.00905592623454.\n",
      "35.7296653375898 0.18587918911465645 14\n",
      "The training loss is 33.76286644318847 with std:46.789665070537815. The val loss is 29.994759240122004 with std:41.53629966740522.\n",
      "29.994759240122004 0.18587918911465645 14\n",
      "The training loss is 32.617250041960595 with std:44.04814020935202. The val loss is 32.15545365137007 with std:47.02017804611778.\n",
      "32.15545365137007 0.18587918911465645 14\n",
      "Evaluating for {'degree': 14, 'lmda': 0.24244620170823283} ...\n",
      "The training loss is 30.836370599416373 with std:44.60938466810414. The val loss is 35.75224871827036 with std:48.09061787210776.\n",
      "35.75224871827036 0.24244620170823283 14\n",
      "The training loss is 33.76841798907785 with std:46.82374622506733. The val loss is 30.009125446307493 with std:41.58041238116504.\n",
      "30.009125446307493 0.24244620170823283 14\n",
      "The training loss is 32.62178266874079 with std:44.09656025387528. The val loss is 32.14428084667845 with std:47.002456865375.\n",
      "32.14428084667845 0.24244620170823283 14\n",
      "Evaluating for {'degree': 14, 'lmda': 0.31622776601683794} ...\n",
      "The training loss is 30.845815639249004 with std:44.635857801092996. The val loss is 35.783793468647616 with std:48.19891924826158.\n",
      "35.783793468647616 0.31622776601683794 14\n",
      "The training loss is 33.77483359243831 with std:46.867817228555765. The val loss is 30.027049757005276 with std:41.63491481271719.\n",
      "30.027049757005276 0.31622776601683794 14\n",
      "The training loss is 32.62803644156624 with std:44.15891686276434. The val loss is 32.12956144217824 with std:46.98116967027095.\n",
      "32.12956144217824 0.31622776601683794 14\n",
      "Evaluating for {'degree': 14, 'lmda': 0.41246263829013524} ...\n",
      "The training loss is 30.85812210250246 with std:44.66901890562126. The val loss is 35.82754234370066 with std:48.34226751641199.\n",
      "35.82754234370066 0.41246263829013524 14\n",
      "The training loss is 33.78304047141802 with std:46.925415662770504. The val loss is 30.04977337437002 with std:41.70258082863506.\n",
      "30.04977337437002 0.41246263829013524 14\n",
      "The training loss is 32.63715792151445 with std:44.239469970508424. The val loss is 32.11113914996878 with std:46.95686210209841.\n",
      "32.11113914996878 0.41246263829013524 14\n",
      "Evaluating for {'degree': 14, 'lmda': 0.5379838403443686} ...\n",
      "The training loss is 30.87437813872102 with std:44.711641626580665. The val loss is 35.88691037307117 with std:48.530382670900714.\n",
      "35.88691037307117 0.5379838403443686 14\n",
      "The training loss is 33.794208848517705 with std:47.00106573227242. The val loss is 30.078742426734557 with std:41.78665864985416.\n",
      "30.078742426734557 0.5379838403443686 14\n",
      "The training loss is 32.650736148297256 with std:44.34356940776373. The val loss is 32.08902123944677 with std:46.93089402542925.\n",
      "32.08902123944677 0.5379838403443686 14\n",
      "Evaluating for {'degree': 14, 'lmda': 0.701703828670383} ...\n",
      "The training loss is 30.895608162377304 with std:44.76766426700957. The val loss is 35.96493980944828 with std:48.7738742747283.\n",
      "35.96493980944828 0.701703828670383 14\n",
      "The training loss is 33.8097765102639 with std:47.10046259797502. The val loss is 30.11556317525871 with std:41.89101578593083.\n",
      "30.11556317525871 0.701703828670383 14\n",
      "The training loss is 32.670892639851516 with std:44.47785506220221. The val loss is 32.063512025661716 with std:46.90589908112049.\n",
      "32.063512025661716 0.701703828670383 14\n",
      "Evaluating for {'degree': 14, 'lmda': 0.9152473108773893} ...\n",
      "The training loss is 30.92261584851594 with std:44.84259238428048. The val loss is 36.06358597934632 with std:49.083199880180516.\n",
      "36.06358597934632 0.9152473108773893 14\n",
      "The training loss is 33.83141589040058 with std:47.230614695420705. The val loss is 30.161888168868906 with std:42.02039237980843.\n",
      "30.161888168868906 0.9152473108773893 14\n",
      "The training loss is 32.70030492105619 with std:44.65043715284497. The val loss is 32.03535728043736 with std:46.88634573079327.\n",
      "32.03535728043736 0.9152473108773893 14\n",
      "Evaluating for {'degree': 14, 'lmda': 1.1937766417144369} ...\n",
      "The training loss is 30.955863026110773 with std:44.94382304052889. The val loss is 36.18306655432867 with std:49.4671598650575.\n",
      "36.18306655432867 1.1937766417144369 14\n",
      "The training loss is 33.860931881193856 with std:47.39992339420037. The val loss is 30.21923584760662 with std:42.18082285197411.\n",
      "30.21923584760662 1.1937766417144369 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 32.74211037199369 with std:44.871035829678725. The val loss is 32.00587325936711 with std:46.879152662734754.\n",
      "32.00587325936711 1.1937766417144369 14\n",
      "Evaluating for {'degree': 14, 'lmda': 1.5570684047537318} ...\n",
      "The training loss is 30.995492846985684 with std:45.08083315052306. The val loss is 36.32161566984135 with std:49.93136414336109.\n",
      "36.32161566984135 1.5570684047537318 14\n",
      "The training loss is 33.900121387940594 with std:47.61821967318646. The val loss is 30.288796602271812 with std:42.38026781464947.\n",
      "30.288796602271812 1.5570684047537318 14\n",
      "The training loss is 32.79966392560578 with std:45.151072597396464. The val loss is 31.977033746108294 with std:46.894286424892435.\n",
      "31.977033746108294 1.5570684047537318 14\n",
      "Evaluating for {'degree': 14, 'lmda': 2.030917620904737} ...\n",
      "The training loss is 31.041605766268784 with std:45.26533806094674. The val loss is 36.47594450645864 with std:50.47754813663505.\n",
      "36.47594450645864 2.030917620904737 14\n",
      "The training loss is 33.95069815517532 with std:47.896861714144634. The val loss is 30.37135165994742 with std:42.6294621454572.\n",
      "30.37135165994742 2.030917620904737 14\n",
      "The training loss is 32.87620768480701 with std:45.50374695083772. The val loss is 31.951516283720554 with std:46.94528334640764.\n",
      "31.951516283720554 2.030917620904737 14\n",
      "Evaluating for {'degree': 14, 'lmda': 2.6489692876105297} ...\n",
      "The training loss is 31.094858584165216 with std:45.51173319227628. The val loss is 36.64250753556746 with std:51.10474106525458.\n",
      "36.64250753556746 2.6489692876105297 14\n",
      "The training loss is 34.01446500855883 with std:48.24912093884064. The val loss is 30.46749830599113 with std:42.942977268245414.\n",
      "30.46749830599113 2.6489692876105297 14\n",
      "The training loss is 32.974650940853564 with std:45.94422569966524. The val loss is 31.932775881472672 with std:47.04972002631598.\n",
      "31.932775881472672 2.6489692876105297 14\n",
      "Evaluating for {'degree': 14, 'lmda': 3.4551072945922217} ...\n",
      "The training loss is 31.157418671080514 with std:45.83822056401945. The val loss is 36.81943969490965 with std:51.81280199656943.\n",
      "36.81943969490965 3.4551072945922217 14\n",
      "The training loss is 34.09396099329719 with std:48.69120548259167. The val loss is 30.578396792068393 with std:43.34057561453743.\n",
      "30.578396792068393 3.4551072945922217 14\n",
      "The training loss is 33.09780470765239 with std:46.490242699513814. The val loss is 31.925312199642025 with std:47.2298027370338.\n",
      "31.925312199642025 3.4551072945922217 14\n",
      "Evaluating for {'degree': 14, 'lmda': 4.506570337745478} ...\n",
      "The training loss is 31.23432424670988 with std:46.26891587864608. The val loss is 37.00895518488758 with std:52.60796929750944.\n",
      "37.00895518488758 4.506570337745478 14\n",
      "The training loss is 34.19380463814723 with std:49.24431102835405. The val loss is 30.707221592037886 with std:43.84909718940816.\n",
      "30.707221592037886 4.506570337745478 14\n",
      "The training loss is 33.2494842322493 with std:47.16361884369517. The val loss is 31.935393726328837 with std:47.513399026616895.\n",
      "31.935393726328837 4.506570337745478 14\n",
      "Evaluating for {'degree': 14, 'lmda': 5.878016072274912} ...\n",
      "The training loss is 31.335408814862955 with std:46.837036310207395. The val loss is 37.22017604401314 with std:53.5094885957987.\n",
      "37.22017604401314 5.878016072274912 14\n",
      "The training loss is 34.32293535375954 with std:49.93798766944164. The val loss is 30.861464439083125 with std:44.50525099641515.\n",
      "30.861464439083125 5.878016072274912 14\n",
      "The training loss is 33.43684761350009 with std:47.993325576243144. The val loss is 31.972571869958276 with std:47.935910050426564.\n",
      "31.972571869958276 5.878016072274912 14\n",
      "Evaluating for {'degree': 14, 'lmda': 7.666822074546214} ...\n",
      "The training loss is 31.47813155772118 with std:47.589149626433844. The val loss is 37.472720911320856 with std:54.55652625985153.\n",
      "37.472720911320856 7.666822074546214 14\n",
      "The training loss is 34.49800429851739 with std:50.814898780257096. The val loss is 31.056287678046857 with std:45.35965729380174.\n",
      "31.056287678046857 7.666822074546214 14\n",
      "The training loss is 33.67425112843624 with std:49.02055631005533. The val loss is 32.05236920981575 with std:48.54332884301125.\n",
      "32.05236920981575 7.666822074546214 14\n",
      "Evaluating for {'degree': 14, 'lmda': 10.0} ...\n",
      "The training loss is 31.691878558139617 with std:48.590447819152295. The val loss is 37.80178774646626 with std:55.81518680936439.\n",
      "37.80178774646626 10.0 14\n",
      "The training loss is 34.74834204756081 with std:51.93683106462351. The val loss is 31.319317024795946 with std:46.48226979425479.\n",
      "31.319317024795946 10.0 14\n",
      "The training loss is 33.98892058647738 with std:50.30581695379811. The val loss is 32.200608645126046 with std:49.396660789134614.\n",
      "32.200608645126046 10.0 14\n"
     ]
    }
   ],
   "source": [
    "#list of lambda values to try.. use np.logspace\n",
    "search_lambda = np.logspace(-2,1,num=27)\n",
    "#list of degrees\n",
    "search_degree = np.arange(1,15,1)\n",
    "\n",
    "params = {'degree':search_degree,'lmda':search_lambda,}\n",
    "k_fold =3\n",
    "#call to the grid search function\n",
    "grid_val,grid_val_std = grid_search_cv(params,k_fold,fold_ind,do_cross_validation_reg,X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from helper import plot_cv_result\n",
    "plot_cv_result(np.log((grid_val.T)),search_lambda,search_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from helper import plot_cv_result\n",
    "plot_cv_result(np.log((grid_val_std.T)),search_lambda,search_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.465896057722029\n",
      "Best score achieved using degree:2 and lambda:0.01\n"
     ]
    }
   ],
   "source": [
    "# best val score\n",
    "best_score = np.min(grid_val)\n",
    "print(best_score)\n",
    "\n",
    "# params which give best val score\n",
    "d,l= np.unravel_index(np.argmin(grid_val),grid_val.shape)\n",
    "best_degree = search_degree[d]\n",
    "best_lambda = search_lambda[l]\n",
    "print('Best score achieved using degree:{} and lambda:{}'.format(best_degree,best_lambda))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 11.34080986909835 with std:18.817289416677134. The test loss is 14.535089828109243 with std:20.78133204720384.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14.535089828109243"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluate on the test set\n",
    "X_train_poly,mu,std = expand_and_normalize_X(X_train,best_degree)\n",
    "w = get_w_analytical_with_regularization(X_train_poly,y_train,best_lambda)\n",
    "X_test_poly = expand_X(X_test,best_degree)\n",
    "X_test_poly[:,1:] =  (X_test_poly[:,1:]-mu)/std\n",
    "\n",
    "get_loss(w,X_train_poly,y_train,X_test_poly,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How can you interpret the linear regression coefficients?\n",
    "\n",
    "**Question**: Is it good to have coefficients' values close to zero? \n",
    "\n",
    "**Question**: How would you proceed to improve the prediction?\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
